#!/usr/bin/env python3
"""
Llama 4 Compression Analysis for Google Colab

This script runs Llama 4 locally on Google Colab to get REAL token probabilities
for compression analysis. All text samples are embedded in the code.

To use in Google Colab:
1. Copy-paste this entire file into a Colab cell
2. Run the cell
3. Download the results JSON file when complete

Expected runtime: 30-60 minutes depending on GPU
Memory requirement: ~12GB GPU RAM (use T4 or better)
"""

import json
import math
import time
import csv
from datetime import datetime
from io import StringIO

# ============================================================================
# TEXT SAMPLES (embedded in code for easy Colab copy-paste)
# ============================================================================

TEXT_SAMPLES = {
    "kl_intro_10kb": """KL divergence measures how well a distribution p is fitted by a model distribution q.

Here's the KL divergence formula for two discrete distributions p_1, ..., p_n and q_1, ..., q_n:

D(p,q) = Σ p_i log₂(p_i/q_i)

You can think of KL divergence like a wonky distance measure between two distributions. But a better intuition goes like this: When you keep sampling from the true distribution p, KL divergence tells you how fast you can figure out you're sampling from p and not some other distribution q. Small KL means that q is a pretty good imposter pretending to be p.

Before we dive into KL, let's refresh on Bayes' theorem—it's how you update your beliefs when you get new info. Example: You've got a coin that might be rigged. To keep it simple, let's say there are only two options: it's either fair (50/50 for heads/tails) or it's biased toward tails (25/75 for heads/tails).

To use Bayes, you need a starting guess (a prior) about which it is. Let's say you think there's a 2/3 chance it's fair and 1/3 chance it's biased. You flip the coin—Heads! This is evidence for the hypothesis that the coin is fair since heads are more likely with a fair coin.

Bayes' rule calculates the new probability (the posterior) that it's fair. The standard way to write the rule goes like this:

P(fair | heads) = P(heads | fair) · P(fair) / P(heads)

This is no rocket science — if you multiply both sides by P(heads), both sides express the quantity P(fair AND heads) using conditional probabilities.

I like to think about Bayes' rule a bit differently:

Posterior odds = Prior odds × Likelihood ratio

This formula is just another way of writing down the Bayes' rule. To understand the message of this version, let's think in odds instead of probabilities. Gamblers love this—instead of "1/3 chance of this, 2/3 chance of that," they say "odds are 1:2." Odds don't need to add to one (1:2 is the same as 2:4), which is actually often pretty handy - allows you to focus on the more important parts of the picture.

With odds, Bayes' formula is super clean! You just have to multiply your prior odds by P(heads | fair)/P(heads | biased)—that's how much more likely heads are under each hypothesis. These conditional probabilities P(event | hypothesis) are called likelihoods, so this ratio is the likelihood ratio.

KL divergence is about what happens when you keep flipping. Picture our Bayesian hero flipping over and over, updating her beliefs each time using Bayes. If she gets {H, T, T, H, T}, every flip, she multiplies her odds by a likelihood ratio: 2/1 for heads, 2/3 for tails.

Before we see what happens long-term, let's improve our calculator a bit. We'll get a bit more clarity if we take logs of everything so we can add instead of multiply. Instead of multiplying odds, we just add so-called log-odds.

Notice that all the likelihoods (numbers in yellow rows) are now negative numbers. That makes sense - probabilities are smaller than one, so after taking the log, they become negative numbers. It's often useful to talk about absolute values of those numbers, which is why people define a quantity called surprisal: Whenever you have something happening with probability p, the expression log 1/p can be called a surprisal and its values are bits.

This is a logarithmic viewpoint on how surprised you are when something happens. Getting heads when you thought it was 1% likely? Super surprising (log 1/0.01 ≈ 6.6 bits). Getting heads on a fair coin? Meh (log 1/0.5 = 1 bit).

When we subtract surprisals for the same outcome under different hypotheses, we get how much evidence that outcome provides. For our coin example, heads give us 1 bit of surprisal given the fair hypothesis and two bits of surprisal given the biased hypothesis, so we get 1 bit of evidence towards the fair hypothesis. Analogously, tails give 0.58 bits of evidence towards the biased hypothesis.

To get the final probability, add up all the surprisals for each hypothesis, exponentiate, and don't forget the prior. In our example, the total evidence for the fair hypothesis is:

1 - 0.58 - 0.58 + 1 - 0.58 ≈ 0.25.

We started with one bit favoring the fair hypothesis, so we end up with 1.25 bits for fair. Convert that back and you get about 70% probability the coin is fair.

Let's say the coin actually is biased. How fast will our Bayesian hero figure this out? We can calculate the average number of bits that she learns per flip. Heads give -1 bit (negative because it points the wrong way), tails give +0.58 bits. On average, each flip gives:

0.25 × (-1) + 0.75 × 0.58 ≈ 0.19

bits of evidence toward the truth. This is the KL divergence between the true 25%/75% distribution and the model 50%/50% distribution!

What does this mean in practice? After about 5 flips, you get one bit of evidence on average. So if you start thinking 2:1 the coin is fair, after ~5 flips you'll be at 1:1. Another 5 flips gets you to 2:1 it's biased, then 4:1, and so on.

The actual odds bounce around this average. But thanks to the law of large numbers, after N flips the total evidence will be close to 0.19 × N. More precisely, it's 0.19N ± O(√N). Ultimately, we use logs and talk about bits because the law of large numbers works for adding stuff, not multiplying.

KL divergence is just the general formula for expected evidence accumulation. Say you've got two distributions p and q, where p is what's really happening, but you only know it's either p or q.

You can keep sampling from the unknown distribution and play the Bayesian game: Whenever you sample outcome i, compare the likelihoods p_i and q_i and update your beliefs. This means adding log 1/p_i bits to p's surprise total and log 1/q_i bits to q's.

On average, each sample from the true distribution p gives you:

D(p,q) = Σ p_i · log(p_i/q_i)

bits of evidence toward the truth.

When this number is small (less than 1), you can think of 1/D(p,q) as "how many samples until I get one bit of evidence." One bit means the odds for the true hypothesis doubled. This isn't the same as doubling the probability. Going from 1:1 to 2:1 odds means 50% → 66.7% probability. But with lopsided odds like 1:1000, gaining a bit toward the underdog (making it 2:1000) almost doubles its probability. And gaining a bit the other way (1:2000) almost halves the underdog's probability. The truth's probability shoots up exponentially until it's comparable to the alternative, then the alternative's probability tanks exponentially.

Notice KL divergence is about the evidence, not your starting beliefs. It tells you how fast beliefs change, no matter where you start. Even though we've been all about Bayes' rule here, KL divergence works for both Bayesian and frequentist statistics.

Remember our riddle about training a language model? We've got some true distribution p of what letter typically follows short pieces of text like "My name is A". We also have our model's guess q, and we need to measure how good the match is.

This is done by optimizing KL divergence, because this literally means that we want to make it hard for a Bayesian detective to distinguish between the true language p and language model q. Later on, we will discuss that we can't optimize KL divergence, but instead optimize cross-entropy, but the story stays the same.

What about using ℓ₁ norm (Σ |p_i - q_i|) or ℓ₂ norm (Σ (p_i - q_i)²)? They might work okay, but they don't have the probability story behind them, which leads to some iffy behavior. Take these two possibilities:

1. p_i = 0.5, q_i = 0.49
2. p_i = 0.01, q_i = 0.0

Regular norms (ℓ₁, ℓ₂) think these errors are about the same size. But KL knows better—the first one's basically fine, but the second is a disaster model! For example, the word "God" typically does not follow with "zilla", but your model should understand that this may sometimes happen. The distribution q('zilla' | 'God') = 0.0 means that the model does not even begin to consider this option which makes it pretty bad!

Back to our riddle: How do you measure distance from independence?

Remember our three joint distributions p₁, p₂, p₃: They all have the same marginals (same row and column totals). If the marginals were independent, we'd get this product distribution q. Which table is the "most" independent?

We need to measure how different each table is from the ideal independent one. There are more reasonable ways to do this (people often use correlation, but that has problems. First, correlation only works for numbers, not general stuff like {☀️, ☁️}. Plus, zero correlation doesn't mean independence), but using KL divergence is a very principled way to do this. We've got two distributions: the "truth" (one of our tables) and the "model" (the ideal independent table). The KL between them tells us how well the model matches reality—basically, how long until a Bayesian detective figures out the data isn't coming from the independent model.

The KL divergences for our tables:
D(p₁, q) ≈ 0.40
D(p₂, q) ≈ 0.04
D(p₃, q) ≈ 0.21

So table 2 is "closest" to independence!

This works for any joint distribution (r, s). The KL divergence between (r, s) and the independent version r ⊗ s is called mutual information between r and s—it's a super-important quantity in information theory.

In the next section, we'll dig deeper into the KL formula and see how it connects to entropy and cross-entropy. """,

    "pi_digits_10kb": """3.141592653589793238462643383279502884197169399375105820974944592307816406286 208998628034825342117067982148086513282306647093844609550582231725359408128481 117450284102701938521105559644622948954930381964428810975665933446128475648233 786783165271201909145648566923460348610454326648213393607260249141273724587006 606315588174881520920962829254091715364367892590360011330530548820466521384146 951941511609433057270365759591953092186117381932611793105118548074462379962749 567351885752724891227938183011949129833673362440656643086021394946395224737190 702179860943702770539217176293176752384674818467669405132000568127145263560827 785771342757789609173637178721468440901224953430146549585371050792279689258923 542019956112129021960864034418159813629774771309960518707211349999998372978049 951059731732816096318595024459455346908302642522308253344685035261931188171010 003137838752886587533208381420617177669147303598253490428755468731159562863882 353787593751957781857780532171226806613001927876611195909216420198938095257201 065485863278865936153381827968230301952035301852968995773622599413891249721775 283479131515574857242454150695950829533116861727855889075098381754637464939319 255060400927701671139009848824012858361603563707660104710181942955596198946767 837449448255379774726847104047534646208046684259069491293313677028989152104752 162056966024058038150193511253382430035587640247496473263914199272604269922796 782354781636009341721641219924586315030286182974555706749838505494588586926995 690927210797509302955321165344987202755960236480665499119881834797753566369807 426542527862551818417574672890977772793800081647060016145249192173217214772350 141441973568548161361157352552133475741849468438523323907394143334547762416862 518983569485562099219222184272550254256887671790494601653466804988627232791786 085784383827967976681454100953883786360950680064225125205117392984896084128488 626945604241965285022210661186306744278622039194945047123713786960956364371917 287467764657573962413890865832645995813390478027590099465764078951269468398352 595709825822620522489407726719478268482601476990902640136394437455305068203496 252451749399651431429809190659250937221696461515709858387410597885959772975498 930161753928468138268683868942774155991855925245953959431049972524680845987273 644695848653836736222626099124608051243884390451244136549762780797715691435997 700129616089441694868555848406353422072225828488648158456028506016842739452267 467678895252138522549954666727823986456596116354886230577456498035593634568174 324112515076069479451096596094025228879710893145669136867228748940560101503308 617928680920874760917824938589009714909675985261365549781893129784821682998948 722658804857564014270477555132379641451523746234364542858444795265867821051141 354735739523113427166102135969536231442952484937187110145765403590279934403742 007310578539062198387447808478489683321445713868751943506430218453191048481005 370614680674919278191197939952061419663428754440643745123718192179998391015919 561814675142691239748940907186494231961567945208095146550225231603881930142093 762137855956638937787083039069792077346722182562599661501421503068038447734549 202605414665925201497442850732518666002132434088190710486331734649651453905796 268561005508106658796998163574736384052571459102897064140110971206280439039759 515677157700420337869936007230558763176359421873125147120532928191826186125867 321579198414848829164470609575270695722091756711672291098169091528017350671274 858322287183520935396572512108357915136988209144421006751033467110314126711136 990865851639831501970165151168517143765761835155650884909989859982387345528331 635507647918535893226185489632132933089857064204675259070915481416549859461637 180270981994309924488957571282890592323326097299712084433573265489382391193259 746366730583604142813883032038249037589852437441702913276561809377344403070746 921120191302033038019762110110044929321516084244485963766983895228684783123552 658213144957685726243344189303968642624341077322697802807318915441101044682325 271620105265227211166039666557309254711055785376346682065310989652691862056476 931257058635662018558100729360659876486117910453348850346113657686753249441668 039626579787718556084552965412665408530614344431858676975145661406800700237877 659134401712749470420562230538994561314071127000407854733269939081454664645880 797270826683063432858785698305235808933065757406795457163775254202114955761581 400250126228594130216471550979259230990796547376125517656751357517829666454779 174501129961489030463994713296210734043751895735961458901938971311179042978285 647503203198691514028708085990480109412147221317947647772622414254854540332157 185306142288137585043063321751829798662237172159160771669254748738986654949450 114654062843366393790039769265672146385306736096571209180763832716641627488880 078692560290228472104031721186082041900042296617119637792133757511495950156604 963186294726547364252308177036751590673502350728354056704038674351362222477158 915049530984448933309634087807693259939780541934144737744184263129860809988868 741326047215695162396586457302163159819319516735381297416772947867242292465436 680098067692823828068996400482435403701416314965897940924323789690706977942236 250822168895738379862300159377647165122893578601588161755782973523344604281512 627203734314653197777416031990665541876397929334419521541341899485444734567383 162499341913181480927777103863877343177207545654532207770921201905166096280490 926360197598828161332316663652861932668633606273567630354477628035045077723554 710585954870279081435624014517180624643626794561275318134078330336254232783944 975382437205835311477119926063813346776879695970309833913077109870408591337464 144282277263465947047458784778720192771528073176790770715721344473060570073349 243693113835049316312840425121925651798069411352801314701304781643788518529092 854520116583934196562134914341595625865865570552690496520985803385072242648293 972858478316305777756068887644624824685792603953527734803048029005876075825104 747091643961362676044925627420420832085661190625454337213153595845068772460290 161876679524061634252257719542916299193064553779914037340432875262888963995879 475729174642635745525407909145135711136941091193932519107602082520261879853188 770584297259167781314969900901921169717372784768472686084900337702424291651300 500516832336435038951702989392233451722013812806965011784408745196012122859937 162313017114448464090389064495444006198690754851602632750529834918740786680881 833851022833450850486082503930213321971551843063545500766828294930413776552793 975175461395398468339363830474611996653858153842056853386218672523340283087112 328278921250771262946322956398989893582116745627010218356462201349671518819097 303811980049734072396103685406643193950979019069963955245300545058068550195673 022921913933918568034490398205955100226353536192041994745538593810234395544959 778377902374216172711172364343543947822181852862408514006660443325888569867054 315470696574745855033232334210730154594051655379068662733379958511562578432298 827372319898757141595781119635833005940873068121602876496286744604774649159950 549737425626901049037781986835938146574126804925648798556145372347867330390468 838343634655379498641927056387293174872332083760112302991136793862708943879936 201629515413371424892830722012690147546684765357616477379467520049075715552781 965362132392640616013635815590742202020318727760527721900556148425551879253034 351398442532234157623361064250639049750086562710953591946589751413103482276930 624743536325691607815478181152843667957061108615331504452127473924544945423682 886061340841486377670096120715124914043027253860764823634143346235189757664521 641376796903149501910857598442391986291642193994907236234646844117394032659184 044378051333894525742399508296591228508555821572503107125701266830240292952522 011872676756220415420516184163484756516999811614101002996078386909291603028840 026910414079288621507842451670908700069928212066041837180653556725253256753286 129104248776182582976515795984703562226293486003415872298053498965022629174878 820273420922224533985626476691490556284250391275771028402799806636582548892648 802545661017296702664076559042909945681506526530537182941270336931378517860904 070866711496558343434769338578171138645587367812301458768712660348913909562009 939361031029161615288138437909904231747336394804575931493140529763475748119356 709110137751721008031559024853090669203767192203322909433467685142214477379393 751703443661991040337511173547191855046449026365512816228824462575916333039107 225383742182140883508657391771509682887478265699599574490661758344137522397096 834080053559849175417381883999446974867626551658276584835884531427756879002909 517028352971634456212964043523117600665101241200659755851276178583829204197484 423608007193045761893234922927965019875187212726750798125547095890455635792122 103334669749923563025494780249011419521238281530911407907386025152274299581807 247162591668545133312394804947079119153267343028244186041426363954800044800267 049624820179289647669758318327131425170296923488962766844032326092752496035799 646925650493681836090032380929345958897069536534940603402166544375589004563288 225054525564056448246515187547119621844396582533754388569094113031509526179378 002974120766514793942590298969594699556576121865619673378623625612521632086286 922210327488921865436480229678070576561514463204692790682120738837781423356282 360896320806822246801224826117718589638140918390367367222088832151375560037279 839400415297002878307667094447456013455641725437090697939612257142989467154357 846878861444581231459357198492252847160504922124247014121478057345510500801908 699603302763478708108175450119307141223390866393833952942578690507643100638351 983438934159613185434754649556978103829309716465143840700707360411237359984345 225161050702705623526601276484830840761183013052793205427462865403603674532865 105706587488225698157936789766974220575059683440869735020141020672358502007245 22563265134105592401902742162484391403599895353""",

    "declaration": """The unanimous Declaration of the thirteen united States of America, When in the Course of human events, it becomes necessary for one people to dissolve the political bands which have connected them with another, and to assume among the powers of the earth, the separate and equal station to which the Laws of Nature and of Nature's God entitle them, a decent respect to the opinions of mankind requires that they should declare the causes which impel them to the separation.

We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.--That to secure these rights, Governments are instituted among Men, deriving their just powers from the consent of the governed, --That whenever any Form of Government becomes destructive of these ends, it is the Right of the People to alter or to abolish it, and to institute new Government, laying its foundation on such principles and organizing its powers in such form, as to them shall seem most likely to effect their Safety and Happiness. Prudence, indeed, will dictate that Governments long established should not be changed for light and transient causes; and accordingly all experience hath shewn, that mankind are more disposed to suffer, while evils are sufferable, than to right themselves by abolishing the forms to which they are accustomed. But when a long train of abuses and usurpations, pursuing invariably the same Object evinces a design to reduce them under absolute Despotism, it is their right, it is their duty, to throw off such Government, and to provide new Guards for their future security.--Such has been the patient sufferance of these Colonies; and such is now the necessity which constrains them to alter their former Systems of Government. The history of the present King of Great Britain is a history of repeated injuries and usurpations, all having in direct object the establishment of an absolute Tyranny over these States. To prove this, let Facts be submitted to a candid world.

He has refused his Assent to Laws, the most wholesome and necessary for the public good.

He has forbidden his Governors to pass Laws of immediate and pressing importance, unless suspended in their operation till his Assent should be obtained; and when so suspended, he has utterly neglected to attend to them.

He has refused to pass other Laws for the accommodation of large districts of people, unless those people would relinquish the right of Representation in the Legislature, a right inestimable to them and formidable to tyrants only.

He has called together legislative bodies at places unusual, uncomfortable, and distant from the depository of their public Records, for the sole purpose of fatiguing them into compliance with his measures.

He has dissolved Representative Houses repeatedly, for opposing with manly firmness his invasions on the rights of the people.

He has refused for a long time, after such dissolutions, to cause others to be elected; whereby the Legislative powers, incapable of Annihilation, have returned to the People at large for their exercise; the State remaining in the mean time exposed to all the dangers of invasion from without, and convulsions within.

He has endeavoured to prevent the population of these States; for that purpose obstructing the Laws for Naturalization of Foreigners; refusing to pass others to encourage their migrations hither, and raising the conditions of new Appropriations of Lands.

He has obstructed the Administration of Justice, by refusing his Assent to Laws for establishing Judiciary powers.

He has made Judges dependent on his Will alone, for the tenure of their offices, and the amount and payment of their salaries.

He has erected a multitude of New Offices, and sent hither swarms of Officers to harrass our people, and eat out their substance.

He has kept among us, in times of peace, Standing Armies without the Consent of our legislatures.

He has affected to render the Military independent of and superior to the Civil power.

He has combined with others to subject us to a jurisdiction foreign to our constitution, and unacknowledged by our laws; giving his Assent to their Acts of pretended Legislation:

For Quartering large bodies of armed troops among us:

For protecting them, by a mock Trial, from punishment for any Murders which they should commit on the Inhabitants of these States:

For cutting off our Trade with all parts of the world:

For imposing Taxes on us without our Consent:

For depriving us in many cases, of the benefits of Trial by Jury:

For transporting us beyond Seas to be tried for pretended offences:

For abolishing the free System of English Laws in a neighbouring Province, establishing therein an Arbitrary government, and enlarging its Boundaries so as to render it at once an example and fit instrument for introducing the same absolute rule into these Colonies:

For taking away our Charters, abolishing our most valuable Laws, and altering fundamentally the Forms of our Governments:

For suspending our own Legislatures, and declaring themselves invested with power to legislate for us in all cases whatsoever.

He has abdicated Government here, by declaring us out of his Protection and waging War against us.

He has plundered our seas, ravaged our Coasts, burnt our towns, and destroyed the lives of our people.

He is at this time transporting large Armies of foreign Mercenaries to compleat the works of death, desolation and tyranny, already begun with circumstances of Cruelty & perfidy scarcely paralleled in the most barbarous ages, and totally unworthy the Head of a civilized nation.

He has constrained our fellow Citizens taken Captive on the high Seas to bear Arms against their Country, to become the executioners of their friends and Brethren, or to fall themselves by their Hands.

He has excited domestic insurrections amongst us, and has endeavoured to bring on the inhabitants of our frontiers, the merciless Indian Savages, whose known rule of warfare, is an undistinguished destruction of all ages, sexes and conditions.

In every stage of these Oppressions We have Petitioned for Redress in the most humble terms: Our repeated Petitions have been answered only by repeated injury. A Prince, whose character is thus marked by every act which may define a Tyrant, is unfit to be the ruler of a free people.

Nor have We been wanting in attentions to our Brittish brethren. We have warned them from time to time of attempts by their legislature to extend an unwarrantable jurisdiction over us. We have reminded them of the circumstances of our emigration and settlement here. We have appealed to their native justice and magnanimity, and we have conjured them by the ties of our common kindred to disavow these usurpations, which, would inevitably interrupt our connections and correspondence. They too have been deaf to the voice of justice and of consanguinity. We must, therefore, acquiesce in the necessity, which denounces our Separation, and hold them, as we hold the rest of mankind, Enemies in War, in Peace Friends.

We, therefore, the Representatives of the united States of America, in General Congress, Assembled, appealing to the Supreme Judge of the world for the rectitude of our intentions, do, in the Name, and by Authority of the good People of these Colonies, solemnly publish and declare, That these United Colonies are, and of Right ought to be Free and Independent States; that they are Absolved from all Allegiance to the British Crown, and that all political connection between them and the State of Great Britain, is and ought to be totally dissolved; and that as Free and Independent States, they have full Power to levy War, conclude Peace, contract Alliances, establish Commerce, and to do all other Acts and Things which Independent States may of right do. And for the support of this Declaration, with a firm reliance on the protection of divine Providence, we mutually pledge to each other our Lives, our Fortunes and our sacred Honor.

Georgia

Button Gwinnett

Lyman Hall

George Walton

 

North Carolina

William Hooper

Joseph Hewes

John Penn

 

South Carolina

Edward Rutledge

Thomas Heyward, Jr.

Thomas Lynch, Jr.

Arthur Middleton

 

Massachusetts

John Hancock

Maryland

Samuel Chase

William Paca

Thomas Stone

Charles Carroll of Carrollton

 

Virginia

George Wythe

Richard Henry Lee

Thomas Jefferson

Benjamin Harrison

Thomas Nelson, Jr.

Francis Lightfoot Lee

Carter Braxton

 

Pennsylvania

Robert Morris

Benjamin Rush

Benjamin Franklin

John Morton

George Clymer

James Smith

George Taylor

James Wilson

George Ross

Delaware

Caesar Rodney

George Read

Thomas McKean

 

New York

William Floyd

Philip Livingston

Francis Lewis

Lewis Morris

 

New Jersey

Richard Stockton

John Witherspoon

Francis Hopkinson

John Hart

Abraham Clark

 

New Hampshire

Josiah Bartlett

William Whipple

 

Massachusetts

Samuel Adams

John Adams

Robert Treat Paine

Elbridge Gerry

 

Rhode Island

Stephen Hopkins

William Ellery

 

Connecticut

Roger Sherman

Samuel Huntington

William Williams

Oliver Wolcott

 

New Hampshire

Matthew Thornton""",

    "human_mitochondrial_dna": "GATCACAGGTCTATCACCCTATTAACCACTCACGGGAGCTCTCCATGCATTTGGTATTTTCGTCTGGGGGGTATGCACGCGATAGCATTGCGAGACGCTGGAGCCGGAGCACCCTATGTCGCAGTATCTGTCTTTGATTCCTGCCTCATCCTATTATTTATCGCACCTACGTTCAATATTACAGGCGAACATACTTACTAAAGTGTGTTAATTAATTAATGCTTGTAGGACATAATAATAACAATTGAATGTCTGCACAGCCACTTTCCACACAGACATCATAACAAAAAATTTCCACCAAACCCCCCCTCCCCCGCTTCTGGCCACAGCACTTAAACACATCTCTGCCAAACCCCAAAAACAAAGAACCCTAACACCAGCCTAACCAGATTTCAAATTTTATCTTTTGGCGGTATGCACTTTTAACAGTCACCCCCCAACTAACACATTATTTTCCCCTCCCACTCCCATACTACTAATCTCATCAATACAACCCCCGCCCATCCTACCCAGCACACACACACCGCTGCTAACCCCATACCCCGAACCAACCAAACCCCAAAGACACCCCCCACAGTTTATGTAGCTTACCTCCTCAAAGCAATACACTGAAAATGTTTAGACGGGCTCACATCACCCCATAAACAAATAGGTTTGGTCCTAGCCTTTCTATTAGCTCTTAGTAAGATTACACATGCAAGCATCCCCGTTCCAGTGAGTTCACCCTCTAAATCACCACGATCAAAAGGAACAAGCATCAAGCACGCAGCAATGCAGCTCAAAACGCTTAGCCTAGCCACACCCCCACGGGAAACAGCAGTGATTAACCTTTAGCAATAAACGAAAGTTTAACTAAGCTATACTAACCCCAGGGTTGGTCAATTTCGTGCCAGCCACCGCGGTCACACGATTAACCCAAGTCAATAGAAGCCGGCGTAAAGAGTGTTTTAGATCACCCCCTCCCCAATAAAGCTAAAACTCACCTGAGTTGTAAAAAACTCCAGTTGACACAAAATAGACTACGAAAGTGGCTTTAACATATCTGAACACACAATAGCTAAGACCCAAACTGGGATTAGATACCCCACTATGCTTAGCCCTAAACCTCAACAGTTAAATCAACAAAACTGCTCGCCAGAACACTACGAGCCACAGCTTAAAACTCAAAGGACCTGGCGGTGCTTCATATCCCTCTAGAGGAGCCTGTTCTGTAATCGATAAACCCCGATCAACCTCACCACCTCTTGCTCAGCCTATATACCGCCATCTTCAGCAAACCCTGATGAAGGCTACAAAGTAAGCGCAAGTACCCACGTAAAGACGTTAGGTCAAGGTGTAGCCCATGAGGTGGCAAGAAATGGGCTACATTTTCTACCCCAGAAAACTACGATAGCCCTTATGAAACTTAAGGGTCGAAGGTGGATTTAGCAGTAAACTAAGAGTAGAGTGCTTAGTTGAACAGGGCCCTGAAGCGCGTACACACCGCCCGTCACCCTCCTCAAGTATACTTCAAAGGACATTTAACTAAAACCCCTACGCATTTATATAGAGGAGACAAGTCGTAACATGGTAAGTGTACTGGAAAGTGCACTTGGACGAACCAGAGTGTAGCTTAACACAAAGCACCCAACTTACACTTAGGAGATTTCAACTTAACTTGACCGCTCTGAGCTAAACCTAGCCCCAAACCCACTCCACCTTACTACCAGACAACCTTAGCCAAACCATTTACCCAAATAAAGTATAGGCGATAGAAATTGAAACCTGGCGCAATAGATATAGTACCGCAAGGGAAAGATGAAAAATTATAACCAAGCATAATATAGCAAGGACTAACCCCTATACCTTCTGCATAATGAATTAACTAGAAATAACTTTGCAAGGAGAGCCAAAGCTAAGACCCCCGAAACCAGACGAGCTACCTAAGAACAGCTAAAAGAGCACACCCGTCTATGTAGCAAAATAGTGGGAAGATTTATAGGTAGAGGCGACAAACCTACCGAGCCTGGTGATAGCTGGTTGTCCAAGATAGAATCTTAGTTCAACTTTAAATTTGCCCACAGAACCCTCTAAATCCCCTTGTAAATTTAACTGTTAGTCCAAAGAGGAACAGCTCTTTGGACACTAGGAAAAAACCTTGTAGAGAGAGTAAAAAATTTAACACCCATAGTAGGCCTAAAAGCAGCCACCAATTAAGAAAGCGTTCAAGCTCAACACCCACTACCTAAAAAATCCCAAACATATAACTGAACTCCTCACACCCAATTGGACCAATCTATCACCCTATAGAAGAACTAATGTTAGTATAAGTAACATGAAAACATTCTCCTCCGCATAAGCCTGCGTCAGATTAAAACACTGAACTGACAATTAACAGCCCAATATCTACAATCAACCAACAAGTCATTATTACCCTCACTGTCAACCCAACACAGGCATGCTCATAAGGAAAGGTTAAAAAAAGTAAAAGGAACTCGGCAAATCTTACCCCGCCTGTTTACCAAAAACATCACCTCTAGCATCACCAGTATTAGAGGCACCGCCTGCCCAGTGACACATGTTTAACGGCCGCGGTACCCTAACCGTGCAAAGGTAGCATAATCACTTGTTCCTTAAATAGGGACCTGTATGAATGGCTCCACGAGGGTTCAGCTGTCTCTTACTTTTAACCAGTGAAATTGACCTGCCCGTGAAGAGGCGGGCATAACACAGCAAGACGAGAAGACCCTATGGAGCTTTAATTTATTAATGCAAACAGTACCTAACAAACCCACAGGTCCTAAACTACCAAACCTGCATTAAAAATTTCGGTTGGGGCGACCTCGGAGCAGAACCCAACCTCCGAGCAGTACATGCTAAGACTTCACCAGTCAAAGCGAACTACTATACTCAATTGATCCAATAACTTGACCAACGGAACAAGTTACCCTAGGGATAACAGCGCAATCCTATTCTAGAGTCCATATCAACAATAGGGTTTACGACCTCGATGTTGGATCAGGACATCCCGATGGTGCAGCCGCTATTAAAGGTTCGTTTGTTCAACGATTAAAGTCCTACGTGATCTGAGTTCAGACCGGAGTAATCCAGGTCGGTTTCTATCTACNTTCAAATTCCTCCCTGTACGAAAGGACAAGAGAAATAAGGCCTACTTCACAAAGCGCCTTCCCCCGTAAATGATATCATCTCAACTTAGTATTATACCCACACCCACCCAAGAACAGGGTTTGTTAAGATGGCAGAGCCCGGTAATCGCATAAAACTTAAAACTTTACAGTCAGAGGTTCAATTCCTCTTCTTAACAACATACCCATGGCCAACCTCCTACTCCTCATTGTACCCATTCTAATCGCAATGGCATTCCTAATGCTTACCGAACGAAAAATTCTAGGCTATATACAACTACGCAAAGGCCCCAACGTTGTAGGCCCCTACGGGCTACTACAACCCTTCGCTGACGCCATAAAACTCTTCACCAAAGAGCCCCTAAAACCCGCCACATCTACCATCACCCTCTACATCACCGCCCCGACCTTAGCTCTCACCATCGCTCTTCTACTATGAACCCCCCTCCCCATACCCAACCCCCTGGTCAACCTCAACCTAGGCCTCCTATTTATTCTAGCCACCTCTAGCCTAGCCGTTTACTCAATCCTCTGATCAGGGTGAGCATCAAACTCAAACTACGCCCTGATCGGCGCACTGCGAGCAGTAGCCCAAACAATCTCATATGAAGTCACCCTAGCCATCATTCTACTATCAACATTACTAATAAGTGGCTCCTTTAACCTCTCCACCCTTATCACAACACAAGAACACCTCTGATTACTCCTGCCATCATGACCCTTGGCCATAATATGATTTATCTCCACACTAGCAGAGACCAACCGAACCCCCTTCGACCTTGCCGAAGGGGAGTCCGAACTAGTCTCAGGCTTCAACATCGAATACGCCGCAGGCCCCTTCGCCCTATTCTTCATAGCCGAATACACAAACATTATTATAATAAACACCCTCACCACTACAATCTTCCTAGGAACAACATATGACGCACTCTCCCCTGAACTCTACACAACATATTTTGTCACCAAGACCCTACTTCTAACCTCCCTGTTCTTATGAATTCGAACAGCATACCCCCGATTCCGCTACGACCAACTCATACACCTCCTATGAAAAAACTTCCTACCACTCACCCTAGCATTACTTATATGATATGTCTCCATACCCATTACAATCTCCAGCATTCCCCCTCAAACCTAAGAAATATGTCTGATAAAAGAGTTACTTTGATAGAGTAAATAATAGGAGCTTAAACCCCCTTATTTCTAGGACTATGAGAATCGAACCCATCCCTGAGAATCCAAAATTCTCCGTGCCACCTATCACACCCCATCCTAAAGTAAGGTCAGCTAAATAAGCTATCGGGCCCATACCCCGAAAATGTTGGTTATACCCTTCCCGTACTAATTAATCCCCTGGCCCAACCCGTCATCTACTCTACCATCTTTGCAGGCACACTCATCACAGCGCTAAGCTCGCACTGATTTTTTACCTGAGTAGGCCTAGAAATAAACATGCTAGCTTTTATTCCAGTTCTAACCAAAAAAATAAACCCTCGTTCCACAGAAGCTGCCATCAAGTATTTCCTCACGCAAGCAACCGCATCCATAATCCTTCTAATAGCTATCCTCTTCAACAATATACTCTCCGGACAATGAACCATAACCAATACTACCAATCAATACTCATCATTAATAATCATAATAGCTATAGCAATAAAACTAGGAATAGCCCCCTTTCACTTCTGAGTCCCAGAGGTTACCCAAGGCACCCCTCTGACATCCGGCCTGCTTCTTCTCACATGACAAAAACTAGCCCCCATCTCAATCATATACCAAATCTCTCCCTCACTAAACGTAAGCCTTCTCCTCACTCTCTCAATCTTATCCATCATAGCAGGCAGTTGAGGTGGATTAAACCAAACCCAGCTACGCAAAATCTTAGCATACTCCTCAATTACCCACATAGGATGAATAATAGCAGTTCTACCGTACAACCCTAACATAACCATTCTTAATTTAACTATTTATATTATCCTAACTACTACCGCATTCCTACTACTCAACTTAAACTCCAGCACCACGACCCTACTACTATCTCGCACCTGAAACAAGCTAACATGACTAACACCCTTAATTCCATCCACCCTCCTCTCCCTAGGAGGCCTGCCCCCGCTAACCGGCTTTTTGCCCAAATGGGCCATTATCGAAGAATTCACAAAAAACAATAGCCTCATCATCCCCACCATCATAGCCACCATCACCCTCCTTAACCTCTACTTCTACCTACGCCTAATCTACTCCACCTCAATCACACTACTCCCCATATCTAACAACGTAAAAATAAAATGACAGTTTGAACATACAAAACCCACCCCATTCCTCCCCACACTCATCGCCCTTACCACGCTACTCCTACCTATCTCCCCTTTTATACTAATAATCTTATAGAAATTTAGGTTAAATACAGACCAAGAGCCTTCAAAGCCCTCAGTAAGTTGCAATACTTAATTTCTGTAACAGCTAAGGACTGCAAAACCCCACTCTGCATCAACTGAACGCAAATCAGCCACTTTAATTAAGCTAAGCCCTTACTAGACCAATGGGACTTAAACCCACAAACACTTAGTTAACAGCTAAGCACCCTAATCAACTGGCTTCAATCTACTTCTCCCGCCGCCGGGAAAAAAGGCGGGAGAAGCCCCGGCAGGTTTGAAGCTGCTTCTTCGAATTTGCAATTCAATATGAAAATCACCTCGGAGCTGGTAAAAAGAGGCCTAACCCCTGTCTTTAGATTTACAGTCCAATGCTTCACTCAGCCATTTTACCTCACCCCCACTGATGTTCGCCGACCGTTGACTATTCTCTACAAACCACAAAGACATTGGAACACTATACCTATTATTCGGCGCATGAGCTGGAGTCCTAGGCACAGCTCTAAGCCTCCTTATTCGAGCCGAGCTGGGCCAGCCAGGCAACCTTCTAGGTAACGACCACATCTACAACGTTATCGTCACAGCCCATGCATTTGTAATAATCTTCTTCATAGTAATACCCATCATAATCGGAGGCTTTGGCAACTGACTAGTTCCCCTAATAATCGGTGCCCCCGATATGGCGTTTCCCCGCATAAACAACATAAGCTTCTGACTCTTACCTCCCTCTCTCCTACTCCTGCTCGCATCTGCTATAGTGGAGGCCGGAGCAGGAACAGGTTGAACAGTCTACCCTCCCTTAGCAGGGAACTACTCCCACCCTGGAGCCTCCGTAGACCTAACCATCTTCTCCTTACACCTAGCAGGTGTCTCCTCTATCTTAGGGGCCATCAATTTCATCACAACAATTATCAATATAAAACCCCCTGCCATAACCCAATACCAAACGCCCCTCTTCGTCTGATCCGTCCTAATCACAGCAGTCCTACTTCTCCTATCTCTCCCAGTCCTAGCTGCTGGCATCACTATACTACTAACAGACCGCAACCTCAACACCACCTTCTTCGACCCCGCCGGAGGAGGAGACCCCATTCTATACCAACACCTATTCTGATTTTTCGGTCACCCTGAAGTTTATATTCTTATCCTACCAGGCTTCGGAATAATCTCCCATATTGTAACTTACTACTCCGGAAAAAAAGAACCATTTGGATACATAGGTATGGTCTGAGCTATGATATCAATTGGCTTCCTAGGGTTTATCGTGTGAGCACACCATATATTTACAGTAGGAATAGACGTAGACACACGAGCATATTTCACCTCCGCTACCATAATCATCGCTATCCCCACCGGCGTCAAAGTATTTAGCTGACTCGCCACACTCCACGGAAGCAATATGAAATGATCTGCTGCAGTGCTCTGAGCCCTAGGATTCATCTTTCTTTTCACCGTAGGTGGCCTGACTGGCATTGTATTAGCAAACTCATCACTAGACATCGTACTACACGACACGTACTACGTTGTAGCCCACTTCCACTATGTCCTATCAATAGGAGCTGTATTTGCCATCATAGGAGGCTTCATTCACTGATTTCCCCTATTCTCAGGCTACACCCTAGACCAAACCTACGCCAAAATCCATTTCACTATCATATTCATCGGCGTAAATCTAACTTTCTTCCCACAACACTTTCTCGGCCTATCCGGAATGCCCCGACGTTACTCGGACTACCCCGATGCATACACCACATGAAACATCCTATCATCTGTAGGCTCATTCATTTCTCTAACAGCAGTAATATTAATAATTTTCATGATTTGAGAAGCCTTCGCTTCGAAGCGAAAAGTCCTAATAGTAGAAGAACCCTCCATAAACCTGGAGTGACTATATGGATGCCCCCCACCCTACCACACATTCGAAGAACCCGTATACATAAAATCTAGACAAAAAAGGAAGGAATCGAACCCCCCAAAGCTGGTTTCAAGCCAACCCCATGGCCTCCATGACTTTTTCAAAAAGGTATTAGAAAAACCATTTCATAACTTTGTCAAAGTTAAATTATAGGCTAAATCCTATATATCTTAATGGCACATGCAGCGCAAGTAGGTCTACAAGACGCTACTTCCCCTATCATAGAAGAGCTTATCACCTTTCATGATCACGCCCTCATAATCATTTTCCTTATCTGCTTCCTAGTCCTGTATGCCCTTTTCCTAACACTCACAACAAAACTAACTAATACTAACATCTCAGACGCTCAGGAAATAGAAACCGTCTGAACTATCCTGCCCGCCATCATCCTAGTCCTCATCGCCCTCCCATCCCTACGCATCCTTTACATAACAGACGAGGTCAACGATCCCTCCCTTACCATCAAATCAATTGGCCACCAATGGTACTGAACCTACGAGTACACCGACTACGGCGGACTAATCTTCAACTCCTACATACTTCCCCCATTATTCCTAGAACCAGGCGACCTGCGACTCCTTGACGTTGACAATCGAGTAGTACTCCCGATTGAAGCCCCCATTCGTATAATAATTACATCACAAGACGTCTTGCACTCATGAGCTGTCCCCACATTAGGCTTAAAAACAGATGCAATTCCCGGACGTCTAAACCAAACCACTTTCACCGCTACACGACCGGGGGTATACTACGGTCAATGCTCTGAAATCTGTGGAGCAAACCACAGTTTCATGCCCATCGTCCTAGAATTAATTCCCCTAAAAATCTTTGAAATAGGGCCCGTATTTACCCTATAGCACCCCCTCTACCCCCTCTAGAGCCCACTGTAAAGCTAACTTAGCATTAACCTTTTAAGTTAAAGATTAAGAGAACCAACACCTCTTTACAGTGAAATGCCCCAACTAAATACTACCGTATGGCCCACCATAATTACCCCCATACTCCTTACACTATTCCTCATCACCCAACTAAAAATATTAAACACAAACTACCACCTACCTCCCTCACCAAAGCCCATAAAAATAAAAAATTATAACAAACCCTGAGAACCAAAATGAACGAAAATCTGTTCGCTTCATTCATTGCCCCCACAATCCTAGGCCTACCCGCCGCAGTACTGATCATTCTATTTCCCCCTCTATTGATCCCCACCTCCAAATATCTCATCAACAACCGACTAATCACCACCCAACAATGACTAATCAAACTAACCTCAAAACAAATGATAACCATACACAACACTAAAGGACGAACCTGATCTCTTATACTAGTATCCTTAATCATTTTTATTGCCACAACTAACCTCCTCGGACTCCTGCCTCACTCATTTACACCAACCACCCAACTATCTATAAACCTAGCCATGGCCATCCCCTTATGAGCGGGCACAGTGATTATAGGCTTTCGCTCTAAGATTAAAAATGCCCTAGCCCACTTCTTACCACAAGGCACACCTACACCCCTTATCCCCATACTAGTTATTATCGAAACCATCAGCCTACTCATTCAACCAATAGCCCTGGCCGTACGCCTAACCGCTAACATTACTGCAGGCCACCTACTCATGCACCTAATTGGAAGCGCCACCCTAGCAATATCAACCATTAACCTTCCCTCTACACTTATCATCTTCACAATTCTAATTCTACTGACTATCCTAGAAATCGCTGTCGCCTTAATCCAAGCCTACGTTTTCACACTTCTAGTAAGCCTCTACCTGCACGACAACACATAATGACCCACCAATCACATGCCTATCATATAGTAAAACCCAGCCCATGACCCCTAACAGGGGCCCTCTCAGCCCTCCTAATGACCTCCGGCCTAGCCATGTGATTTCACTTCCACTCCATAACGCTCCTCATACTAGGCCTACTAACCAACACACTAACCATATACCAATGATGGCGCGATGTAACACGAGAAAGCACATACCAAGGCCACCACACACCACCTGTCCAAAAAGGCCTTCGATACGGGATAATCCTATTTATTACCTCAGAAGTTTTTTTCTTCGCAGGATTTTTCTGAGCCTTTTACCACTCCAGCCTAGCCCCTACCCCCCAATTAGGAGGGCACTGGCCCCCAACAGGCATCACCCCGCTAAATCCCCTAGAAGTCCCACTCCTAAACACATCCGTATTACTCGCATCAGGAGTATCAATCACCTGAGCTCACCATAGTCTAATAGAAAACAACCGAAACCAAATAATTCAAGCACTGCTTATTACAATTTTACTGGGTCTCTATTTTACCCTCCTACAAGCCTCAGAGTACTTCGAGTCTCCCTTCACCATTTCCGACGGCATCTACGGCTCAACATTTTTTGTAGCCACAGGCTTCCACGGACTTCACGTCATTATTGGCTCAACTTTCCTCACTATCTGCTTCATCCGCCAACTAATATTTCACTTTACATCCAAACATCACTTTGGCTTCGAAGCCGCCGCCTGATACTGGCATTTTGTAGATGTGGTTTGACTATTTCTGTATGTCTCCATCTATTGATGAGGGTCTTACTCTTTTAGTATAAATAGTACCGTTAACTTCCAATTAACTAGTTTTGACAACATTCAAAAAAGAGTAATAAACTTCGCCTTAATTTTAATAATCAACACCCTCCTAGCCTTACTACTAATAATTATTACATTTTGACTACCACAACTCAACGGCTACATAGAAAAATCCACCCCTTACGAGTGCGGCTTCGACCCTATATCCCCCGCCCGCGTCCCTTTCTCCATAAAATTCTTCTTAGTAGCTATTACCTTCTTATTATTTGATCTAGAAATTGCCCTCCTTTTACCCCTACCATGAGCCCTACAAACAACTAACCTGCCACTAATAGTTATGTCATCCCTCTTATTAATCATCATCCTAGCCCTAAGTCTGGCCTATGAGTGACTACAAAAAGGATTAGACTGAACCGAATTGGTATATAGTTTAAACAAAACGAATGATTTCGACTCATTAAATTATGATAATCATATTTACCAAATGCCCCTCATTTACATAAATATTATACTAGCATTTACCATCTCACTTCTAGGAATACTAGTATATCGCTCACACCTCATATCCTCCCTACTATGCCTAGAAGGAATAATACTATCGCTGTTCATTATAGCTACTCTCATAACCCTCAACACCCACTCCCTCTTAGCCAATATTGTGCCTATTGCCATACTAGTCTTTGCCGCCTGCGAAGCAGCGGTGGGCCTAGCCCTACTAGTCTCAATCTCCAACACATATGGCCTAGACTACGTACATAACCTAAACCTACTCCAATGCTAAAACTAATCGTCCCAACAATTATATTACTACCACTGACATGACTTTCCAAAAAACACATAATTTGAATCAACACAACCACCCACAGCCTAATTATTAGCATCATCCCTCTACTATTTTTTAACCAAATCAACAACAACCTATTTAGCTGTTCCCCAACCTTTTCCTCCGACCCCCTAACAACCCCCCTCCTAATACTAACTACCTGACTCCTACCCCTCACAATCATGGCAAGCCAACGCCACTTATCCAGTGAACCACTATCACGAAAAAAACTCTACCTCTCTATACTAATCTCCCTACAAATCTCCTTAATTATAACATTCACAGCCACAGAACTAATCATATTTTATATCTTCTTCGAAACCACACTTATCCCCACCTTGGCTATCATCACCCGATGAGGCAACCAGCCAGAACGCCTGAACGCAGGCACATACTTCCTATTCTACACCCTAGTAGGCTCCCTTCCCCTACTCATCGCACTAATTTACACTCACAACACCCTAGGCTCACTAAACATTCTACTACTCACTCTCACTGCCCAAGAACTATCAAACTCCTGAGCCAACAACTTAATATGACTAGCTTACACAATAGCTTTTATAGTAAAGATACCTCTTTACGGACTCCACTTATGACTCCCTAAAGCCCATGTCGAAGCCCCCATCGCTGGGTCAATAGTACTTGCCGCAGTACTCTTAAAACTAGGCGGCTATGGTATAATACGCCTCACACTCATTCTCAACCCCCTGACAAAACACATAGCCTACCCCTTCCTTGTACTATCCCTATGAGGCATAATTATAACAAGCTCCATCTGCCTACGACAAACAGACCTAAAATCGCTCATTGCATACTCTTCAATCAGCCACATAGCCCTCGTAGTAACAGCCATTCTCATCCAAACCCCCTGAAGCTTCACCGGCGCAGTCATTCTCATAATCGCCCACGGGCTTACATCCTCATTACTATTCTGCCTAGCAAACTCAAACTACGAACGCACTCACAGTCGCATCATAATCCTCTCTCAAGGACTTCAAACTCTACTCCCACTAATAGCTTTTTGATGACTTCTAGCAAGCCTCGCTAACCTCGCCTTACCCCCCACTATTAACCTACTGGGAGAACTCTCTGTGCTAGTAACCACGTTCTCCTGATCAAATATCACTCTCCTACTTACAGGACTCAACATACTAGTCACAGCCCTATACTCCCTCTACATATTTACCACAACACAATGGGGCTCACTCACCCACCACATTAACAACATAAAACCCTCATTCACACGAGAAAACACCCTCATGTTCATACACCTATCCCCCATTCTCCTCCTATCCCTCAACCCCGACATCATTACCGGGTTTTCCTCTTGTAAATATAGTTTAACCAAAACATCAGATTGTGAATCTGACAACAGAGGCTTACGACCCCTTATTTACCGAGAAAGCTCACAAGAACTGCTAACTCATGCCCCCATGTCTAACAACATGGCTTTCTCAACTTTTAAAGGATAACAGCTATCCATTGGTCTTAGGCCCCAAAAATTTTGGTGCAACTCCAAATAAAAGTAATAACCATGCACACTACTATAACCACCCTAACCCTGACTTCCCTAATTCCCCCCATCCTTACCACCCTCGTTAACCCTAACAAAAAAAACTCATACCCCCATTATGTAAAATCCATTGTCGCATCCACCTTTATTATCAGTCTCTTCCCCACAACAATATTCATGTGCCTAGACCAAGAAGTTATTATCTCGAACTGACACTGAGCCACAACCCAAACAACCCAGCTCTCCCTAAGCTTCAAACTAGACTACTTCTCCATAATATTCATCCCTGTAGCATTGTTCGTTACATGGTCCATCATAGAATTCTCACTGTGATATATAAACTCAGACCCAAACATTAATCAGTTCTTCAAATATCTACTCATCTTCCTAATTACCATACTAATCTTAGTTACCGCTAACAACCTATTCCAACTGTTCATCGGCTGAGAGGGCGTAGGAATTATATCCTTCTTGCTCATCAGTTGATGATACGCCCGAGCAGATGCCAACACAGCAGCCATTCAAGCAATCCTATACAACCGTATCGGCGATATCGGTTTCATCCTCGCCTTAGCATGATTTATCCTACACTCCAACTCATGAGACCCACAACAAATAGCCCTTCTAAACGCTAATCCAAGCCTCACCCCACTACTAGGCCTCCTCCTAGCAGCAGCAGGCAAATCAGCCCAATTAGGTCTCCACCCCTGACTCCCCTCAGCCATAGAAGGCCCCACCCCAGTCTCAGCCCTACTCCACTCAAGCACTATAGTTGTAGCAGGAATCTTCTTACTCATCCGCTTCCACCCCCTAGCAGAAAATAGCCCACTAATCCAAACTCTAACACTATGCTTAGGCGCTATCACCACTCTGTTCGCAGCAGTCTGCGCCCTTACACAAAATGACATCAAAAAAATCGTAGCCTTCTCCACTTCAAGTCAACTAGGACTCATAATAGTTACAATCGGCATCAACCAACCACACCTAGCATTCCTGCACATCTGTACCCACGCCTTCTTCAAAGCCATACTATTTATGTGCTCCGGGTCCATCATCCACAACCTTAACAATGAACAAGATATTCGAAAAATAGGAGGACTACTCAAAACCATACCTCTCACTTCAACCTCCCTCACCATTGGCAGCCTAGCATTAGCAGGAATACCTTTCCTCACAGGTTTCTACTCCAAAGACCACATCATCGAAACCGCAAACATATCATACACAAACGCCTGAGCCCTATCTATTACTCTCATCGCTACCTCCCTGACAAGCGCCTATAGCACTCGAATAATTCTTCTCACCCTAACAGGTCAACCTCGCTTCCCCACCCTTACTAACATTAACGAAAATAACCCCACCCTACTAAACCCCATTAAACGCCTGGCAGCCGGAAGCCTATTCGCAGGATTTCTCATTACTAACAACATTTCCCCCGCATCCCCCTTCCAAACAACAATCCCCCTCTACCTAAAACTCACAGCCCTCGCTGTCACTTTCCTAGGACTTCTAACAGCCCTAGACCTCAACTACCTAACCAACAAACTTAAAATAAAATCCCCACTATGCACATTTTATTTCTCCAACATACTCGGATTCTACCCTAGCATCACACACCGCACAATCCCCTATCTAGGCCTTCTTACGAGCCAAAACCTGCCCCTACTCCTCCTAGACCTAACCTGACTAGAAAAGCTATTACCTAAAACAATTTCACAGCACCAAATCTCCACCTCCATCATCACCTCAACCCAAAAAGGCATAATTAAACTTTACTTCCTCTCTTTCTTCTTCCCACTCATCCTAACCCTACTCCTAATCACATAACCTATTCCCCCGAGCAATCTCAATTACAATATATACACCAACAAACAATGTTCAACCAGTAACTACTACTAATCAACGCCCATAATCATACAAAGCCCCCGCACCAATAGGATCCTCCCGAATCAACCCTGACCCCTCTCCTTCATAAATTATTCAGCTTCCTACACTATTAAAGTTTACCACAACCACCACCCCATCATACTCTTTCACCCACAGCACCAATCCTACCTCCATCGCTAACCCCACTAAAACACTCACCAAGACCTCAACCCCTGACCCCCATGCCTCAGGATACTCCTCAATAGCCATCGCTGTAGTATATCCAAAGACAACCATCATTCCCCCTAAATAAATTAAAAAAACTATTAAACCCATATAACCTCCCCCAAAATTCAGAATAATAACACACCCGACCACACCGCTAACAATCAATACTAAACCCCCATAAATAGGAGAAGGCTTAGAAGAAAACCCCACAAACCCCATTACTAAACCCACACTCAACAGAAACAAAGCATACATCATTATTCTCGCACGGACTACAACCACGACCAATGATATGAAAAACCATCGTTGTATTTCAACTACAAGAACACCAATGACCCCAATACGCAAAACTAACCCCCTAATAAAATTAATTAACCACTCATTCATCGACCTCCCCACCCCATCCAACATCTCCGCATGATGAAACTTCGGCTCACTCCTTGGCGCCTGCCTGATCCTCCAAATCACCACAGGACTATTCCTAGCCATGCACTACTCACCAGACGCCTCAACCGCCTTTTCATCAATCGCCCACATCACTCGAGACGTAAATTATGGCTGAATCATCCGCTACCTTCACGCCAATGGCGCCTCAATATTCTTTATCTGCCTCTTCCTACACATCGGGCGAGGCCTATATTACGGATCATTTCTCTACTCAGAAACCTGAAACATCGGCATTATCCTCCTGCTTGCAACTATAGCAACAGCCTTCATAGGCTATGTCCTCCCGTGAGGCCAAATATCATTCTGAGGGGCCACAGTAATTACAAACTTACTATCCGCCATCCCATACATTGGGACAGACCTAGTTCAATGAATCTGAGGAGGCTACTCAGTAGACAGTCCCACCCTCACACGATTCTTTACCTTTCACTTCATCTTGCCCTTCATTATTGCAGCCCTAGCAACACTCCACCTCCTATTCTTGCACGAAACGGGATCAAACAACCCCCTAGGAATCACCTCCCATTCCGATAAAATCACCTTCCACCCTTACTACACAATCAAAGACGCCCTCGGCTTACTTCTCTTCCTTCTCTCCTTAATGACATTAACACTATTCTCACCAGACCTCCTAGGCGACCCAGACAATTATACCCTAGCCAACCCCTTAAACACCCCTCCCCACATCAAGCCCGAATGATATTTCCTATTCGCCTACACAATTCTCCGATCCGTCCCTAACAAACTAGGAGGCGTCCTTGCCCTATTACTATCCATCCTCATCCTAGCAATAATCCCCATCCTCCATATATCCAAACAACAAAGCATAATATTTCGCCCACTAAGCCAATCACTTTATTGACTCCTAGCCGCAGACCTCCTCATTCTAACCTGAATCGGAGGACAACCAGTAAGCTACCCTTTTACCATCATTGGACAAGTAGCATCCGTACTATACTTCACAACAATCCTAATCCTAATACCAACTATCTCCCTAATTGAAAACAAAATACTCAAATGGGCCTGTCCTTGTAGTATAAACTAATACACCAGTCTTGTAAACCGGAGATGAAAACCTTTTTCCAAGGACAAATCAGAGAAAAAGTCTTTAACTCCACCATTAGCACCCAAAGCTAAGATTCTAATTTAAACTATTCTCTGTTCTTTCATGGGGAAGCAGATTTGGGTACCACCCAAGTATTGACTCACCCATCAACAACCGCTATGTATTTCGTACATTACTGCCAGCCACCATGAATATTGTACGGTACCATAAATACTTGACCACCTGTAGTACATAAAAACCCAATCCACATCAAAACCCCCTCCCCATGCTTACAAGCAAGTACAGCAATCAACCCTCAACTATCACACATCAACTGCAACTCCAAAGCCACCCCTCACCCACTAGGATACCAACAAACCTACCCACCCTTAACAGTACATAGTACATAAAGCCATTTACCGTACATAGCACATTACAGTCAAATCCCTTCTCGTCCCCATGGATGACCCCCCTCAGATAGGGGTCCCTTGACCACCATCCTCCGTGAAATCAATATCCCGCACAAGAGTGCTACTCTCCTCGCTCCGGGCCCATAACACTTGGGGGTAGCTAAAGTGAACTGTATCCGACATCTGGTTCCTACTTCAGGGTCATAAAGCCTAAATAGCCCACACGTTCCCCTTAAATAAGACATCACGATG""",

    "huffman_code_10kb": """import heapq
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Optional, Any

class HuffmanNode:
    # Node in the Huffman tree.
    def __init__(self, char: Optional[str] = None, freq: int = 0, left: Optional['HuffmanNode'] = None, right: Optional['HuffmanNode'] = None):
        self.char = char
        self.freq = freq
        self.left = left
        self.right = right
    
    def __lt__(self, other: 'HuffmanNode') -> bool:
        return self.freq < other.freq
    
    def __repr__(self) -> str:
        if self.char:
            return f"HuffmanNode({self.char}, {self.freq})"
        return f"HuffmanNode(Internal, {self.freq})"

class HuffmanCoding:
    # Complete implementation of Huffman coding algorithm.
    
    def __init__(self):
        self.root: Optional[HuffmanNode] = None
        self.char_to_code: Dict[str, str] = {}
        self.code_to_char: Dict[str, str] = {}
    
    def build_frequency_table(self, text: str) -> Dict[str, int]:
        # Build frequency table from input text.
        return dict(Counter(text))
    
    def build_huffman_tree(self, freq_table: Dict[str, int]) -> Optional[HuffmanNode]:
        # Build Huffman tree from frequency table.
        if not freq_table:
            return None
        
        # Create a priority queue with leaf nodes
        heap: List[HuffmanNode] = []
        for char, freq in freq_table.items():
            node = HuffmanNode(char=char, freq=freq)
            heapq.heappush(heap, node)
        
        # Build the tree bottom-up
        while len(heap) > 1:
            left = heapq.heappop(heap)
            right = heapq.heappop(heap)
            
            # Create internal node
            internal = HuffmanNode(
                freq=left.freq + right.freq,
                left=left,
                right=right
            )
            heapq.heappush(heap, internal)
        
        return heap[0] if heap else None
    
    def generate_codes(self, root: Optional[HuffmanNode], code: str = "") -> None:
        # Generate Huffman codes by traversing the tree.
        if not root:
            return
        
        # Leaf node - store the code
        if root.char is not None:
            # Handle edge case of single character
            if code == "":
                code = "0"
            self.char_to_code[root.char] = code
            self.code_to_char[code] = root.char
            return
        
        # Recursively generate codes
        if root.left:
            self.generate_codes(root.left, code + "0")
        if root.right:
            self.generate_codes(root.right, code + "1")
    
    def encode(self, text: str) -> Tuple[str, Dict[str, str]]:
        # Encode text using Huffman coding.
        if not text:
            return "", {}
        
        # Build frequency table
        freq_table = self.build_frequency_table(text)
        
        # Build Huffman tree
        self.root = self.build_huffman_tree(freq_table)
        
        # Generate codes
        self.char_to_code = {}
        self.code_to_char = {}
        self.generate_codes(self.root)
        
        # Encode the text
        encoded = "".join(self.char_to_code[char] for char in text)
        
        return encoded, self.char_to_code
    
    def decode(self, encoded: str, root: Optional[HuffmanNode] = None) -> str:
        # Decode Huffman-encoded string.
        if not encoded or not root:
            root = self.root
        
        if not root:
            return ""
        
        # Handle single character edge case
        if root.char is not None:
            return root.char * len(encoded)
        
        decoded = []
        current = root
        
        for bit in encoded:
            if bit == "0":
                current = current.left
            else:
                current = current.right
            
            # Reached a leaf node
            if current and current.char is not None:
                decoded.append(current.char)
                current = root
        
        return "".join(decoded)
    
    def compress(self, text: str) -> Tuple[bytes, Dict[str, str]]:
        # Compress text to bytes.
        encoded_str, code_table = self.encode(text)
        
        # Convert bit string to bytes
        # Pad to make it divisible by 8
        padding = 8 - len(encoded_str) % 8
        if padding != 8:
            encoded_str += "0" * padding
        
        # Convert to bytes
        compressed = bytearray()
        for i in range(0, len(encoded_str), 8):
            byte = encoded_str[i:i+8]
            compressed.append(int(byte, 2))
        
        return bytes(compressed), code_table
    
    def decompress(self, compressed: bytes, code_table: Dict[str, str], original_length: int) -> str:
        # Decompress bytes back to text.
        # Convert bytes back to bit string
        bit_string = "".join(format(byte, "08b") for byte in compressed)
        
        # Rebuild the tree from code table
        self.root = self._rebuild_tree(code_table)
        
        # Decode
        decoded = self.decode(bit_string)
        
        # Remove padding by using original length
        return decoded[:original_length]
    
    def _rebuild_tree(self, code_table: Dict[str, str]) -> HuffmanNode:
        # Rebuild Huffman tree from code table.
        root = HuffmanNode()
        
        for char, code in code_table.items():
            current = root
            for bit in code:
                if bit == "0":
                    if not current.left:
                        current.left = HuffmanNode()
                    current = current.left
                else:
                    if not current.right:
                        current.right = HuffmanNode()
                    current = current.right
            current.char = char
        
        return root
    
    def calculate_compression_ratio(self, original: str, compressed: bytes) -> float:
        # Calculate compression ratio.
        original_bits = len(original) * 8
        compressed_bits = len(compressed) * 8
        return original_bits / compressed_bits if compressed_bits > 0 else 0
    
    def print_code_table(self, code_table: Dict[str, str]) -> None:
        # Pretty print the code table.
        print("Huffman Code Table:")
        print("-" * 40)
        for char, code in sorted(code_table.items()):
            display_char = repr(char) if char in ["\n", "\t", " "] else char
            print(f"{display_char:10} | {code}")
    
    def analyze_encoding(self, text: str) -> Dict[str, Any]:
        # Analyze the encoding efficiency.
        freq_table = self.build_frequency_table(text)
        total_chars = len(text)
        
        encoded, code_table = self.encode(text)
        
        # Calculate average bits per character
        avg_bits = sum(len(code_table[char]) * freq_table[char] 
                      for char in freq_table) / total_chars
        
        # Calculate entropy (theoretical minimum)
        import math
        entropy = -sum((freq/total_chars) * math.log2(freq/total_chars) 
                      for freq in freq_table.values())
        
        return {
            'original_size': total_chars * 8,
            'encoded_size': len(encoded),
            'compression_ratio': (total_chars * 8) / len(encoded),
            'average_bits_per_char': avg_bits,
            'entropy': entropy,
            'efficiency': entropy / avg_bits if avg_bits > 0 else 0,
            'unique_chars': len(freq_table),
            'code_table': code_table
        }

def demonstrate_huffman():
    # Demonstrate Huffman coding with various examples.
    
    # Example 1: Simple text
    print("Example 1: Simple repetitive text")
    print("=" * 50)
    text1 = "abracadabra"
    huffman1 = HuffmanCoding()
    encoded1, codes1 = huffman1.encode(text1)
    
    print(f"Original text: {text1}")
    print(f"Original size: {len(text1) * 8} bits")
    print(f"Encoded size: {len(encoded1)} bits")
    print(f"Compression ratio: {len(text1) * 8 / len(encoded1):.2f}x")
    huffman1.print_code_table(codes1)
    
    # Verify decoding
    decoded1 = huffman1.decode(encoded1)
    print(f"Decoded text: {decoded1}")
    print(f"Decoding correct: {text1 == decoded1}")
    print()
    
    # Example 2: English text
    print("\nExample 2: English text")
    print("=" * 50)
    text2 = "the quick brown fox jumps over the lazy dog"
    huffman2 = HuffmanCoding()
    analysis2 = huffman2.analyze_encoding(text2)
    
    print(f"Original text: {text2}")
    print(f"Original size: {analysis2['original_size']} bits")
    print(f"Encoded size: {analysis2['encoded_size']} bits")
    print(f"Compression ratio: {analysis2['compression_ratio']:.2f}x")
    print(f"Average bits per character: {analysis2['average_bits_per_char']:.2f}")
    print(f"Theoretical minimum (entropy): {analysis2['entropy']:.2f}")
    print(f"Encoding efficiency: {analysis2['efficiency']:.1%}")
    print()
    
    # Example 3: Highly skewed distribution
    print("\nExample 3: Highly skewed distribution")
    print("=" * 50)
    text3 = "a" * 100 + "b" * 10 + "c" * 5 + "d" * 2 + "e"
    huffman3 = HuffmanCoding()
    analysis3 = huffman3.analyze_encoding(text3)
    
    print(f"Text composition: 100 as, 10 bs, 5 cs, 2 ds, 1 e")
    print(f"Original size: {analysis3['original_size']} bits")
    print(f"Encoded size: {analysis3['encoded_size']} bits")
    print(f"Compression ratio: {analysis3['compression_ratio']:.2f}x")
    print(f"Average bits per character: {analysis3['average_bits_per_char']:.2f}")
    print(f"Theoretical minimum (entropy): {analysis3['entropy']:.2f}")
    huffman3.print_code_table(analysis3['code_table'])
    
    # Example 4: Binary data compression
    print("\nExample 4: DNA sequence")
    print("=" * 50)
    dna = "ATGCGATCGTAGCTAGCTAGCTAGCTACGATCGATCGATCGTAGCTAGCTAGCTAGCTACGAT" * 5
    huffman4 = HuffmanCoding()
    compressed4, codes4 = huffman4.compress(dna)
    
    print(f"DNA sequence length: {len(dna)} characters")
    print(f"Original size: {len(dna)} bytes")
    print(f"Compressed size: {len(compressed4)} bytes")
    print(f"Compression ratio: {len(dna) / len(compressed4):.2f}x")
    print(f"Bits per nucleotide: {len(compressed4) * 8 / len(dna):.2f}")
    huffman4.print_code_table(codes4)
    
    # Verify compression/decompression
    decompressed4 = huffman4.decompress(compressed4, codes4, len(dna))
    print(f"Decompression correct: {dna == decompressed4}")

class AdaptiveHuffman:
    # Adaptive Huffman coding that updates the tree as it encodes.
    
    def __init__(self):
        self.root = None
        self.nodes = {}
        self.next_node_num = 0
    
    # Additional implementation for adaptive variant...
    # This would be quite complex, so I'll focus on the static version above

if __name__ == "__main__":
    demonstrate_huffman()
    
    # Test on larger text
    print("\n\nLarge text compression test:")
    print("=" * 50)
    
    # Generate a larger text with known distribution
    large_text = "the " * 1000 + "quick " * 500 + "brown " * 300 + "fox " * 200
    large_text += "jumps " * 150 + "over " * 100 + "lazy " * 50 + "dog " * 25
    
    huffman = HuffmanCoding()
    start_time = time.time() if 'time' in globals() else 0
    
    compressed, code_table = huffman.compress(large_text)
    compression_time = time.time() - start_time if 'time' in globals() else 0
    
    print(f"Original size: {len(large_text)} characters ({len(large_text)} bytes)")
    print(f"Compressed size: {len(compressed)} bytes")
    print(f"Compression ratio: {len(large_text) / len(compressed):.2f}x")
    print(f"Compression time: {compression_time:.3f} seconds" if compression_time else "")
    
    # Verify decompression
    decompressed = huffman.decompress(compressed, code_table, len(large_text))
    print(f"Decompression successful: {large_text == decompressed}")""",

    "repeated_10kb": "The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog."
}

# ============================================================================
# LLAMA 4 COMPRESSION ANALYSIS
# ============================================================================

def install_requirements():
    """Install required packages for Colab."""
    import subprocess
    import sys
    
    packages = [
        "torch", 
        "transformers", 
        "accelerate",
        "bitsandbytes",
        "huggingface_hub"
    ]
    
    print("📦 Installing required packages...")
    for package in packages:
        print(f"Installing {package}...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", package, "-q"])
    
    print("✅ All packages installed!")

def load_llama4_model():
    """Load Llama 4 model locally."""
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from huggingface_hub import login
    import os
    
    print("🦙 Loading Llama 4 model...")
    print("This will download ~25GB and may take 10-15 minutes on first run...")
    
    # Authenticate with HuggingFace
    try:
        # Try to get token from Colab secrets first
        from google.colab import userdata
        hf_token = userdata.get('HF_TOKEN')
        print("✅ Found HF_TOKEN in Colab secrets")
    except:
        # Fallback to manual token input
        print("⚠️  HF_TOKEN not found in Colab secrets")
        hf_token = input("Please enter your HuggingFace token: ")
    
    try:
        login(token=hf_token)
        print("✅ Successfully authenticated with HuggingFace")
    except Exception as e:
        print(f"❌ Authentication failed: {e}")
        return None, None
    
    # Try different Llama models in order of preference
    model_options = [
        "meta-llama/Llama-4-Scout-17B-16E",  # Llama 4 Scout - what you have access to
        "meta-llama/Llama-3.2-3B",
        "meta-llama/Llama-3.2-1B", 
        "microsoft/DialoGPT-medium",  # Fallback option
        "gpt2-medium"  # Final fallback
    ]
    
    model_name = None
    for option in model_options:
        try:
            # Test if we can access this model
            from transformers import AutoConfig
            config = AutoConfig.from_pretrained(option, token=hf_token if "llama" in option.lower() else None)
            model_name = option
            print(f"✅ Will use model: {model_name}")
            break
        except Exception as e:
            print(f"⚠️  Cannot access {option}: {str(e)[:100]}...")
            continue
    
    if not model_name:
        print("❌ No accessible models found")
        return None, None
    
    try:
        # Load tokenizer
        print("Loading tokenizer...")
        tokenizer_kwargs = {"trust_remote_code": True}
        if "llama" in model_name.lower():
            tokenizer_kwargs["token"] = hf_token
            
        tokenizer = AutoTokenizer.from_pretrained(model_name, **tokenizer_kwargs)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # Load model with memory optimization for Colab
        print(f"Loading model (this takes time)... Model size: {model_name}")
        model_kwargs = {
            "torch_dtype": torch.float16,
            "device_map": "auto",
            "trust_remote_code": True,
        }
        
        # Use more aggressive quantization for larger models
        if "17B" in model_name or "Scout" in model_name:
            model_kwargs["load_in_4bit"] = True  # 4-bit for 17B model
            print("Using 4-bit quantization for large model")
        else:
            model_kwargs["load_in_8bit"] = True  # 8-bit for smaller models
            print("Using 8-bit quantization")
            
        if "llama" in model_name.lower():
            model_kwargs["token"] = hf_token
            
        model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)
        
        print(f"✅ Loaded {model_name}")
        print(f"Device: {next(model.parameters()).device}")
        
        return model, tokenizer
        
    except Exception as e:
        print(f"❌ Failed to load {model_name}: {e}")
        print("\nTroubleshooting:")
        if "403" in str(e) or "not in the authorized list" in str(e):
            print("🔒 ACCESS REQUIRED:")
            print("1. Go to https://huggingface.co/meta-llama/Llama-3.2-3B")
            print("2. Click 'Request access to this model'")
            print("3. Fill out the form and wait for approval (usually takes a few minutes)")
            print("4. Once approved, restart this script")
            print("\nAlternatively, the script will try other available models...")
        else:
            print("1. Make sure you're using GPU runtime in Colab")
            print("2. Restart runtime if you get memory errors") 
            print("3. Try the smaller 1B model instead")
        return None, None

def analyze_text_llama4(text, text_name, model, tokenizer):
    """Analyze text compression using Llama 4."""
    import torch
    
    print(f"\n{'='*60}")
    print(f"ANALYZING: {text_name}")
    print(f"{'='*60}")
    print(f"Text length: {len(text)} characters")
    
    # Tokenize text
    print("Tokenizing...")
    tokens = tokenizer.encode(text, return_tensors='pt')
    num_tokens = tokens.shape[1]
    print(f"Number of tokens: {num_tokens}")
    
    # Handle long sequences
    max_length = 2048  # Conservative for Colab
    if num_tokens > max_length:
        print(f"⚠️  Truncating to {max_length} tokens")
        tokens = tokens[:, :max_length]
        num_tokens = max_length
        text = text[:int(len(text) * max_length / num_tokens)]
    
    # Move to GPU
    device = next(model.parameters()).device
    tokens = tokens.to(device)
    
    # Process token by token with progress
    total_bits = 0
    detailed_data = []
    
    print("Calculating token probabilities...")
    start_time = time.time()
    
    for i in range(num_tokens - 1):
        if i % 50 == 0:
            elapsed = time.time() - start_time
            percent = (i / (num_tokens - 1)) * 100
            eta = elapsed * (num_tokens - 1 - i) / max(i, 1)
            print(f"Progress: {percent:.1f}% ({i}/{num_tokens-1}) ETA: {eta/60:.1f}m", end='\r')
        
        # Get context up to current position
        context = tokens[:, :i+1]
        
        with torch.no_grad():
            outputs = model(context)
            logits = outputs.logits
            
            # Get probabilities for next token
            next_token_logits = logits[0, -1, :]
            log_probs = torch.log_softmax(next_token_logits, dim=0)
            
            # Get actual next token
            actual_token_id = tokens[0, i+1].item()
            actual_token = tokenizer.decode([actual_token_id])
            
            # Get log probability and convert to bits
            logprob_actual = log_probs[actual_token_id].item()
            surprisal_bits = -logprob_actual / math.log(2)
            
            total_bits += surprisal_bits
            
            # Get top 5 predictions for analysis
            top_probs, top_indices = torch.topk(torch.softmax(next_token_logits, dim=0), 5)
            top_tokens = [tokenizer.decode([idx.item()]) for idx in top_indices]
            top_logprobs = [log_probs[idx].item() for idx in top_indices]
            
            detailed_data.append({
                'position': i + 1,
                'token': actual_token,
                'surprisal': surprisal_bits,
                'probability': math.exp(logprob_actual),
                'logprob': logprob_actual,
                'top_tokens': top_tokens,
                'top_logprobs': top_logprobs
            })
        
        # Clear cache every 100 tokens
        if (i + 1) % 100 == 0:
            torch.cuda.empty_cache()
    
    elapsed_total = time.time() - start_time
    print(f"\nCompleted in {elapsed_total/60:.1f} minutes!")
    
    # Calculate summary
    num_predictions = len(detailed_data)
    avg_bits_per_token = total_bits / num_predictions if num_predictions > 0 else 0
    bits_per_char = total_bits / len(text)
    compression_ratio = 8.0 / bits_per_char if bits_per_char > 0 else 0
    
    summary = {
        'text_name': text_name,
        'text_length': len(text),
        'num_tokens': num_tokens,
        'num_predictions': num_predictions,
        'total_bits': total_bits,
        'bits_per_token': avg_bits_per_token,
        'bits_per_char': bits_per_char,
        'compression_ratio': compression_ratio,
        'model': tokenizer.name_or_path,
        'processing_time_minutes': elapsed_total / 60,
        'tokens_per_second': num_predictions / elapsed_total,
        'timestamp': datetime.now().isoformat()
    }
    
    # Print results
    print(f"\nLlama 4 Analysis Results for {text_name}:")
    print(f"  Total bits: {total_bits:.0f}")
    print(f"  Bits per token: {avg_bits_per_token:.2f}")
    print(f"  Bits per character: {bits_per_char:.2f}")
    print(f"  Compression ratio: {compression_ratio:.1f}x")
    print(f"  Processing speed: {num_predictions/elapsed_total:.1f} tokens/second")
    
    # Show most/least surprising tokens
    if detailed_data:
        sorted_data = sorted(detailed_data, key=lambda x: x['surprisal'])
        
        print(f"\nMost predictable tokens:")
        for i, token_data in enumerate(sorted_data[:3]):
            print(f"  {i+1}. {repr(token_data['token'])} "
                  f"({token_data['surprisal']:.2f} bits, P={token_data['probability']:.4f})")
        
        print(f"\nMost surprising tokens:")
        for i, token_data in enumerate(sorted_data[-3:]):
            print(f"  {i+1}. {repr(token_data['token'])} "
                  f"({token_data['surprisal']:.2f} bits, P={token_data['probability']:.4f})")
    
    return summary, detailed_data

def main():
    """Main execution function."""
    print("🚀 LLAMA 4 COMPRESSION ANALYSIS FOR GOOGLE COLAB")
    print("="*60)
    
    # Install requirements
    install_requirements()
    
    # Load model
    model, tokenizer = load_llama4_model()
    if model is None:
        print("❌ Could not load model. Exiting.")
        return
    
    # Run analysis on all text samples
    all_results = {}
    
    for text_name, text_content in TEXT_SAMPLES.items():
        if not text_content.strip():
            print(f"⚠️  Skipping empty text: {text_name}")
            continue
            
        try:
            summary, detailed_data = analyze_text_llama4(
                text_content, text_name, model, tokenizer
            )
            all_results[text_name] = {
                'summary': summary,
                'detailed_data': detailed_data[:100]  # Keep first 100 for space
            }
            
        except Exception as e:
            print(f"❌ Error analyzing {text_name}: {e}")
            continue
    
    # Save results
    results_json = json.dumps(all_results, indent=2, default=str)
    
    print(f"\n{'='*60}")
    print("🎉 ALL ANALYSIS COMPLETE!")
    print(f"{'='*60}")
    
    print("\n📊 SUMMARY OF ALL TEXTS:")
    for text_name, result in all_results.items():
        summary = result['summary']
        print(f"{text_name}:")
        print(f"  Compression ratio: {summary['compression_ratio']:.1f}x")
        print(f"  Bits per character: {summary['bits_per_char']:.2f}")
        print(f"  Processing time: {summary['processing_time_minutes']:.1f} minutes")
    
    # Save to file (for download from Colab)
    with open('llama4_compression_results.json', 'w') as f:
        f.write(results_json)
    
    print(f"\n💾 Results saved to 'llama4_compression_results.json'")
    print("📥 You can download this file from Colab's file browser")
    
    return all_results

# ============================================================================
# RUN THE ANALYSIS
# ============================================================================

if __name__ == "__main__":
    results = main()