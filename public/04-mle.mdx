# Minimizing KL divergence <a id="updating-beliefs"></a>

In this chapter, we'll explore what happens when we minimize the KL divergence between two distributions. Specifically, we'll look at the following problem that often arises in statistics:

<Block>
We begin with a distribution $p$ that captures our best guess of the "truth". We want to find a simpler model for it from a family of distributions $Q$. 
</Block>

We will solve this problem by minimizing $D(p, q)$ over all $q \in Q$. If you remember the previous chapters, this should make a plenty of sense! We will also see how a special case of this approach is equivalent to the _maximum likelihood principle_ (MLE) in statistics.

Let's dive into the details!

<KeyTakeaway>
A good model $q$ for a distribution $p$ has small $D(p,q)$. A special case of this is the maximum likelihood principle. 
</KeyTakeaway>

## ü•î Simpler models by minimizing KL <a id="mle"></a>

Imagine we have some data‚Äîfor instance, the 16 foot length measurements $X_1, \dots, X_{16}$ from [our statistics riddle](00-riddles#statistics). Assuming the order of the data doesn't matter, it's convenient to represent these data as an empirical distribution $p$, where each outcome is assigned a probability of $1/16$. This is what we call an _empirical distribution_. In some sense, this empirical distribution is the "best fit" for our data; it's the most precise distribution matching what we have observed. 

However, this is a terrible predictive model‚Äîit assigns zero probability to outcomes not present in the data! If we were to measure the 17th person's foot, the model is going to be "infinitely surprised" by this since it assigns zero probability to this outcome.<Footnote>Remember, we have discussed in [the first chapter](01-kl_intro) how we can measure the surprisal of seeing the outcome $x$ as $\log \frac{1}{p(x)}$.</Footnote> We need something better.

One common approach is to first identify a family of distributions $Q$ that we believe would be a good model for the data. For example, we might suspect that foot lengths follow a Gaussian distribution.<Footnote>Why Gaussians? We'll see soon in the next few chapters.</Footnote> In this case, $Q$ would be the set of all Gaussian distributions with varying means and variances. 

Now, how do we pinpoint the optimal parameters $\mu$ and $\sigma^2$ to best match the empirical distribution? KL divergence offers a natural solution: Choose $\mu$ and $\sigma$ to minimize $D(p, N(\mu,\sigma^2))$<Footnote> $N(\mu, \sigma^2)$ denotes the Gaussian distributionwith mean $\mu$ and variance $\sigma^2$. </Footnote>. More broadly, if we have a collection of candidate distributions $Q$ (like all Gaussians), we select the new model <Math math ="q_{\textrm{new}}\in Q" /> like this:

<Math id="kl_new_model" displayMode={true} math = "q_{\textrm{new}} = \argmin_{q\in Q} D(p, q)" />

Why does this make sense? Recall that KL divergence is designed to measure how well a model $q$ approximates the true distribution $p$. More specifically, it is the rate of how quickly a Bayesian detective learns that the true distribution is $p$, not $q$. So, minimizing KL simply means selecting the best "imposter" that it takes the longest to separate from the truth. That's pretty reasonable!


Visually, I like to think of it like this. First, imagine a "potato" of all possible distributions. The empirical distribution $p$ is a single lonely point within it. Then, there's a subset of distributions $Q$ that we believe would be a good model for the data -- I drew it as a kind of a "half-plane" in the picture below since it's parameterized by two values: $\mu$ and $\sigma^2$. Although KL divergence is not, technically speaking, a distance between distributions, because it's asymetric, it's close enough to think of it as a distance. Minimizing KL is then like finding the closest point in $Q$ to $p$; or, if you want, it's like projecting $p$ onto $Q$.


![KL divergence potato](04-mle/potato_mle.png)


## ‚ùå Cross-entropy loss <a id="cross_entropy"></a>

Let's write down our minimization formula again, in its full glory: 
<Math id="kl_new_model_close" displayMode={true} math = "q_{\textrm{new}} = \argmin_{q\in Q} D(p, q) = \argmin_{q\in Q} \sum_x p(x) \cdot \log \frac{p(x)}{q(x)}" />

Here, $x$ ranges over all possible values in the distribution. We can simplify this equation a bit. Let's remember that we can split the KL divergence into cross-entropy and entropy: 

<Math displayMode={true} math=" \underbrace{\argmin_{q\in Q} \sum_x p(x) \cdot \log \frac{p(x)}{q(x)}}_{\textrm{KL divergence}} = \underbrace{\sum_x p(x) \cdot \log \frac{1}{q(x)}}_{\textrm{Cross-entropy}} - \underbrace{\sum_x p(x) \cdot \log \frac{1}{p(x)}}_{\textrm{Entropy}}. " />

In this particular case, the _second_ term on the right - the entropy of $p$ - turns out to be completely irrelevant. Remember, we are optimizing over distributions $q \in Q$, and this term has the same value for all of them. So, __optimizing KL is the same as optimizing the cross-entropy__: 

<Math id="cross-loss" displayMode={true} math = "q_{\textrm{new}} = \argmin_{q\in Q} D(p, q)  = \argmin_{q\in Q} H(p, q) =  \argmin_{q\in Q} \sum_x p(x) \cdot \log \frac{1}{q(x)}" />

Minimizing cross-entropy is a super-important approach in statistics and machine learning. Let's go through several examples to see this in action. 

Importantly, none of the following examples are using <EqRef id="cross-loss"/> as some kind of black box formula. That's because this equivalence is not really some kind of rocket science - saying 'we minimize KL divergence' and 'we optimize cross-entropy' is more like different accents. 

First, let's play a bit more with fitting feet lengths by a Gaussian. 

<Expand headline = "Example: Fitting a Gaussian">
In the following widget, you can see how we can fit an empirical distribution over 16 numbers with a Gaussian. You can see the cross-entropies of various fits and test that the fit that minimizes it looks reasonable. 

<GaussianFitWidget />

Let's see how we can derive an actual formula for $\mu$ in the best-fitting Gaussian from the cross-entropy loss. We have 
<Math displayMode={true} math = "H(p, N(\mu, \sigma^2)) = \sum_{i = 1}^{16} \frac{1}{16} \cdot \log \frac{1}{1/\sqrt{2\pi\sigma^2} \cdot e^{-(x_i - \mu)^2/(2\sigma^2)}}"/> 

This is the formula for cross-entropy. The formula for KL divergence would have $1/16$ instead of $1$ in the numerator; it does not make a difference if we just try to make the expression as small as possible. 

Minimizing this expression is a standard math analysis problem, and we will do the math [in a later chapter](07-machine_learning). The upshot is that the $\mu, \sigma$ making the value the least are $\mu = \sum_{i = 1}^{16} \frac{1}{16} \cdot x_i$ (i.e., the sample mean), and $\sigma^2 = \sum_{i = 1}^{16}\frac{1}{16} \cdot (x_i - \mu)^2$. 

This is also how the widget above computes the best-fitting Gaussian. 

Final remark: You may notice that the cross-entropy in the widget is sometimes negative, although cross-entropy should clearly be positive by our definition. That's because we are using probability densities (for $q$) in the formula, not probability. Let's not worry about why this is legit too much, there's an [advanced block](05-max_entropy#continuous) later on if you are eager for details. 

</Expand>

Now, let's return to our [Intelligence test riddle](00-riddles#intelligence) and see how neural networks are trained. 

<RiddleExplanation id = "intelligence">
Let's talk about how large language models (LLMs) are trained. Do you still remember the Intelligence test widget? 


<LetterPredictionWidget/>

This widget is essentially also how LLMs are trained. The task they are optimized for is that given a text snippet, they should output a probability distribution over what the next letter might be.<Footnote>In practice, it would be a _token_, not letter. If you don't know what a token is, don't worry. If you do, note that above widget converts token probabilities of GPT2/Llama 4 to letter probabilities. </Footnote> Basically, to train, you first collect many text snippets (I collected some random<Footnote>All pages I took are about a topic that starts with 'A'. If you played the game for a bit longer, you may have learned a lot of Abraham Lincoln, anarchism, and Aristotle. Also, I am sorry for spoilers from Atlas Shrugged... üòÖ </Footnote> Wikipedia text for the widget). Then, consider some pair of a text snippet + the following letter, like this one: 

<Math displayMode={true} math = "(\textsf{The Abraham Lincoln National Cemetery is located in \_}, \,\, \textsf{E})"/>

Given the snippet, the neural net outputs the distribution predicting the next letter, like this:

<Math displayMode={true} math = "q_{LLM}(\textrm{letter} \,|\, \textsf{\dots is located in \_}) = \{\textsf{E}: 0.39,\, \textsf{A}: 0.34,\, \textsf{I}: 0.12, \dots\}"/>

We can now understand one important aspect of the training process -- setting up the _loss function_. This is a function that takes the actual next letter (<Math math = "\textsf{E}"/>) and the distribution $q_{LLM}$, and it outputs a single number -- how well the distribution is matching the truth. If the neural net nails it (<Math math = "q_{LLM} = \{\textsf{E}: 1.00 \}"/>), the loss should be zero, and worse guess means larger loss. 

Although training LLMs is pretty complicated, it's ultimately just using gradient descent to make the loss function as small as possible. So, we should better do a good job picking the right one!

Fortunately, setting up loss functions for neural nets is fundamentally not much more complicated than fitting numbers with a Gaussian distribution. We can still use the same approach - just minimize KL divergence between "the truth" and our model for it. Once we work out what $p$ is and what $q$ is, we are done! 

The simplest way to set up $p$ and $q$ is like in the table below. To define $p$, we use the empirical distribution $p_{emp}(\textrm{letter} | \textrm{snippet})$ that simples gives $100\%$ probability to the actual letter following the snippet. For the model $q$, we take the distribution that the net outputs.

<SnippetDistributionTableWidget/>

Well, to be precise, it's a bit more complicated than that since both $p$ and $q$ should not be just conditional distributions of letter given snippet, they should be the _joint distributions_ over what we see. So, the definition of $p$ is 

<Math displayMode={true} math = "p(\textrm{snippet}, \textrm{letter}) = p_{emp}(\textrm{snippet}) \cdot p_{emp}(\textrm{letter} | \textrm{snippet})"/>

Here, the distribution $p_{emp}(\textrm{snippet})$ is simply the empirical, uniform distribution over the snippets. In our case, it would assign the probability of $\frac{1}{1000}$ to each of 1000 snippets I took from Wikipedia. 

The distribution $q$ is analogously this:
<Math displayMode={true} math = "q(\textrm{snippet}, \textrm{letter}) = p_{emp}(\textrm{snippet}) \cdot q_{LLM}(\textrm{letter} | \textrm{snippet})"/>

At this point, we are done. Now when we know what $p$ and $q$ are, it remains to write down the KL formula and run the math autopilot to work out what exactly our we supposed to optimize. We write: 

<Math displayMode={true} math = "D(p, q) = \sum_{(\textrm{snippet}, \textrm{letter})} p_{emp}(\textrm{snippet}) \cdot p_{emp}(\textrm{letter} | \textrm{snippet}) \cdot \log \frac{p_{emp}(\textrm{snippet}) \cdot p_{emp}(\textrm{letter} | \textrm{snippet})}{p_{emp}(\textrm{snippet}) \cdot q_{LLM}(\textrm{letter} | \textrm{snippet})}"/>

where the sum goes over all pairs (snippet, letter) in our dataset. This is a mouthful, but we can simplify drastically. Plugging in $p_{emp}(\textrm{snippet}) = 1/n$ for $n$ the number of snippets, and $p_{emp}(\textrm{letter} | \textrm{snippet}) = 1$ for the actual letter, and zero otherwise, we get: 

<Math displayMode={true} math = "D(p, q) = \sum_{\textrm{snippet}}  \frac{1}{n} \cdot \log \frac{1}{q_{LLM}(\textrm{actual letter} | \textrm{snippet})}"/>

This is really as simple as it gets. To train a neural net, you look at the distribution it outputs on a snippet, take the probability <Math math = "q_{LLM}(\textrm{actual letter} | \textrm{snippet})" /> that the net assigns to the actual letter following the snippet, and compute the surprisal (i.e., $q \rightarrow \log 1/q$). You just keep summing surprisals for the snippets in your dataset. The average is the score that we are trying to push down during LLM training. 

So, no wonder that Llama 4 has probably beaten you in the game<Footnote>In a game, I did not ask you to predict the whole distribution for the next letter, as that would be pretty tiring. So I computed the overall score by summing expressions $\log k$, where $k$ is the number of guesses until the correct letter. You can view this as an 'optimistic' bound on what your true cross-entropy score would be. If you guessed the correct letter after $k$ trials, surely you would give that letter probability at most $1/k$ if I asked you for the whole distribution! To compare apples with apples, I also compute this 'optimistic cross-entropy loss' for the LLMs. </Footnote> - __being good at the game is it's entire shtick__! All of the knowledge / intelligence in the model accumulated inside because it helps the model to be good at this game. 

In fact, this is a point I wanted to make when selecting three examples snippets for the table above. Since the text snippets I collected were cut at a random place, many of them are cut in the middle of some obvious English word, so they just test whether you know English. But, from time to time, guessing the next letter requires some nontrivial knowledge. 

In the first snippet, you have to know that Lincoln national cemetery is in $\textsf{Elwood}$. The next word in the second widget is $\textsf{Achilles}$. To guess that, you have to know Greek mythology. Llama guesses 't', because of $\textsf{Thetys}$ - the wife of Peleus referenced in the text by 'her'. Finally, the last snippet continues <Math math = "\textsf{virtue to selfishness. }" /> Predicting the next letter well requires knowing Rand's philosophy (though distinguishing between <Math math = "\textsf{virtue to selfishness}"/> and <Math math = "\textsf{vice to victimhood}"/> remains a hard call). 

<ImageGallery images={[{src: "04-mle/twitter_volcano.png", alt: "volcano"}]} width="75%" caption="Taken from the [What If book](https://what-if.xkcd.com/34/). "/>

</RiddleExplanation>

Let's also see how we can use cross-entropy score to grade experts from our [prediction riddle](00-riddles#predictions). 

<RiddleExplanation id="predictions"><a id="application-evaluating-predictions"></a>

Recall the riddle: We asked experts to predict future events‚Äîhow do we score them?

<ImageGallery images={[{src: "00-introduction/questions.png", alt: "questions"}]} width="75%" />


### üåà Ideal KL score

To grade predictions, let $p = (p_1, \dots, p_n)$ be the true probabilities of the events we ask our experts about. Let's also focus on a particular expert and let $q$ be what they predicted. It for sure seems like a good idea to give the expert the score (loss) <Math displayMode={false} math="D(p,q)" />. 

This is what this chapter is about -- minimizing KL is a good way to go, since ultimately it's about finding a model that's the hardest to distinguish from the truth by Bayes' rule. 

If all $n$ events we are predicting are independent,<Footnote>They of course never are, but here we will always assume they are. </Footnote> the formula looks like this:

<Math displayMode={true} math="D(p,q) =
\sum_{i = 1}^n D(p_i, q_i)
= \sum_{i = 1}^n  \left(
    p_i\log\frac{p_i}{q_i} + (1-p_i)\log\frac{1-p_i}{1-q_i}
    \right)" />


Ok, there's a huge problem with this score. Can you spot it?

### üéØ Cross-entropy score

The problem: we have no clue what the "true" probabilities $p = (p_1, \dots, p_n)$ are! <Footnote> We could go down a philosophical rabbit hole about whether "true" probability even is even a meaningful concept. But let's not. </Footnote> All we know is what actually happened - the ground-truth row in the picture above. Technically speaking, we know an empirical distribution $\hat{p}$ where each $\hat{p}_i$ is either 0 or 1‚Äîthe whole distribution is concentrated on the single list of outcomes we saw.

So, we can only compute the KL score with $\hat{p}$, not $p$. Let's see how the formula looks like: 

<Math displayMode={true} math="
D(\hat{p}, q) =
\sum_{i = 1}^n  \left(
    \hat{p}_i\log\frac{\hat{p}_i}{q_i} + (1-\hat{p}_i)\log\frac{1-\hat{p}_i}{1-q_i}
    \right)
"/>

We can actually simplify the formula a bit. Either you can observe algebraically, that since <Math math = "\hat{p}_i \in \{0,1\}"/>, we have <Math math = "\hat{p}_i\log\frac{\hat{p}_i}{q_i} = \hat{p}_i\log\frac{1}{q_i}" /> (recall that <Math math = "0\log 0 = 0"/>). Or, much more fancily, you can notice that since the entropy of <Math math = "\hat{p}"/> is literally zero, the KL divergence equals cross-entropy:

<Math displayMode={true} math="
D(\hat{p}, q) = H(\hat{p}, q) =
\sum_{i = 1}^n  \left(
    \hat{p}_i\log\frac{1}{q_i} + (1-\hat{p}_i)\log\frac{1}{1-q_i}
    \right)
"/>

This is the _cross-entropy score_ used in forecasting tournaments. Forecasting community typically calls it the [Log-score](https://forecasting.wiki/wiki/Log_score), though. 

### üë∂ Example

Try the log-score on our example! You can see how it really doesn't like the $99\%$-confidence failed prediction. On the other hand, it's too early to compare the second and the third expert. Another popular score is called brier score, it's just the so-called mean squared error or $\ell_2$ metric (i.e., if you predicted 0.8 probability and the event happens, your score is $(1-0.8)^2 = 0.04$). Brier score does not really care whether your failed prediction had probability of $0.9$ or $0.9999$, so it doesn't penalize the first expert for his overconfidence. 

<ExpertRatingWidget
    title="Comparing Scoring Methods"
    showBrierScore={true}
  />

### ‚ö†Ô∏è Connection to the idealized score
Feel free to skip this one. 

Let's dig into how our cross-entropy score relates to our idealized KL score. Technically speaking, cross-entropy score is a random variable. I want you to imagine an obtruse probability space of what might have happened if butterflies flipped their wings a bit differently a few years ago. Only in this weird multiverse space we can talk about 'the true' probability $p_i$ that an event happens - because the event happened in some worlds in the butterfly multiverse, but not others. With this view, $\hat{p}_i$s are random variables - Each $\hat{p}_i$ has value 1 with probability $p_i$, otherwise it's 0. Our cross-entropy score is also a random variable attaining different values in different worlds. 

So what's the expected cross-entropy score? Since $E_p[\hat{p}_i] = p_i$, [linearity of expectation](https://brilliant.org/wiki/linearity-of-expectation/) gives us:

<Math displayMode={true} math="
E_p[H(\hat{p}, q)] =
\sum_{i = 1}^n  \left(
    p_i\log\frac{1}{q_i} + (1-p_i)\log\frac{1}{1-q_i}
    \right)
    = H(p, q)
"/>

Nice! Give experts lots of questions, and by the law of large numbers, their cross-entropy score with $\hat{p}$ is close to the idealized cross-entropy score with $p$. 

This is analogous to how [in the cross-entropy widget in an earlier chapter](02-crossentropy#cross), the wiggly line is different each time you run the experiment (cross-entropy with empirical distribution depends on the actual flips), but it tracks the dashed red line (the rate given by cross-entropy with the underlying distribution). 

Now remember that <Math id="entropy-relation" displayMode={true} math="D(p,q) = H(p,q) - H(p)" />. Since $H(p)$ is a constant if we compare two experts $q_1, q_2$, we can observe that $D(p,q_1) < D(p, q_2)$ if and only if $H(p, q_1) < H(p, q_2)$. So, comparing two experts using their cross-entropy is the same as comparing them using their KL (which we otherwise can't compute). Putting everything together:

_In the long run, comparing experts by the log-score is the same as comparing them by KL with the 'true' underlying distribution!_

In other words, although we can't compute the idealized KL divergence with the butterfly multiverse distribution $p$, the log-score rule behaves the same in the long run. Understanding this also helps us to understand the flip side: We can trust the log-score (and any other score) only if we ask the experts many questions. Otherwise, what we can compute - $H(\hat{p}, q)$ - may be too noisy to use as an approximation for what we actually care about - $H(p,q)$. 

<Expand headline="Example: Coin flipping">
Let's make this concrete. We flip a fair coin $N$ times ($p_1 = \dots = p_N = 1/2$). Expert 1 nails it ($q_1 = \dots = q_N = 1/2$), while Expert 2 is a bit off ($q'_1 = \dots = q'_N = 0.6$).

The idealized KL scores: 
<Math displayMode={true} math = "D(p, q) = D(p_1, q_1) = 0"/>
<Math displayMode={true} math = "D(p, q') = D(p_1, q'_1) \approx 0.03"/>

We can't compute these scores in practice, because in practice we would not know $p$. However, we can compute cross-entropy. The expected cross-entropy score is this: 
<Math displayMode={true} math = "E[H(\hat{p}, q)] = H(p) + D(p, q) = 1 + 0 = 1"/>
<Math displayMode={true} math = "E[H(\hat{p}, q')] = H(p) + D(p, q') = 1 + D(p_1, q'_1) \approx 1.03"/>

For large $N$, the law of large numbers says that the actual score we measure - $H(\hat{p}, q)$ - is likely close to the expected score $H(p, q)$. In that case, since both scores shift by the same amount, cross-entropy still picks the best expert just like KL would. 

However, this only works _in the long run_. Disentangling $1$ from $1.03$ reliably would require about $N \approx 1000$ questions. That's why the original riddle does not try to pretend we can actually say which expert is good. With just 5 predictions, any scoring is going to be pretty noisy. 
</Expand>

</RiddleExplanation>


## üïµÔ∏è Maximum likelihood principle <a id="mle"></a>

We already understand that there's not much difference between minimizing cross-entropy or KL divergence between $p$ and $q$, whenever $p$ is fixed. There's one more equivalent way to think about this. Let's write the cross-entropy formula once more: 

<Math id="cross-loss" displayMode={true} math = "q_{\textrm{new}} =  \argmin_{q\in Q} H(p, q) =  \argmin_{q\in Q} \sum_x p(x) \cdot \log \frac{1}{q(x)}" />

In many scenarios, $p$ is literally just the uniform, empirical distribution over some data points $x_1, \dots, x_n$. In those cases, we can just write: 

<Math id="cross-loss-empirical" displayMode={true} math = "q_{\textrm{new}} =  \argmin_{q\in Q} \sum_{i = 1}^n \log \frac{1}{q(x_i)}" />

Another way to write this is:

<Math id="mle" displayMode={true} math = "q_{\textrm{new}} =  \argmin_{q\in Q} \prod_{i = 1}^n \frac{1}{q(x_i)} = \argmax_{q\in Q} \prod_{i = 1}^n q(x_i)" />

The expression $q(x_i)$ is typically called _likelihood_, because this is how we typically call $p(\textrm{data} | \textrm{hypothesis})$. Then, the product $\prod_{i = 1}^n q(x_i)$ is the overall likelihood of all of our data under the model. So, __minimizing the cross-entropy is equivalent to maximizing the likelihood__. 

The methodology of selecting $q$ that maximizes $\prod_{i = 1}^n q(x_i)$ is called the _maximum likelihood principle_ and it's one of the most important workhorses in statistics and machine learning. We can now see that it's really the same as our methodology of minimizing KL divergence (ours is a bit more general, we can use it for non-uniform $p$). 

In a sense, this is not surprising at all. Let's remember how we defined KL divergence in the first chapter. It was all about the Bayesian detective trying to distinguish the truth from the imposter. But look, the detective accumulates the evidence literally by multiplying his current probability distribution by the likelihoods. KL divergence / cross-entropy / entropy is just a useful language to talk about this since it's often easier to talk about "summing stuff" instead of "multiplying stuff". 

So, indeed, minimizing cross-entropy is equivalent to maximizing the overall likelihood. My hope is that the preceding chapters give you enough intuition so that this is not just some kind of black-box algebraic trick. It's more like two languages talking about the same thing. Either you like to write likelihoods and keep multiplying them (maximum likelihood principle, used in statistics), or you like to take logarithms, talk about bits, and keep summing them (cross-entropy, used in machine learning or information theory). <Footnote>If you try to compute the maximum likelihood in some concrete case (like the Gaussian example in this chapter), the first thing after writing $\max \prod_{i = 1}^n q(x_i)$ is typically to take $\log$ and write $\max \sum_{i = 1}^n \log q(x_i)$. This expression is called _log-likelihood_ in statistics, but we now understand it's basically the same cross-entropy (up to some details like having the opposite sign). On the other hand, in machine learning, we typically optimize the cross-entropy $H(p,q)$. But sometimes people convert it to the so-called [perplexity](https://en.wikipedia.org/wiki/Perplexity) defined as $\textrm{perplexity}(p, q) = 2^{H(p, q)}$. We now understand that this is basically the same as looking at the likelihood (except perplexity is not the product of all likelihoods, but their geometrical average, and also 1 over that). </Footnote>


In this chapter, we have seen several definitions, applied to a few different problems in a somewhat different ways. But there's a single versatile principle that underlies all the examples. Algebraically, we can think of it as: __If you have to choose a model for $p$, try to make $D(p,q)$ small.__ But ultimately, I think it's useful if you can, in your head, compile this principle down to: __A good model is hard to distinguish from the truth by a Bayesian detective.__




## üöÄ What's next? <a id="next-steps"></a>

In the [next chapter](05-max_entropy), we will see what happens if we minimize the _first_ parameter in $D(p, q)$. 




