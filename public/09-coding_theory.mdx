# Coding Theory

KL divergence and entropy are deeply connected to coding theory. It would be distasteful not to say anything about the connection. But it's also not so relevant to the main story, hence this bonus chapter. 

The goal of the chapter is to explain what's happening in the riddle about [Wikipedia compression](00-introduction#wikipedia), we discuss it at the end. 


## Code intro
Say you've got a long DNA string built from letters \{A,C,G,T\}. You want to store it using as little disk space as possible.  

Here's the most basic plan: We assign a binary code for each letter and encode the string using that code. 

For example, we can use <Math math = "A \rightarrow \textsf{00}, C \rightarrow \textsf{01}, G \rightarrow \textsf{10}, T \rightarrow \textsf{11} " />. The string gets stored using just 2 bits per letter. Done! 

Can we do better? Sometimes, yes! Here's a riddle: try to build a code that encodes the following string with 1.75 bits per letter on average. 

<BuildYourOwnCodeWidget /> 

.

.

.

SPOILER

.

.

.

Here's the solution: <Math math = "A \rightarrow \textsf{0}, C \rightarrow \textsf{10}, G \rightarrow \textsf{110}, T \rightarrow \textsf{111} " />. Since the letter frequencies in the text are <Math math = "\frac12, \frac14, \frac18, \frac18"/>, this encoding only uses 
<Math displayMode={true} id = "code-example" math = "\frac12 \cdot 1 + \frac14 \cdot 2 + \frac18 \cdot 3 + \frac18 \cdot 3 = 1.75" />
bits. Notice that when we use the code $A \rightarrow \textsf{0}$, no other letter can get a code-name starting with $\textsf{0}$. The reason is that such a code may not be decodable. E.g., if you use codes <Math  math = "\textsf{0}" /> and <Math  math = "\textsf{00}" />, how do you decode <Math  math = "\textsf{00}" />? 

OK, using shorter codes for more frequent letters makes sense, but what's the best way to do this? Coding theory knows the answer. Roughly speaking, it tells us to __try to give a letter with frequency $p_i$ a code-name of length $\log 1/p_i$__. 

For example, looking at <EqRef id = "code-example"/>, you can rewrite the left-hand side as: 
<Math displayMode={true} id = "code-example2" math = " \frac12 \cdot \log \frac{1}{1/2} + \frac14 \cdot \log \frac{1}{1/4} + \frac18 \cdot \log \frac{1}{1/8} + \frac18 \cdot \log\frac{1}{1/8}. " />
Every letter with frequency $p$ got code-name of length exactly $\log 1/p$! Notice that for general strings of length $N$, the length of codes satisfying the $p \rightarrow \log 1/p$ rule is this: 
<Math displayMode={true} math = "N \cdot \sum_{i = 1}^k p_i \cdot \log \frac{1}{p_i} = N \cdot H(p)." />
That is, if you manage to construct a code with $p \rightarrow \log 1/p$, you spend $H(p)$ bits per letter on average. 

If we implement our $p \rightarrow \log 1/p$-rule-of-thumb, we get what's known as Shannon's code. This code sorts the letter-frequencies down from the largest one, and assigns to each letter of frequency $p$ a code of length $\lceil \log \frac{1}{p} \rceil$ (because we can't have a code name with 1.5 bits). Here it is for English alphabet:

<ShannonCodeWidget />

You can notice that the number of bits per letter (also called _rate_) is a bit north of the entropy of the underlying distribution. This is because of the rounding $\log 1/p \rightarrow \lceil \log 1/p \rceil$. For example, if $p = 0.49$, we would like to give the letter a code name of length $\log \frac{1}{0.49} \approx 1.03$. Too bad that code names have to be integers and Shannon's code assigns it a code name of length $2$. As a general rule, Shannon's code can use up to 1 bit per letter more than what's the entropy. <Footnote>Try to work out the details of what's happening in Shannon's code. Especially: Why does Shannon's algorithm of assigning letters never runs out of available code names? </Footnote>

Unfortunately, there situations where Shannon's code is really almost 1 bit worse than the entropy. Take a string that uses just two characters $\textsf{A}, \textsf{B}$, but the frequency of $ \textsf{B}$ is close to zero. Then, the entropy is close to zero, but Shannon can't do better than 1 bit per letter. 

We can get rid of this annoying slack of one bit using the following trick. Think of constructing Shannon's code not for every letter, but for every _pair of letters_ (i.e., the alphabet grows from $26$ to $26^2$). If Shannon's code loses 1 bit per encoded letter-pair, it means it loses $1/2$ bits per actual letter! Doing this trick not for a pair of letters but a tuple of several letters can bring us arbitrarily close to a code that spends $H(p)$ bits per letter. 

So, there are codes that get arbitrarily close to $H(p)$ bits per letter, even if they are a bit less intuitive since they are encoding letter-tuples, not individual letters. This is essentially the first half of the most important theorem in coding theory - [Shannon's source coding theorem](https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem). The second part of the theorem says that we can't beat $H(p)$. Details in the expand box. 

<Expand headline = "Details of Shannon's source coding theorem">
To see why we can't do better than $H(p)$ bits per letter on average, we will have to understand [Kraft's inequlaity](https://en.wikipedia.org/wiki/Kraft%E2%80%93McMillan_inequality). This inequality says that if we work with alphabet with $k$ letters and we build a code with words of lengths $\ell_1, \dots, \ell_k$, then it has to be the case that 
<Math id = "kraft" displayMode = {true} math = "\sum_{i = 1}^k 2^{- \ell_i} \le 1. " />

To understand this inequality, go back to widgets above - whenever we give a code-name to a letter, like $e \rightarrow 000$, we can no longer use codewords that start with '000' for other letters, since that would create clashes. The node with '000' has to be a _leaf_. Some leaves 'take more space' than others - for example, using code name of '0' intuitively kills off half of the space of possibilities. 

A bit more formally, imagine continuing the full binary tree up to some very large depth $N$. Then a code word of length $\ell_i$ is above <Math math="2^{N - \ell_i}" /> of nodes at depth $N$. Since different code words cover disjoint intervals of depth-$N$ nodes, and there are $2^N$ nodes at depth $N$, it has to be the case for any valid code that 
<Math displayMode={true} math="\sum_{i = 1}^k 2^{N - \ell_i} \le 2^N" />
Divide by $2^N$ and you get Kraft's inequality <EqRef id = "kraft"/>. 

I actually like to think about Kraft's inequality as equality. Why? Well, if your code is such that the left-hand side is really smaller than $1$, then you are stupid. <Footnote>You can notice that Shannon's code above is 'stupid' since for English, it leaves some space at the right of the binary tree. In fact, Shannon's code is useful mostly for didactical reasons and in practice you would construct the code by [Huffman's algorithm](https://en.wikipedia.org/wiki/Huffman_coding). </Footnote> In the widget below, you can see how codes can be iteratively improved to get equality. 

<KraftInequalityWidget />


The reason why this is helpful is that I like to think about the numbers <Math math="q_i = 2^{-\ell_i}" /> as some kind of idealized probabilities. Widget above shows that we can pretty much assume that the numbers $q_i$ always sum up to 1. You can think about the numbers $q_i$ as the probability distribution _implied_ by your code-name lengths $\ell_i$. Intuitively, the code is optimized for the distribution $q$, not $p$. 

This setup with $p$ and $q$ is little bit like our discussions in the first two chapters. In fact, let's write down the fact that KL divergence between $p$ and $q$ is always nonnegative. Let's write it down in the form $H(p, q) \ge H(p)$:
<Math displayMode = {true} math="\sum_{i = 1}^k p_i \log \frac{1}{q_i} \ge \sum_{i = 1}^k p_i \log \frac{1}{p_i}" />
Plugging in <Math math="q_i = 2^{-\ell_i}" />, we get
<Math displayMode = {true} math="\sum_{i = 1}^k p_i \ell_i \ge H(p)." />
That is, the average code-name length is at least $H(p)$. This works for any code, hence the second part of [Shannon's source coding theorem](https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem). 
</Expand>

## Entropy intuition

Using the fact that good codes can achieve a rate close to entropy can give us a lot of intuition! Remember [how we defined](/02-crossentropy) entropy as the "average surprisal" of a distribution? We could also ask: if we keep sampling from the distribution, how many bits per result do we need to store them? 

For example, if we are flipping a fair coin and want to keep the results, there is nothing smarter then remembering a file like '0101100' where 0/1 corresponds to H/T. This is 1 bit per flip. 

But if we are flipping a coin where heads have only $0.01$ probability, there are better ways to store the results! For example, we could split the outcomes to batches of 10, and give a code-name to each of the $2^{10}$ possibilities in a batch. If we use a short code-name for the very probably outcome of 'TTTTTTTTTT', we can spare a lot of disk space!

The entropy of the coin flip is the rate of the best code, it's the speed of how much 'useful information' we are generating with our flips.  

With coding theory intuitions, cross-entropy also becomes also very natural: it's how many bits you need when data comes from $p$ but you are using the code that's optimized for a different distribution $q$. For example, in the top widget, you can construct the best code for 'AACATACG', but then run it on 'AAAAAAAA'. 

Finally, relative entropy is how much worse your mismatched code is compared to the optimal one. 


## Better ways of storing data


## Wikipedia riddle
We can now revisit the riddle about how much Wikipedia (and other texts) can be compressed. 


<CompressionWidget />


<Expand headline = "🌐 How large is Wikipedia?"><a id = "hutter"></a>
Back to [our Wikipedia riddle](00-introduction#wikipedia) and Hutter's compression challenge. 
Remember, the riddle is about how much can we compress a file with English Wikipedia. 

There are more approaches to compress text files. Let's go through them and you can see in the next widget how they fare for various types of data. 

- _Baseline_: The stadard way to store text files is UTF-8. Lying a bit, this format store each letter using 8 bits. <Footnote>Why is it lying? [UTF-8](https://en.wikipedia.org/wiki/UTF-8) itself is a beautiful example of good engineering inspired by coding theory. There are around 100 characters (English letters, digits) that are stored using 8 bits, fancier letters from reasonable alphabets are stored using 16 bits, and emojis like 😀 or hieroglyphs like 𓀀 take 32 bits. Classic coding theory—rare stuff gets longer codes. But English Wiki is mostly standard stuff, so 8 bits per letter it is. </Footnote>

- _Optimal letter-independent code_: We discussed above, how coding theory tells us what's the best way of compressing files if we don't want to use tricks like "'th' is usually followed by 'e'". We can treat the letters as independent and encode the using $H(p)$ bits per letter on average, where $p$ are the English letter frequencies. [In this widget above](../02-crossentropy#construction), we used those frequencies as an example; the entropy is a bit north of 4 bits. So, we can shrink the file almost by __2x__ just by using that different letters have different frequencies. 

- _Zipping_: Standard compression algorithms like those used in zip use codes, but also look for repeating patterns or take advantage of frequencies of letter pairs. They can compress English text up to a factor around __3x__.  

- _The best algorithms_ in Hutter's competition have compress by a factor of about __8x__. That's about 1 bit per letter.  

- _LLMs_: We know about algorithms that are arguably even better compressors - LLMs. Large Language Models are literally trained on being able to predict text - given a snippet of the text like "My name is A", LLM tries to predict the distribution $p$ of the next letter <Footnote>Well, token.</Footnote>. 

If we can predict the next letter of a text, this means you can also compress the text. The compression algorithm is this: Given text $s$ = "My name is A", we run LLM to guess the distribution $p_s$ of the next letter. Then, we use the best code for $p_s$ to store the actual next letter, say 'l'. 

If the actual next letter is 'l', then the surprisal of LLM upon seeing this letter is $\log 1/p('l')$. We can now save the letter 'l' to a file by using the best possible code encoding symbols with frequencies $p$; that is, we can store 'l' using $\log 1/p('l')$ bits of memory. The decoding algorithm is very similar, it runs the same LLM and always compute the next code $p_s$ used to decode the next letter. <Footnote>In practice, this gets more complicated whenever $\log 1/p('l')$ is not a whole number. We had the same issue in our coding theory discussion, let's not go into it. </Footnote>

The fun part (that we will understand better later on) is that LLMs are being trained to optimize the so-called cross-entropy loss - in a nutshell this means that if the next letter is 'l', we want the surprisal $\log 1/p('l')$ of the network to be as small as possible. Using our coding-theory intuitions - the surprisal is also the code length in the best code - we can view the training of LLMs as the training to compress English text as much as possible. 

[todo fill numbers]
It is thus maybe not so surprising that they are incredibly good at it! In the following experiments, GPT2 achieved similar compression to the winners of Hutter challenge and Llamma 3 was a good bit better. Wait a minute, how come then that the world record is 120MB and not ~50MB? The problem is that Hutter only wants you to compress 1GB of text, not the whole Internet. If you use GPT2 to compress a piece of text, then the size of the compressed file should also include the size of GPT2 itself. If Hutter's challenge was about compressing 1000GB-sized file, using LLMs would be a no-brainer, but, alas, Hutter created his challenge before the current AI paradigm based on working with _lots_ of data. 

Before playing with the widget, let me tell you about one of the coolest experiments I know of. Claude Shannon, who [invented information theory](https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication) in the late 40s, did [the following](https://www.princeton.edu/~wbialek/rome/refs/shannon_51.pdf) a few years later. He'd show people partial sentences and ask them to guess the next letter. This way, he figured that people can compress English to about 0.5 - 1 bits per letter, similarly to the GPT-2 performance. As far as I can say, this is the first experiment about next-token prediction; Claude Shannon was so based he did it 60+ years before it became cool! 

See compression factors of different types of text and different algorithms with the widget below. Notice how the compression ratio depends heavily on the structure and predictability of the text. <Footnote>Let me know what text I should add to the widget! </Footnote>


</Expand>
