# Minimizing KL divergence <a id="updating-beliefs"></a>

In this chapter, we will see what happens when we minimize KL divergence between two distributions. In fact, we will discuss two setups:

1. We start with a distribution $p$ and want to find a simpler model of it from a family of distributions $Q$. We can do so by minimizing $D(p, q)$ over $q \in Q$. This corresponds to the maximum likelihood principle (MLE) in statistics.

2. If we start with a distribution $q$ and want to find a better model of it from a family of distributions $P$, we do so by minimizing $D(p, q)$ over $p \in P$. This corresponds to the maximum entropy principle (MaxEnt) in statistics.

Let's see the details!

## Minimizing KL & Maximum Likelihood Estimation (MLE) <a id="kl-and-mle"></a>

Suppose we are given some dataâ€”say, the 16 foot length measurements $X_1, \dots, X_{16}$ from our statistics riddle. Assuming the order of the data is irrelevant, it's handy to think about those data as an empirical distribution $p$ that assigns each outcome a probability of $1/16$. This is also called an _empirical distribution_. In some sense, this empirical distribution is the "best fit" of our data. In another sense, it is a horrible model of it -- it assigns zero probability to outcomes that are not in the data, although if we look at length of the foot of the 17th guy, it'l like to be outside the data. So, we need another model.

One way to approach this is to first find a family of distributions $Q$ that we think are a good model for the data. For example, we might think that the distribution of foot lengths is Gaussian.<Footnote>Why Gaussians? We will see soon...</Footnote> So, we can take $Q$ to be the set of all Gaussian distributions with different means and variances. Visually, you could think about the potato representing all possible distributions -- $p$ is a point in the potato, and $Q$ is a subset of it, that kind of looks like a half-plane as it is parameterized by two parameters $\mu$ and $\sigma^2$.

![KL divergence potato](03-minimizing/potato_mle.png)

Now, suppose we believe that a normal distribution is a good model for these data. How do we choose the best parameters $\mu$ and $\sigma^2$ to match the empirical distribution? KL divergence offers a natural answer: Choose $\mu$ and $\sigma$ to minimize $D(p, N(\mu,\sigma^2))$<Footnote> $N(\mu, \sigma^2)$ denotes the gaussian with mean $\mu$ and variance $\sigma^2$. </Footnote>. More generally, if we have a set of candidate distributions $Q$ (e.g., all Gaussians), select the $q\in Q$ that minimizes $D(p,q)$.

Why does this methodology make sense? Remember, KL is supposed to measure how well the model $q$ approximates the data $p$, so minimizing KL really just means picking the best fitting model in $Q$. If you still remember the [first chapter](01-kl_intro), you may even "compile" KL down to Bayes' theorem: choosing $q$ to minimize $D(p,q)$ really means choosing $q$ that takes the longest time to distinguish from the truth $p$ by a Bayesian. Very reasonable!

### Maximum Likelihood principle <a id="kl-divergence-and-maximum-likelihood"></a>

Let's take a closer look at our minimize-KL approach. Let's focus on the most common case, where $p$ is literally the empirical distribution over $X_1, \dots, X_n$. To avoid cluttered notation, let's assume all $X_i$ are distinct<Footnote>Feel free to check that the general case is the same. </Footnote>. In that case, we can write the KL divergence we are minimizing as:

$$D(p,q) = \sum_{X_i\in\mathcal{X}} \frac{1}{n}\log\frac{1/n}{q(X_i)}$$

Splitting this into entropy and cross-entropy:

$$D(p,q) = \sum_{X_i\in\mathcal{X}} \frac{1}{n}\log\frac{1}{q(X_i)} - \sum_{X_i\in\mathcal{X}} \frac{1}{n}\log\frac{1}{n}$$

Note that the entropy term (the second sum) is constant with respect to $q$. <Footnote> In this special case of $p$ being uniform distribution, we even have $\sum \frac{1}{n}\log n = \log n$, but the important part is that it's a constant independent of $q$. </Footnote> Thus, minimizing KL divergence is equivalent to minimizing the cross-entropy:

$$\min_{q \in Q} \sum_{X_i\in\mathcal{X}} \frac{1}{n}\log\frac{1}{q(X_i)}$$

Here, $q(X_i)$ is the probability (or probability density) that the model assigns to the data point $X_i$. Dropping the constant factor $1/n$ and using the identity $\log(1/x)=-\log x$, this is equivalent to maximizing the product:

$$\max_{q \in Q} \prod_{X_i} q(X_i)$$

This expression has very clear meaning: it is the conditional probability of seeing the data $X_i$, provided that they have been sampled independently from the model distribution $q$. This kind of conditional probability is also called the "likelihood of $X$ under $q$".

So what's the conclusion? Our approach of minimizing KL divergence is equivalent (at least for empirical distributions) to maximizing the likelihood of the data under the model $q$. The latter is in fact a very common approach in statistics, and it is called the maximum likelihood estimation (MLE) principle. We just derived it from the KL divergence perspective!

## Finding a Better Model & Maximum Entropy Principle <a id="finding-better-model"></a>

So, minimizing $\min_{q \in Q} D(p,q)$ is pretty useful, what aboutn the other direction $\min_{p \in P} D(p,q)$? This is also extremely useful, so let's see an example of what it means. 


Let's say I want a probability distribution for how long it takes me to eat lunch. Maybe my first (very crappy) model of this is a uniform distribution between $0$ and $600$ minutes. 

How do I improve this model? I should get my hands on some empirical data, so let's say that I will observe my lunch times for a while. Recording the whole empirical distribution sounds like too much work, so let's say I just record the average time it takes me to eat lunch -- say, 15 minutes. 

Given this new information, how should I choose a better model? KL divergence suggests that I among all distributions $p$ with average of 15, I should pick $\arg\min_{p \in P} D(p,q)$, where $q$ is my original model (the uniform distribution).

Intuitively, I want to find a (true) distribution $p$ so that $q$ is as good model of it as possible. This time it's a bit harder to see clearly why this is the right thing to do, see this footnote for a detailed explanation. 
<Footnote>



 </Footnote>

## The Maximum Entropy Principle <a id="maximum-entropy-principle"></a>

Let's recall the second example. I want to have a distribution modeling how long it takes me to eat a lunch. Maybe it's rounded to minutes and it never took more than 10 hours, so let's say the domain of my distribution is $\{1, \dots, 600\}$.

As a first, probably very basic model, we can just take the uniform distribution as a prior guess $q_{unif}$. That is, we assign $1/600$ probability to each option. This principle of using uniform distribution when we don't know anything makes sense since uniform distribution is literally the only distribution that gives the same weight to each outcome. (The fancy name for this is the principle of indifference.)

Now let's say that we know a bit more, maybe that we want a distribution with average 15 minutes. In that case, for $P$ being the set of all distributions with average 15, KL divergence suggests finding $p \in P$ minimizing:

$$KL(p,q_\text{uniform})$$

Using our splitting of KL divergence:

$$KL(p,q_\text{uniform}) = \sum_i p_i\log p_i - \sum_i p_i\log (1/600)$$

Since the second term is a constant $\log (1/600) = \log(600)$, minimizing KL divergence is equivalent to maximizing the negative of the first term, which is the entropy of $p$:

$$H(p) = -\sum_i p_i\log p_i$$

This is the maximum entropy principle: given a set $P$ of distributions subject to certain constraints, choose the $p\in P$ that maximizes the entropy $H(p)$.

### The Principle of Indifference as a Prior <a id="principle-of-indifference"></a>

The maximum entropy principle can be seen as a natural extension of the principle of indifference. When we have no information, we use a uniform distribution (the distribution with maximum entropy). When we gain some information in the form of constraints (like knowing the mean), we choose the distribution that is as "uniform" as possible while satisfying those constraints.

In other words, the maximum entropy principle tells us to assume as little as possible beyond what we know. It's a formalization of Occam's razor for probability distributions.

## Connecting KL and Bayes' Theorem <a id="connecting-kl-and-bayes-theorem"></a>

One fascinating aspect of KL divergence is its deep connection to Bayesian reasoning. In fact, Bayes' theorem itself can be derived as a special case of the principle of minimizing KL divergence.

When deriving the minimum KL principle, we relied on an analogy with Bayes' theorem, essentially starting with $q$ and conditioning on new information. One can show that Bayesian updating is a special case of the minimum KL principle.

For example, consider a joint distribution (such as the table for weather and commuting). Upon learning that it is sunny, Bayesian updating dictates that we set $P(\text{sunny})=1$ while keeping the conditional distribution $P(\cdot|\text{sunny})$ unchanged (i.e., zeroing out the other rows and normalizing the sunny row).

According to the minimum KL principle, given the original distribution $q$ and the new data (that outcomes outside the sunny row have probability zero), the updated distribution $p$ is the one that minimizes $KL(p,q)$ while satisfying the constraint. In fact, choosing $p=q(\cdot|\text{sunny})$ achieves $KL(p,q)=0$ (the minimal possible value). Thus, the minimum KL principle endorses Bayesian updating.

In some sense, we can even say that the principle of "minimize KL whenever you can" and the principle of "use Bayes' theorem whenever you can" are kind of equivalent.

## Next Steps <a id="next-steps"></a>

In the next part, we'll explore more concrete modeling applications of these principles. We'll see how they apply to various fields like machine learning, statistical modeling, and information theory.
