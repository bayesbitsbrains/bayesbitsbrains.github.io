# Minimizing KL divergence <a id="updating-beliefs"></a>

In this chapter, we will see what happens when we minimize KL divergence between two distributions. In fact, we will discuss two setups:

1. We start with a distribution $p$ and want to find a simpler model of it from a family of distributions $Q$. We can do so by minimizing $D(p, q)$ over $q \in Q$. This corresponds to the maximum likelihood principle (MLE) in statistics.

2. If we start with a distribution $q$ and want to find a better model of it from a family of distributions $P$, we do so by minimizing $D(p, q)$ over $p \in P$. This corresponds to the maximum entropy principle (MaxEnt) in statistics.

Let's see the details!

## Minimizing KL & Maximum Likelihood Estimation (MLE) <a id="mle"></a>

Suppose we are given some dataâ€”say, the 16 foot length measurements $X_1, \dots, X_{16}$ from our statistics riddle. Assuming the order of the data is irrelevant, it's handy to think about those data as an empirical distribution $p$ that assigns each outcome a probability of $1/16$. This is also called an _empirical distribution_. In some sense, this empirical distribution is the "best fit" of our data. In another sense, it is a horrible model of it -- it assigns zero probability to outcomes that are not in the data, although if we look at length of the foot of the 17th guy, it'l like to be outside the data. So, we need another model.

One way to approach this is to first find a family of distributions $Q$ that we think are a good model for the data. For example, we might think that the distribution of foot lengths is Gaussian.<Footnote>Why Gaussians? We will see soon...</Footnote> So, we can take $Q$ to be the set of all Gaussian distributions with different means and variances. Visually, you could think about the potato representing all possible distributions -- $p$ is a point in the potato, and $Q$ is a subset of it, that kind of looks like a half-plane as it is parameterized by two parameters $\mu$ and $\sigma^2$.

![KL divergence potato](03-minimizing/potato_mle.png)

Now, suppose we believe that a normal distribution is a good model for these data. How do we choose the best parameters $\mu$ and $\sigma^2$ to match the empirical distribution? KL divergence offers a natural answer: Choose $\mu$ and $\sigma$ to minimize $D(p, N(\mu,\sigma^2))$<Footnote> $N(\mu, \sigma^2)$ denotes the gaussian with mean $\mu$ and variance $\sigma^2$. </Footnote>. More generally, if we have a set of candidate distributions $Q$ (e.g., all Gaussians), select the $q\in Q$ that minimizes $D(p,q)$.

Why does this methodology make sense? Remember, KL is supposed to measure how well the model $q$ approximates the data $p$, so minimizing KL really just means picking the best fitting model in $Q$. If you still remember the [first chapter](01-kl_intro), you may even "compile" KL down to Bayes' theorem: choosing $q$ to minimize $D(p,q)$ really means choosing $q$ that takes the longest time to distinguish from the truth $p$ by a Bayesian. Very reasonable!

### Maximum Likelihood principle <a id="mle"></a>

Let's take a closer look at our minimize-KL approach. Let's focus on the most common case, where $p$ is literally the empirical distribution over $X_1, \dots, X_n$. To avoid cluttered notation, let's assume all $X_i$ are distinct<Footnote>Feel free to check that the general case is the same. </Footnote>. In that case, we can write the KL divergence we are minimizing as:

<Math displayMode={true} math="D(p,q) = \sum_{X_i\in\mathcal{X}} \frac{1}{n}\log\frac{1/n}{q(X_i)}" />

Splitting this into entropy and cross-entropy:

<Math displayMode={true} math="D(p,q) = \sum_{X_i\in\mathcal{X}} \frac{1}{n}\log\frac{1}{q(X_i)} - \sum_{X_i\in\mathcal{X}} \frac{1}{n}\log\frac{1}{n}" />

Note that the entropy term (the second sum) is constant with respect to $q$. <Footnote> In this special case of $p$ being uniform distribution, we even have $\sum \frac{1}{n}\log n = \log n$, but the important part is that it's a constant independent of $q$. </Footnote> Thus, minimizing KL divergence is equivalent to minimizing the cross-entropy:

<Math displayMode={true} math="\min_{q \in Q} \sum_{X_i\in\mathcal{X}} \frac{1}{n}\log\frac{1}{q(X_i)}" />

Here, $q(X_i)$ is the probability (or probability density) that the model assigns to the data point $X_i$. Dropping the constant factor $1/n$ and using the identity $\log(1/x)=-\log x$, this is equivalent to maximizing the product:

<Math displayMode={true} math="\max_{q \in Q} \prod_{X_i} q(X_i)" />

This expression has very clear meaning: it is the conditional probability of seeing the data $X_i$, provided that they have been sampled independently from the model distribution $q$. This kind of conditional probability is also called the "likelihood of $X$ under $q$".

So what's the conclusion? Our approach of minimizing KL divergence is equivalent (at least for empirical distributions) to maximizing the likelihood of the data under the model $q$. The latter is in fact a very common approach in statistics, and it is called the maximum likelihood estimation (MLE) principle. We just derived it from the KL divergence perspective!

### Example <a id = "mle_for_mean_sigma"></a>

Let's say that we are given data $X_1, \dots, X_N$ (that we represent by its empirical distribution $p$) and want to find the best fitting Gaussian $N(\mu, \sigma^2)$. How should we choose $\hat\mu$ and $\hat\sigma^2$?

Here's what Maximum likelihood principle recommends: We should maximize the likelihood, or more conveniently, maximize the so-called log-likelihood / minimize cross-entropy:

<Math displayMode={true} math="\hat\mu, \hat\sigma^2 = \argmax_{\mu, \sigma^2} \sum_{i = 1}^N \log\left( \frac{1}{2\pi\sigma^2} e^{-\frac{(X_i-\mu)^2}{\sigma^2}} \right) = \argmin_{\mu, \sigma^2} 2N \cdot \log \sigma + \sum_{i = 1}^N \frac{(X_i-\mu)^2}{\sigma^2}"/>

There are several ways to solve this optimization problem. Probably the cleanest is differentiation: If we define $\mathcal{L}$ to be the above expression, then:
<Math displayMode={true} math = "\frac{\partial \mathcal{L}}{\partial \mu} = \frac{1}{\sigma^2} \sum_{i = 1}^N 2(X_i - \mu) "/>

Setting <Math displayMode={false} math="\frac{\partial \mathcal{L}}{\partial \mu} = 0"/> then leads to $\hat\mu = \frac{1}{N} \sum_{i = 1}^N X_i$.

Similarly,
<Math displayMode={true} math="\frac{\partial \mathcal{L}}{\partial \sigma} = 2N/\sigma -2  \sum_{i = 1}^N \frac{(X_i-\mu)^2}{\sigma^3}"/>

Setting <Math displayMode={false} math="\frac{\partial \mathcal{L}}{\partial \sigma} = 0"/> then leads to <Math displayMode={false} math="\hat\sigma = \frac{1}{N} \sum_{i = 1}^N (X_i - \mu)^2"/>

## Finding a Better Model & Maximum Entropy Principle <a id="finding-better-model"></a>

So, minimizing <Math math="\min_{q \in Q} D(p,q)" /> is pretty useful, what aboutn the other direction <Math math="\min_{p \in P} D(p,q)" />? This is also extremely useful, so let's see an example of what it means.


Let's say I want a probability distribution for how long it takes me to eat lunch. Maybe my first (very crappy) model of this is a uniform distribution between <Math math="0" /> and <Math math="600" /> minutes.

How do I improve this model? I should get my hands on some empirical data, so let's say that I will observe my lunch times for a while. Recording the whole empirical distribution sounds like too much work, so let's say I just record the average time it takes me to eat lunch -- say, 15 minutes.

Given this new information, how should I choose a better model? I claim that among all distributions <Math math="p" /> with average of 15, I should pick <Math math="\arg\min_{p \in P} D(p,q)" />, where <Math math="q" /> is my original model (the uniform distribution).

Why? Intuitively, I want to find a (true) distribution <Math math="p" /> so that <Math math="q" /> is as good model of it as possible, and this is what KL stands for. Admittedly, this time it's not clear why this should be the right thing to do, so see the footnote for a detailed derivation of this from Bayes' rule.
<Footnote>
Remember how at the top of this chapter, we were able to argue that <Math math="\min_{q \in Q} D(p,q)" /> is a good methodology by "compiling it down" to a simple argument about Bayes' theorem? We will do the same here, using a version of so-called [Wallis derivation](https://en.wikipedia.org/wiki/Principle_of_maximum_entropy#The_Wallis_derivation). The reasoning is going to be a bit more involved.

Here is how we formulate our setup to turn this into an application of Bayes' theorem. Imagine that we start with some distribution $q$ and keep sampling from it $N$ times. We use $\hat{q}$ to denote the empirical distribution we get. Now we observe that $\hat{q}$ belongs to some family of distributions $P$ (e.g., all distributions with average 15). What is our current knowledge about $\hat{q}$?

Bayes' theorem is clear on this: we should start with the prior distribution over $\hat{q}$, and then condition on the fact that $\hat{q}$ belongs to $P$ to get the posterior distribution over $\hat{q}$. We will argue the following claim: as $N$ goes to infinity, the posterior distribution is going to be concentrated on the distribution $p \in P$ that minimizes $D(p,q)$. This is the justification for our minimize-KL approach -- our methodology is basically equivalent to "passing from $q$ to $p$ by conditioning on the fact that the distribution belongs to $P$".

Let's prove the claim.

First of all, this is all a bit confusing because we talk about the empirical distribution $\hat{q}$ which is actually a random variable depending on the $N$ samples we took from the space $P_0$ of all empirical distributions (technically, $P_0$ is the space of all distributions where probabilities are multiples of $1/N$ but let's not get bogged down in technicalities). This is technically called a mixture distribution. Here's a picture:

![Wallis derivation](03-minimizing/wallis.png)

We should now understand what kind of distribution $\hat{q}$ has, i.e., we want to understand the probability $P(\hat{q} = p)$ for $p \in P_0$. There are two ways to do that.

1. We can write it down exactly (<Math math="P(\hat{q} = p) = {N \choose p_1N, \dots, p_kN}\cdot \prod_{i = 1}^k q_i ^ {p_i N}" />) and solve it (using Stirling approximation, we have <Math math="P(\hat{q} = p) \approx 2^{H(p)\cdot N}" /> and thus <Math math="P(\hat{q} = p) \approx 2^{-D(p,q)\cdot N}" />).

2. If you like to see what's happening behind the algebra, we can also go back to the [first chapter](01-kl_intro) where our Bayesian friend was pretty much trying to compute the probability we now need. More concretely, if our friend got $N$ and the empirical distribution was $p$, all the evidence in a column of a hypothesis $q$ summed up to the value $N \cdot H(p, q)$. The meaning of the evidence is that the probability <Math math="P(\hat{q} = p)" /> has to be proportional to <Math math="2^{-N \cdot H(p, q)}" />, we just don't know the normalization constant.

Now, waving our hands a little, we can observe that the probability distribution of <Math math="\hat{q}" /> is going to be concentrated on the distribution <Math math="p" /> that minimizes <Math math="H(p,q)" />, as <Math math="N" /> goes to infinity. Thus, the right normalization is <Math math="P(\hat{q} = p) \approx 2^{-N \cdot H(p,q) + N \cdot H(p)} = 2^{-N \cdot D(p,q)}" />.

Whatever road you preferred, we can now continue. While the distribution of $\hat{q}$ is concentrated around the value of $q$, there's nonzero probability that $\hat{q} \in P$. If that happens, we similarly have that the posterior distribution is concentrated around the value <Math math="\tilde{p} = \argmin_{p \in P} D(p,q)"/>. We conclude that in the limit of $N$ going to infinity, doing all the abstract mixture distributions and conditioning on the new evidence is pretty much equivalent to just replacing $q$ with <Math math="\argmin_{p \in P} D(p,q)"/>.
</Footnote>

### The Maximum Entropy Principle <a id="maximum-entropy-principle"></a>

Let's take a closer look at our approach. A seemingly important problem of it is that it only enables us to change an original model $q$ to a better model $p$. But how did we choose the original model $q$? It's a bit of a chicken-and-egg problem.

Fortunately, there's usually a very natural choice for the simplest possible model $q$ -- the uniform distribution. This is the unique distribution that assigns the same probability to each outcome. In some sense, it is the most "uninformative" distribution we can have. There's even a so-called [principle of indifference](https://en.wikipedia.org/wiki/Principle_of_indifference) that states that in case of no additional information, we should opt for the uniform distribution.

So, it would be very interesting to understand what happens when we start with the uniform distribution $q$ (let's say over a discrete set $\{1, \dots, k\}$) and find a better model $p \in P$ by minimizing KL divergence. Let's write it down:

<Math displayMode={true} math="D(p,q_{uniform}) = \sum_{i = 1}^k p_i \log \frac{p_i}{ 1/k}" />

Using our splitting of KL divergence into entropy and cross-entropy, we can rewrite this as:

<Math displayMode={true} math="D(p,q_\text{uniform}) = \sum_{i = 1}^k p_i\log p_i - \sum_i p_i\log (1/k)" />

Since the second term is a constant (and in fact equal to $-\log (1/k) = \log(k)$), minimizing KL divergence is equivalent to maximizing the negative of the first term, which is the entropy of $p$:

<Math displayMode={true} math="\argmin_{p \in P} D(p, q_{uniform}) = \argmax_{p\in P} H(p)" />

We derived what's known as the maximum entropy principle: given a set $P$ of distributions to choose from, we should opt for the $p\in P$ that maximizes the entropy $H(p)$.

### An example

Here's an example of how we can use maximum entropy principle. Let's say that we want to model some data that are real numbers and have certain mean $E_p[X] = \mu$ and variance $E_p[(X-\mu)^2] = \sigma^2$.
We want to pick a distribution that models the data in the absence of any other evidence. Then, maximum entropy principle suggests we should choose the set $P$ to be all distributions with mean $\mu$ and variance $\sigma$ and pick the maximum entropy distribution $p$ in $P$.

As we will discuss in [later](05-machine_learning), this distribution $p$ is the Gaussian with mean $\mu$ and variance $\sigma^2$. So, maximum entropy principle tells us that in the absence of additional evidence, we should model data as Gaussian, with the same mean and variance.

It is often useful to think about this principle a bit more abstractly. Imagine that we don't even know the precise value of $\mu, \sigma^2$, we just believe that the mean and the variance are important parameters to know. Then, maximum entropy principle is telling us that we should model the data by some Gaussian. In other words, we conclude that the family of Gaussians is the *right* family to represent numerical data, if what we care about is their mean and variance. This is a very useful conclusion!

![potato](03-minimizing/potato_maxent.png)

### A few more observations <a id="a-few-observations"></a>

We will spend the whole next chapter discussing all the applications of the maximum entropy principle, but let's already make two observations about it. Those observations correspond to two important properties of entropy.

1. For discrete distributions over $\{1, \dots, k\}$, the distribution that maximizes the entropy is the uniform distribution<Footnote>See e.g. [this](https://stats.stackexchange.com/questions/66108/why-is-entropy-maximised-when-the-probability-distribution-is-uniform)</Footnote>. Its entropy is $\log_2 k$. The implication of this for the max-entropy principle is that if our set $P$ of allowed distributions contains the uniform distribution, then max-entropy principle chooses it. That is, maximum entropy principle is just an extension of the principle of indifference.

2. If you have a joint distribution $(p,q)$, then $H((p,q)) \le H(p) + H(q)$, with equality if and only if $p$ and $q$ are independent.
<Footnote>
This is called [subadditivity of entropy](01-kl_intro/information-theory). Why is it subadditive? If you still remember [our discussion of mutual information](), the mutual information is defined as $D((p,q), p \otimes q)$. You can rewrite this expression as $H(p) + H(q) - H((p,q))$ (try it!), so subadditivity of entropy is equivalent to mutual information being nonnegative.
</Footnote>
The implication for the max-entropy principle is this: Imagine that you work with a joint distribution and you know its two marginals $p,q$. Then, the maximum entropy principle tells us that the joint distribution with these two marginals that maximizes the entropy is the one where they are independent.
Whenever people model stuff by probability distributions, they assume that two distributions are independent all the time. We can think of this assumption as a special case of the maximum entropy principle.

One more point. We have already seen that KL divergence is very closely related to the philosophy of updating your beliefs by Bayes' theorem. This relatioship goes both ways; more in the footnote.

<Footnote>
We've already derived KL divergence and its application from the philosophy of using Bayes' rule. Let's also observe that it can also be done the other way around.

Consider the generic problem solved by Bayes' theorem: We have a joint distribution $(X,Y)$ over two random variables $X$ (that we are interested in) and $Y$ (possible evidences we might see). Then we observe some evidence $Y=y$. We want to know the distribution of $X$ given $Y=y$. The Bayes' rule allows us to compute the conditional distribution $P(X|Y=y)$. This is not rocket science: If you imagine the original joint distribution as a table, you can just zero out the rows that do not correspond to $Y=y$ and normalize the remaining row to get the posterior distribution.

The observation you can make is that <Math math="r = \argmin_{p \in P} D(p, (X,Y))" />, where $P$ is the set of distributions where $P(Y = y') = 0$ for $y' \ne y$. That's because after some rewritings, this minimization problem is equivalent to claiming that $P(X | Y=y) = \argmin_{p} D(p, P(X|Y=y))$ which is true.

So, we can think of the philosophy of using the Bayes' theorem as a special case of the philosophy of minimizing KL divergence. This is all very circular and fuzzy, but the point is that not only the philosophy of minimize-KL-whenever-you-can-ism can be derived from the philosophy of argue-everything-from-Bayes-rule-ism, but we can also derive the philosophy of argue-everything-from-Bayes-rule-ism from the philosophy of minimize-KL-whenever-you-can-ism. Thus it is said that minimize-KL-whenever-you-can-ism is argue-everything-from-Bayes-rule-ism-complete (and vice versa).
</Footnote>

## Next Steps <a id="next-steps"></a>

In the next part, we'll explore more concrete modeling applications of these principles. We'll see how they apply to various fields like machine learning, statistical modeling, and information theory.
