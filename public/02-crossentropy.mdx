# Entropy & Crossentropy

In this chapter, we will see how KL divergence connects with entropy and crossentropy. It's not rocket science:

![Formula for KL divergence](02-crossentropy/crossentropy_entropy_formula.png)

We will also go through a few basic properties that these things have.

## Relative entropy = Crossentropy - Entropy

A quick recap: The [01-kl_intro](previous chapter), we've seen how KL divergence is really the result of repeatedly applying Bayes' theorem, using this table:

![Surprisal](01-kl_intro/repeated_bayes_surprisal.png)

Each cell in this table holds a value of type $\log 1/p$ that's also sometimes called the surprisal.

Last time, we have been parsing this table "row-by-row". That is, we always subtracted two surprisals to compute how the given flip changes our posterior. The Bayesian reasoner then keeps summing up all those surprisal differences.

We can also try to understand the table "column-by-column". That is, the Bayesian reasoner can compute the total surprisal for both hypothesis, then convert those into the final posterior probability. This corresponds to rewriting the KL divergence as follows:

$$
\underbrace\{\sum_{i = 1}^n p_i \log p_i / q_i\}_\{D(p,q)\}
 =
\underbrace\{\sum_{i = 1}^n p_i \log 1 / q_i\}_\{H(p,q)\}
 -
\underbrace\{\sum_{i = 1}^n p_i \log 1 / p_i\}_\{H(p)\}
$$

$H(p,q)$ is called _crossentropy_. You should think of this expression as telling you how surprised you are, on average, if you keep seeing data from $p$ and try to model them by $q$.

$H(p) = H(p, p)$ is an important special case called _entropy_. Intuitively, it's telling you how much surprise there is, if you keep seeing data sampled from $p$ (and you know they are coming from $p$).

Finally, $D(p,q)$ is the difference between these two values. That's also why in some communities, it's known as the _relative entropy_ between $p$ and $q$. <Footnote>Much better name than KL divergence, if you ask me. The name KL divergence seems to be much more common, though, so we stick with it. </Footnote>

### So what is this good for?

Splitting a summation into two summations is not some kind of hard-core mathematical trick; the main advantage of adding crossentropy & entropy to our vocabulary is that they are simply very meaningful concepts. Let's solve a few more riddles and see what I mean by that!

## Two important properties of (cross-, relative-) entropy

All our entropies have bunch of intuitive properties. Here are a few of them.

### Additivity

Let's say that you have two distribution (the "truth" and the "model") $p, q$ and then another two distributions $p', q'$. It's true that
<Footnote>
Here's the proof (by Claude):
Let's start with the definition of KL divergence for the joint distributions:

$$D(p \otimes p', q \otimes q') = \sum_{i=1}^{n}\sum_{j=1}^{m} p(i)p'(j) \log\left(\frac\{p(i)p'(j)\}\{q(i)q'(j)\}\right)$$

Using properties of logarithms, we can rewrite this as:

$$D(p \otimes p', q \otimes q') = \sum_{i=1}^{n}\sum_{j=1}^{m} p(i)p'(j) \left[\log\left(\frac\{p(i)\}\{q(i)\}\right) + \log\left(\frac\{p'(j)\}\{q'(j)\}\right)\right]$$

This expands to:

$$D(p \otimes p', q \otimes q') = \sum_{i=1}^{n}\sum_{j=1}^{m} p(i)p'(j) \log\left(\frac\{p(i)\}\{q(i)\}\right) + \sum_{i=1}^{n}\sum_{j=1}^{m} p(i)p'(j) \log\left(\frac\{p'(j)\}\{q'(j)\}\right)$$

Now, we can factor out terms:

$$D(p \otimes p', q \otimes q') = \sum_{i=1}^{n} p(i) \log\left(\frac\{p(i)\}\{q(i)\}\right) \sum_{j=1}^{m} p'(j) + \sum_{j=1}^{m} p'(j) \log\left(\frac\{p'(j)\}\{q'(j)\}\right) \sum_{i=1}^{n} p(i)$$

Since probability distributions sum to 1:
$$\sum_{i=1}^{n} p(i) = 1 \text{ and } \sum_{j=1}^{m} p'(j) = 1$$

This gives us:

$$D(p \otimes p', q \otimes q') = \sum_{i=1}^{n} p(i) \log\left(\frac\{p(i)\}\{q(i)\}\right) + \sum_{j=1}^{m} p'(j) \log\left(\frac\{p'(j)\}\{q'(j)\}\right)$$

Which is exactly:

$$D(p \otimes p', q \otimes q') = D(p, q) + D(p', q')$$

</Footnote>



$$
D(p \otimes p', q \otimes q') = D(p, q) + D(p', q')
$$
where $\otimes$ stands for the product distribution, i.e. the joint distribution where $p,p'$ are independent. This is super important! In fact, in the precedening chapter, it was important that the flips of the coin were independent, so to make it more formal, we would really rely on this additive property.

In fact, a slightly stronger property called _chain rule_ is true. Imagine that we have two distributions $p,q$ for how I commute to work (\{üö∂‚Äç‚ôÄÔ∏è, üö≤, üöå}), but later I decide that when I take bus, I will also keep track of what line I took (\{‚ë†, ‚ë°, ‚ë¢}); I again have two distributions $p', q'$. Now, $p$ and $p'$ can be combined into an overall distribution $p_{overall}$ over the set \{üö∂‚Äç‚ôÄÔ∏è, üö≤, ‚ë†, ‚ë°, ‚ë¢}; we combine $q,q'$ into $q_{overall}$ as well. It then holds that

<Footnote>
Here's the proof (by Claude). Let $p$ and $q$ be probability distributions over a set $\{1, 2, \ldots, n\}$. Let $p'$ and $q'$ be probability distributions over a set $\{1, 2, \ldots, m\}$.

Define combined distributions $p_{overall}$ and $q_{overall}$ over the set $\{(1,1), (1,2), \ldots, (1,m), 2, 3, \ldots, n\}$ where:

- $p_{overall}(i) = p(i)$ and $q_{overall}(i) = q(i)$ for all $i \in \{2, 3, \ldots, n\}$
- $p_{overall}(1,j) = p(1) \cdot p'(j)$ and $q_{overall}(1,j) = q(1) \cdot q'(j)$ for all $j \in \{1, 2, \ldots, m\}$

Then:
$$D(p_{overall}, q_{overall}) = D(p, q) + p(1) \cdot D(p', q').$$
Here's why. We compute the KL divergence for the combined distributions:

$$D(p_{overall}, q_{overall}) = \sum_\{z \in Z} p_{overall}(z) \log\left(\frac\{p_{overall}(z)\}\{q_{overall}(z)\}\right)$$

Where $Z = \{(1,1), (1,2), \ldots, (1,m), 2, 3, \ldots, n\}$.

We can split this sum based on whether $z$ is in $\{2, 3, \ldots, n\}$ or in $\{(1,1), (1,2), \ldots, (1,m)\}$:

$$D(p_{overall}, q_{overall}) = \sum_{i=2}^{n} p_{overall}(i) \log\left(\frac\{p_{overall}(i)\}\{q_{overall}(i)\}\right) + \sum_{j=1}^{m} p_{overall}(1,j) \log\left(\frac\{p_{overall}(1,j)\}\{q_{overall}(1,j)\}\right)$$

For the first sum, using the definitions of $p_{overall}$ and $q_{overall}$:

$$\sum_{i=2}^{n} p_{overall}(i) \log\left(\frac\{p_{overall}(i)\}\{q_{overall}(i)\}\right) = \sum_{i=2}^{n} p(i) \log\left(\frac\{p(i)\}\{q(i)\}\right)$$

For the second sum, substituting $p_{overall}(1,j) = p(1) \cdot p'(j)$ and $q_{overall}(1,j) = q(1) \cdot q'(j)$:

$$\sum_{j=1}^{m} p_{overall}(1,j) \log\left(\frac\{p_{overall}(1,j)\}\{q_{overall}(1,j)\}\right) = \sum_{j=1}^{m} p(1) \cdot p'(j) \log\left(\frac\{p(1) \cdot p'(j)\}\{q(1) \cdot q'(j)\}\right)$$

Using the properties of logarithms:

$$\sum_{j=1}^{m} p(1) \cdot p'(j) \log\left(\frac\{p(1)\}\{q(1)\} \cdot \frac\{p'(j)\}\{q'(j)\}\right)$$

$$= \sum_{j=1}^{m} p(1) \cdot p'(j) \left[\log\left(\frac\{p(1)\}\{q(1)\}\right) + \log\left(\frac\{p'(j)\}\{q'(j)\}\right)\right]$$

This expands to:

$$\sum_{j=1}^{m} p(1) \cdot p'(j) \log\left(\frac\{p(1)\}\{q(1)\}\right) + \sum_{j=1}^{m} p(1) \cdot p'(j) \log\left(\frac\{p'(j)\}\{q'(j)\}\right)$$

For the first term, factoring out $p(1)$:

$$\sum_{j=1}^{m} p(1) \cdot p'(j) \log\left(\frac\{p(1)\}\{q(1)\}\right) = p(1) \log\left(\frac\{p(1)\}\{q(1)\}\right) \sum_{j=1}^{m} p'(j)$$

Since $\sum_{j=1}^{m} p'(j) = 1$, this simplifies to:

$$p(1) \log\left(\frac\{p(1)\}\{q(1)\}\right)$$

For the second term:

$$\sum_{j=1}^{m} p(1) \cdot p'(j) \log\left(\frac\{p'(j)\}\{q'(j)\}\right) = p(1) \sum_{j=1}^{m} p'(j) \log\left(\frac\{p'(j)\}\{q'(j)\}\right)$$

$$= p(1) \cdot D(p', q')$$

Now, combining all parts:

$$D(p_{overall}, q_{overall}) = \sum_{i=2}^{n} p(i) \log\left(\frac\{p(i)\}\{q(i)\}\right) + p(1) \log\left(\frac\{p(1)\}\{q(1)\}\right) + p(1) \cdot D(p', q')$$

The first two terms together with $p(1) \log\left(\frac\{p(1)\}\{q(1)\}\right)$ form $D(p, q)$, since they cover all elements in the original set:

$$D(p_{overall}, q_{overall}) = D(p, q) + p(1) \cdot D(p', q')$$

This proves the chain rule for KL divergence as specified.
</Footnote>

$$
D(p_{overall}, q_{overall}) = D(p,q) + p_\{\text{bus}} \cdot D(p',q')
$$

Crossentropy and entropy are also additive for independent distributions.

### Nonnegativity / monotonicity

We have already seen in the [previous chapter](01-kl_intro) that if KL were negative for some two distributions, something would be severely wrong with the universe.
There's also a short algebraic proof of this fact. 

<Footnote>
To prove that for any $p,q$ we have (I have used the natural algorithm to make it a bit shorter)

$$
D(p,q) =  \sum_{i = 1}^n p_i \cdot \ln \frac\{p_i\}\{q_i}  \ge 0,
$$

we will estimate the expression $\ln \frac\{p_i\}\{q_i}$ inside the summation. Since we already know that the inequality is tight if all $p_i = q_i$, we should choose an estimate of logarithm which is tight around 1. The best approximation of logarithm around $1$ is $\ln (1+x) \le x$. We can use it like this:

$$
-D(p,q)
= \sum_{i = 1}^n p_i \cdot \ln \frac\{q_i\}\{p_i}
\le \sum_{i = 1}^n p_i \cdot  \left( \frac\{q_i\}\{p_i} - 1 \right)
= \sum_{i = 1}^n q_i - p_i
= 1 - 1 = 0
$$


Because $D(p,q) = H(p,q) - H(p)$, you can view this fact as saying that the crossentropy $H(p,q)$ for fixed $p$ is minimized if we choose $q = p$.
</Footnote>


A bit stronger version of this fact is the so-called monotonicity <Footnote>This is pretty similar property to the so-called [Data processing inequality](https://en.wikipedia.org/wiki/Data_processing_inequality) you might have heard of. </Footnote> Imagine that we have two distributions $p,q$, let's say they are over countries (\{üá∫üá∏,üá®üá¶,üá≤üáΩ,üáØüáµ,üá¨üáß,üáÆüá≥,...}). Then, we zoom out a little bit and consider a smaller set of geographical areas (\{North America, Europe, ...}). We can convert $p,q$ to this rougher set to get distributions $p', q'$ -- e.g. we set $p'(North America) = p(üá∫üá∏) + p(üá®üá¶) + p(üá≤üáΩ)$. Then it's the case that

$$
D(p, q) \ge D(p', q')
$$

This makes sense! Remember that KL divergence means how hard is it to separate $p$ from $q$ using Bayes' theorem. This just says that for our Bayesian reasoner, it gets harder to distinguish $p$ from $q$, as we keep grouping different outcomes together. In the special case, we could group all the outcomes to a single outcome SOMETHING_HAPPENED -- the inequality would than reduce to $D(p,q) \ge 0$.

### Uniqueness

As it turns out, some kind of additivity / chain rule + monotonicity can be used to uniqely pinpoint KL divergence and entropy. By that I mean that any reasonable function of one parameter that has these two properties already [has to be the entropy](<https://en.wikipedia.org/wiki/Entropy_(information_theory)#Characterization>), and any reasonable function of two parameters with these properties [has to be KL divergence](https://blog.alexalemi.com/kl.html).

This is pretty cool and it helps us to appreciate that our definitions are not arbitrary at all. KL is _the right_ way to measure how well a model fits reality.

## Application: Evaluating Predictions <a id="application-evaluating-predictions"></a>

As the next application of KL divergence and cross-entropy, let's solve our [prediction riddle](00-introduction/predictions). Recall that we asked about a few experts to predict future events and recorded their probabilities -- how do we measure their performance?

### Idealized KL score

To grade an expert's predictions, let $p$ denote the true probabilities of events and $q$ denote the expert's predicted probabilities. It sure sounds like a good idea to define the expert's score as:

$$
KL score = D(p,q)
$$

We will for simplicity assume that all the events are independent, where this simplifies to the following formula:

$$
KL score = \sum_{i = 1}^n  \left(
    p_i\log\frac\{p_i\}\{q_i} + (1-p_i)\log\frac\{1-p_i\}\{1-q_i}
    \right)
$$

For example, if we ask an expert about two independent events that happen with probabilities $30\%$ and 90\%, and she estimates the probabilities as $50\%$ and $80\%$, her score would be

$$
KL score = 0.3 \cdot \log 0.3/0.5 + 0.7 \cdot \log 0.7/0.5 \\
+ 0.9 \cdot \log 0.9 / 0.8 + 0.1 \cdot 0.1 / 0.2
\approx ??.
$$

There's just one problem with our approach -- can you see it?

### Crossentropy score

The problem is that when we are grading, we have typically absolutely no idea what the "true" probabilities are! <Footnote> We could now have a very long philosophical discussion about whether something like "true" probability even exists. Fortunately, nothing in this text really depends on these philosophical subtleties. </Footnote>

The only fact we know is whether each event happenned or not. Fortunately, this also corresponds to a valid probability distribution $\hat{p}$ -- it's simply the case that $\hat{p}_i = 0$ or $\hat{p}_i = 1$ for each $i$.

We can now define a new, feasible to compute, score function:

$$
crossentropy score =
\sum_{i = 1}^n  \left(
    \hat{p}_i\log\frac\{\hat{p}_i\}\{q_i} + (1-\hat{p}_i)\log\frac\{1-\hat{p}_i\}\{1-q_i}
    \right)
$$

Why do we call it the _crossentropy score_? At this point, I should mention that to define KL divergence, we use the convention that $0 \cdot \log 0 = 0 \cdot \log 0/0 = 0$. This means that our equation actually simplifies to

$$
crossentropy score =
\sum_{i = 1}^n  \left(
    \hat{p}_i\log\frac\{1\}\{q_i} + (1-\hat{p}_i)\log\frac\{1\}\{1-q_i}
    \right)
$$

That is, the score is both equal to the KL divergence between $\hat{p}$ and $q$, and to crossentropy between them. <Footnote> Another way to see that is via the equation $D(\hat{p}, q) = H(\hat{p}, q) - H(\hat{p})$; the distribution $\hat{p}$ is a trivial distribution supported only on one outcome, so its entropy is 0 and we thus have $D(\hat{p}, q) = H(\hat{p}, q)$. </Footnote> So we might as well call this crossentropy score. In the forecasting community, this score is typically called [Log-score](https://forecasting.wiki/wiki/Log_score), the crossentropy score notation is closer to how this is called in machine learning.

Before we compute the score, let's understand more deeply how it connects to the idealized KL score. Technically speaking, crossentropy score is a random variable because it depends on the "true" randomness of $p$. More concretely, each $\hat{p}_i$ is actually a random variable equal to 1 with probability $p_i$, and otherwise equal to 0.

So what's the expected value of the crossentropy score, i.e., $E_p[crossentropy score]$? Well, since $\hat{p_i}$ is on average equal to $p_i$, i.e., $E_p[\hat{p}_i] = p_i$, we simply have

$$
E_p[crossentropy score] = H(p, q)
$$

This is pretty cool! If we give our experts many questions, then in the long run, we can use the law of large numbers to argue that the score we measure is close to the crossentropy $H(p,q)$.

At this point, let's remember the equation

$$
D(p,q) = H(p,q) - H(p)
$$

Although we can't compute the value of $D(p,q)$, notice that for two expert predictions $q_1, q_2$, we have $D(p,q_1) < D(p, q_2)$ if and only if $H(p, q_1) < H(p, q_2)$: KL divergence and crossentropy only differ by $H(p)$ which is in this case just some constant. That is, _if we compare the experts based by their cross entropy score, it's in the long run the same as comparing them by their KL score!_

### Example

Let's see an example of this. Let's say we flip a fair coin $N$ times ($p_1 = \dots = p_N = 1/2$) and ask two experts for their probabilities. Let's say that the first expert predicts correctly ($q_1 = \dots = q_N = 1/2$) and the other one is a bit off ($q'_1 = \dots = q'_N = 0.6$).

Then, the idealized KL score happens to be $KL(p, q) = N \cdot 0 = 0$ and $KL(p, q') = N \cdot D(p_1, q'_1) \approx 0.03 \cdot N$.

We don't have access to this score, but if we compute their crossentropy scores, the law of large numbers tells us that for large enough $N$, the scores are going to be roughly $H(p,q) = N$ and $H(p, q') \approx 1.03 \cdot N$. Both crossentropies are shifted by the entropy of the underlying process which is $N$. So we are penalizing both experts for something they can't know. But that's OK since both values are shifted by the same amount, so if we use the score to e.g. select the best experts, it behaves the same as using the ideal KL score.

We will not bother computing the cross entropy score for the example data. The reason is that, as we have seen, our score is only reasonable if we can argue via the law of large numbers that the value we compute is going to be close to the crossentropy $H(p,q)$. But that's very fishy assumption for just 8 predictions, especially if some of the events happen with very small or very large probability.

## Coding Theory Intuitions <a id="coding-theory-intuitions"></a>

One of the most powerful intuition pumps about entropy and related concepts comes from information theory, in particular coding theory. Let's explain it here very briefly.

If you look at Morse code, you might notice how the code for certain frequent letters like e or t are much shorter than the codes for rare letters like h or q.

To build the theory behind building codes like Morse's, let's imagine the following task. Consider the distribution over the English alphabet a,b,...,z where the probability of each letter is its approximate frequency in English text.

Our task is as follows. We should come up with some kind of binary encoding scheme so that if we get a long English text, represented as a large amount $n$ of independent samples from the above distribution, the number of bits we use is the least possible.

It turns out that the best possible solution to this problem satisfies that the average amount of bits we spend per letter -- called the rate of the code -- is $\sum p_i \log \frac\{1\}\{p_i}$, the entropy of the distribution. For example, if the entropy of the distribution is about 4.17 bits, there is a code spending that many bits per letter on average, and there is no better code.

Instead of proving why the entropy is the answer to our question, let's see examples of (suboptimal) codes that suggest why the answer is entropy.

First, we could encode each letter using 5 bits -- a becomes 00000, b becomes 00001 and so on. The rate of this scheme is 5 bits per letter, and it can be improved quite a bit.

One way to improve it is to assign codes of different lengths to different letters, based on their frequency. The following turns out to be a good rule for the lengths: if a letter $i$ has frequency $p_i$, its code length should be about $\log \frac\{1\}\{p_i}$.

As a specific example, if the alphabet were just a-d and the letter frequencies were $1/2, 1/4, 1/8, 1/8$, the best code is:

- a - 0
- b - 10
- c - 110
- d - 111

That is, "baba" is encoded as "100100".

You might notice how the letter with frequency $1/2^i$ has a code of length $i$; that is, frequency $p_i$ leads to a code of length $\log \frac\{1\}\{p_i}$.

If all the probabilities in the distribution are $1/2^k$ for some number $k$, the generalization of the above code -- having $1/2^k$ probability letters have lengths $k$ -- is always possible.

If the probabilities in the distribution are not "round", you can't quite achieve a code mapping letters to binary strings as good as the entropy $H(p)$ of the distribution due to the rounding issues. You can only achieve a code of rate at most $1 + H(p)$. For example, the entropy of our English letter distribution is about $4.17$, but the best code mapping letters to bits (so-called Huffman code) has slightly worse rate of about $4.25$.

But that's not a big deal. If you allow codes that don't necessarily map single letters to bit strings, but that can also map pairs of letters and so on, then you can get codes with rate arbitrarily close to the entropy.

## Next Steps <a id="next-steps"></a>

We are now familiar with how KL divergence works! If it is really the right way of how a model fits the truth, it sure seems that making it small is a good way to build good models!? [Let's see](03-minimizing)!
