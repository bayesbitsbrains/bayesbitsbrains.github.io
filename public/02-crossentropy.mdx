# KL properties & (cross-)entropy

In this chapter, we'll see how KL divergence can be split into two pieces called _cross-entropy_ and _entropy_. 

<KeyTakeaway>
![Formula for KL divergence](02-crossentropy/crossentropy_entropy_formula.png)
Cross-entropy measures the average surprisal of model $q$ on data from $p$. When $p = q$, we call it entropy. 
</KeyTakeaway>




## üß† KL = cross-entropy - entropy

Quick refresher: In the [previous chapter](01-kl_intro), we saw how KL divergence comes from repeatedly using Bayes' theorem with log-space updating:

<BayesSequenceLogWidget highlightSurprisals={true} />

Each step adds surprisals ($\log 1/p$) to track evidence.
Last time, we focused on the differences between surprisals to see how much evidence we got for each hypothesis. Our Bayesian detective just keeps adding up these differences.

But the detective could also add up the total surprisal for each hypothesis (green and orange numbers in the above widget), and then compare overall _Total surprisal_  values. This corresponds to writing KL divergence like this:

<Math id="cross-entropy-decomp" displayMode={true} math='
\underbrace{\sum_{i = 1}^n p_i \log \frac{p_i}{q_i}}_{D(p,q)}
 \;\;\;=\;\;\;
\underbrace{\sum_{i = 1}^n p_i \log \frac{1}{q_i}}_{H(p,q)}
 \;\;\;-\;\;\;
\underbrace{\sum_{i = 1}^n p_i \log \frac{1}{p_i}}_{H(p)}
'/>

These two pieces on the right are super important: $H(p,q)$ is called cross-entropy and $H(p)$ is entropy. Let's build intuition for what they mean. 

##  ‚ùå Cross-entropy <a id = "cross"></a>
Think of cross-entropy as: __how surprised you are on average when seeing data from $p$ while modeling it as $q$?__ 

Explore this in the widget below. The widget shows what happens when our Bayesian detective from the previous chapter keeps flipping her coin. The red dashed line is showing cross-entropy - the expected surprisal of the model $q$ as we keep flipping the coin with bias $p$. The orange line shows the entropy‚Äîthis is the expected surprisal when both the model and actual bias are $p$. 
KL divergence is the difference between cross-entropy and entropy. Notice that the cross-entropy line is always above the entropy line (equivalently, KL divergence is always positive). 



If you let the widget run, you will also see a blue and a green curve - the actual surprisal measured by our detective in the flipping simulation. We could also say that these curves measure cross-entropy‚Äîit's the cross-entropy between the _empirical distribution_ $\hat{p}$ (the actual outcomes of the flips) and the model $q$ (blue curve) or $p$ (green curve). The empirical cross-entropies are tracking the dashed lines due to the [law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers). 


<CrossEntropySimulator />



Bottom line: _Better models are less surprised by the data and have smaller cross-entropy. KL divergence measures how far our model is from the best one._

## üé≤ Entropy

The term $H(p) = H(p, p) = \sum_{i = 1}^n p_i \log 1 / p_i$ is a special case of cross-entropy called just plain _entropy_. It's the best possible cross-entropy you can get for distribution $p$‚Äîwhen you model it perfectly as itself.  

Intuitively, entropy tells you how much surprisal or uncertainty is baked into $p$. Even if you know you're flipping a fair coin and hence $p = q = \frac12$, you still don't know which way the coin will land. There's inherent uncertainty in that‚Äîthe outcome still carries surprisal, even if you know the coin's bias. This is what entropy measures. 

The fair coin's entropy is <Math math = "H(\{\textsf{H: }1/2, \textsf{T: }1/2\}) = \frac12\cdot \log2 +  \frac12\cdot \log2 = 1" /> bit. 
But entropy can get way smaller than 1 bit. If you flip a biased coin where heads are very unlikely‚Äîsay $p(\textsf{H} = 0.05)$‚Äîthe entropy of the flip gets close to zero. Makes sense! Sure, if you happen to flip heads, that's super surprising ($\log 1/0.05 \approx 4.32$). However, most flips are boringly predictable tails, so the _average_ surprise gets less than 1 bit. You can check in the widget below that $H(\{\textsf{H}: 0.05, \textsf{T}: 0.95\}) \approx 0.29$ bits per flip. Entropy hits zero when one outcome has 100% probability. 

Entropy can also get way bigger than 1 bit. Rolling a die has entropy $\log_2(6) \approx 2.6$ bits. In general, a uniform distribution over $k$ options has entropy $\log_2 k$, which is the maximum entropy possible for $k$ options. Makes sense‚Äîyou're most surprised on average when the distribution is, in a sense, most uncertain.  

<EntropyWidget numCategories={6} title="Entropy of die-rolling" />

<Expand headline = "Example: correct horse battery staple">
Here's an example. Let's say there are about <Math math="2^{11}" /> English words that can be described as 'common'. If you generate uniformly four such common words and make your password the concatenation of them, the total entropy of your password is thus going to be $44$ bits. That's because entropy is a special case of cross-entropy and is thus additive. 

Having a uniform distribution with 44 bits of entropy is just a different way of saying that we have a uniform distribution with $2^{44}$ possible outcomes. 
This comic wisely teaches us that this many possibilities make it a pretty secure password! Even if an adversary knows how we generated it, cracking it means they have to check about <Math math="2^{44}" /> passwords. 

<a href="https://xkcd.com/936/" target="_blank" rel="noopener noreferrer">
  <img src="02-crossentropy/password.png" alt="password" style={{cursor: 'pointer'}} />
</a>
</Expand>


## ‚öñÔ∏è Relative entropy
KL divergence can be interpreted as the gap between cross-entropy and entropy. It tells us how far your average surprisal (cross-entropy) is from the best possible (entropy). 
That's why in some communities, people call KL divergence the _relative entropy_ between $p$ and $q$. <Footnote>Way better name than 'KL divergence' if you ask me. But 'KL divergence' is what most people use, so I guess we're stuck with it. </Footnote>

## üß© Cracking riddles

Splitting a sum into two parts isn't exactly rocket science‚Äîthe real win is that cross-entropy and entropy are super meaningful concepts on their own. Let's see this in action. 

<RiddleExplanation id="predictions"><a id="application-evaluating-predictions"></a>

Time to solve our [prediction riddle](00-riddles#predictions). We asked experts to predict future events‚Äîhow do we score them?

### Idealized KL score

To grade predictions, let $p = (p_1, \dots, p_n)$ be the true probabilities of the events we ask our experts about, and $q$ be what the expert predicted. It for sure seems like a good idea to give the expert the following score:

<Math displayMode={true} math="S_{KL}(q) = D(p,q)" />

That is, we know that KL divergence is measuring how well a model $q$ matches the reality $p$, so we just use this a score for the expert‚Äîthe lower score the better. 

If all $n$ events are independent,<Footnote>They of course never are, but here we will always assume they are. </Footnote> this formula becomes:

<Math displayMode={true} math="S_{KL}(q) = \sum_{i = 1}^n  \left(
    p_i\log\frac{p_i}{q_i} + (1-p_i)\log\frac{1-p_i}{1-q_i}
    \right)" />


But wait‚Äîthere's a huge problem with this score. Can you spot it?

### üéØ Cross-entropy score

The problem: we have no clue what the "true" probabilities are! <Footnote> We could go down a philosophical rabbit hole about whether "true" probability even is even a meaningful concept. But let's not. </Footnote>

All we know is what actually happened. This gives us an empirical distribution $\hat{p}$ where each $\hat{p}_i$ is either 0 or 1‚Äîit's all concentrated on the one outcome we saw.

What happens when we plug $\hat{p}$ into our KL score? 

<Math displayMode={true} math="
S_{CE}(q) =
\sum_{i = 1}^n  \left(
    \hat{p}_i\log\frac{1}{q_i} + (1-\hat{p}_i)\log\frac{1}{1-q_i}
    \right)
"/>

Since $\hat{p}$'s entropy is zero (one outcome, no uncertainty), cross-entropy and relative entropy are the same. This is why the formula $S_{CE}$ is often called the _cross-entropy score_, though forecasting community calls it the [Log-score](https://forecasting.wiki/wiki/Log_score). 

### üîó Connection to the idealized score
Let's dig into how cross-entropy score relates to our idealized KL score. Technically speaking, cross-entropy score is a random variable‚Äîimagine an obtruse probability space of what might-have-happened. In this space, each $\hat{p}_i$s is a random variable. that has value 1 with probability $p_i$, otherwise it's 0.

So what's the expected cross-entropy score? Since $E_p[\hat{p}_i] = p_i$, [linearity of expectation](https://brilliant.org/wiki/linearity-of-expectation/) gives us:

<Math displayMode={true} math="
E_p[S_{CE}(q)] =
\sum_{i = 1}^n  \left(
    p_i\log\frac{1}{q_i} + (1-p_i)\log\frac{1}{1-q_i}
    \right)
"/>

In other words, <Math  math="E_p[S_{CE}(q)] = H(p,q)"/>. Nice! Give experts lots of questions, and by the law of large numbers, their score will approach the cross-entropy $H(p,q)$ between the true distribution and their guess. This is analogous to how [in the cross-entropy widget above](#cross), the blue line is different each time you run the experiment (it is random variable), but it tracks the dashed red line (the law of large numbers). 

Now remember:
<Math id="entropy-relation" displayMode={true} math="D(p,q) = H(p,q) - H(p)" />
We can't compute $D(p,q)$ directly, but here's the key insight: for two experts with predictions $q_1, q_2$, we have $D(p,q_1) < D(p, q_2)$ if and only if $H(p, q_1) < H(p, q_2)$. They only differ by $H(p)$, which doesn't depend on the experts' predictions. So:

_Comparing experts by cross-entropy is just as good as comparing by KL divergence in the long run!_

<Expand headline="Example: Coin flipping">
Let's make this concrete. We flip a fair coin $N$ times ($p_1 = \dots = p_N = 1/2$). Expert 1 nails it ($q_1 = \dots = q_N = 1/2$), while Expert 2 is a bit off ($q'_1 = \dots = q'_N = 0.6$).

The idealized KL scores: 
<Math displayMode={true} math = "S_{KL}(q) = KL(p, q) = N \cdot 0 = 0"/>
<Math displayMode={true} math = "S_{KL}(q') = KL(p, q') = N \cdot D(p_1, q'_1) \approx 0.03 \cdot N"/>

We can't compute these scores in practice, because in practice we would not know $p$. However, we can compute cross-entropy. The expected cross-entropy score is this: 
<Math displayMode={true} math = "E[S_{CE}(q)] = H(p) + KL(p, q) = N + 0 = N"/>
<Math displayMode={true} math = "E[S_{CE}(q')] = H(p) + KL(p, q') = N + N \cdot D(p_1, q'_1) \approx 1.03 \cdot N"/>

For large $N$, the law of large numbers says that the actual score we measure - $S_{CE}(q)$ - is likely close to the expected score. In that case, since both scores shift by the same amount, cross-entropy still picks the best expert just like KL would. 

However, here's a key point: this only works _in the long run_. Disentangling $N$ from $1.03N$ reliably [would require](fisher-info) about $N \approx 1000$ questions. That's why the original riddle does not try to pretend we can actually say which expert is good. With just 5 predictions, any scoring is going to be pretty noisy. 
</Expand>

Try the log-score on our example! Also, you can compare it with other popular score called brier score, which is just the so-called mean squared error or $\ell_2$ metric (i.e., if you predicted 0.8 probability and the event happens, your score is $(1-0.8)^2 = 0.04$). Brier score does not really care whether your failed prediction had probability of $0.9$ or $0.9999$. 

<ExpertRatingWidget
    title="Comparing Scoring Methods"
    showBrierScore={true}
  />

</RiddleExplanation>


## üöÄ What's next? <a id="next-steps"></a>

We're getting the hang of KL divergence, cross-entropy, and entropy! Quick recap:

![Formula for KL divergence](02-crossentropy/crossentropy_entropy_formula.png)

In the [next chapter](03-entropy_properties), we will do a recap of what kind of properties these functions have. 

