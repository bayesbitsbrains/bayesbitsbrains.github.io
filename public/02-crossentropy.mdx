-- omluvit se ze coding theory je dulezita ale my jdeme smerem statistiky a machine learningu

## KL Divergence as a Natural Measure <a id="kl-divergence-as-a-natural-measure"></a>

Here is another thought experiment leading to KL divergence. Suppose we want to find a reasonable function $f(p,q)$ that measures how well the true distribution $p$ is approximated by a model distribution $q$.

Instead of proposing concrete formulas immediately, we can ask what properties such a function should satisfy. Consider joint distributions—say, the joint distribution over weather (sunny/rainy) and commuting methods (walk/bike/bus) from a previous example.

Such a distribution can be represented as a table with six numbers (which we can input into $f$), or equivalently using conditional probabilities as a "decision tree" where you first learn whether it rains and then, given that, decide on walking, biking, or taking the bus.

Generalizing these thought experiments, it seems natural to require that if we can decompose two distributions $p$ and $q$ as joint distributions $(p_1,p_2)$ and $(q_1,q_2)$, then:

$$
f((p_1, p_2), (q_1, q_2)) = f(p_1, q_1) + \sum_i p_i\cdot f\Bigl(p_2(\cdot|i), q_2(\cdot|i)\Bigr)
$$

A priori, it is not clear whether any function $f$ satisfies this property, since there are many ways to represent a joint distribution using different trees. Remarkably, this constraint almost uniquely characterizes KL divergence.

### KL Divergence is Uniquely Characterized <a id="kl-divergence-is-uniquely-characterized"></a>

The following theorem formalizes this intuition:

**Theorem**: Let $f$ be a function on pairs of probability distributions on an $n$-element set. Suppose $f$ satisfies the following constraints:

1. $$f((p_1, p_2), (q_1, q_2)) = f(p_1, q_1) + \sum_i p_i\cdot f\Bigl(p_2(\cdot|i), q_2(\cdot|i)\Bigr)$$
2. $f(p,q)$ is nonnegative, with $f(p,q)=0$ if and only if $p=q$.
3. $f(p,q)$ is continuous.

Then, $$f(p,q) = c\, KL(p,q)$$ for some positive constant $c$, and conversely.

For example, we require nonnegativity because working with $f(p,q) = -KL(p,q)$ would be counterintuitive. Continuity is required for technical reasons. Note that $f$ is defined only up to a constant factor, which is equivalent to the choice of logarithm base in the definition of $KL$. We use base 2 because we like to think in bits (physicists might choose $\mathrm{e}$); it doesn't really matter.

We will not prove that KL divergence is the only function satisfying the above constraints, but we will verify that it does satisfy them.

## Application: Training Smaller Models <a id="application-training-smaller-models"></a>

Let's solve one of our riddles from the introduction.

In the deep learning riddle, we asked about how to train a smaller language model to mimic a larger one like GPT-4. You may not be surprised to learn that for this task, people typically optimize the KL divergence $D(p,q)$ between the two distributions.

What about other options, like minimizing the $\ell_1$ norm, i.e., $\sum |p_i - q_i|$? or $\ell_2$ norm, i.e., $\sum (p_i - q_i)^2$? Doing that would perhaps also work reasonably well, but there is something very uneasy about those approaches. Consider these two scenarios:

1. $p_i = 0.5, q_i = 0.49$
2. $p_i = 0.01, q_i = 0.0$

The standard geometrical norms ($\ell_1, \ell_2$) treat both of these situations as having similar magnitude of error. KL, though, understands that while in the first situation, the difference is pretty negligible, in the second situation, the model failed utterly and miserably!

## Application: Distance from Independence <a id="application-distance-from-independence"></a>

We can now solve another of our earlier riddles: Given a joint distribution $r = (p,q)$, how far is it from being independent?

Let's try to fit the problem into using KL divergence. We want the answer to be KL between two distributions, one of them "the true one", the other "a model for it".

So, what should be the "true distribution"? Here it's pretty natural, we have the true joint distribution $r$ between $p$ and $q$. Next, what should the model be? Since we are trying to measure the "distance from independence", let's choose the model the joint distribution _as if $p$ and $q$ were independent_. That is, let's choose the distribution $s = p \times q$ which is the product distribution between $p$ and $q$.

$$I(p;q) = KL\bigl((p,q), (p\times q)\bigr)$$

This measure is known as the mutual information. It's one of those formulas that pop up at all kinds of places, especially in information and coding theory. For example, when you analyze the problem of sending some data over a wire that can flip each sent bit with some probability, the mutual information between the original data on one end of the wire and the noisy data on the other end of the wire is a key quantity to understanding how much the unreliability of the wire slows down the transmission. Mutual information is also used in practice to assess how far variables are from being independent, especially when correlation is not applicable (e.g., when the data are categorical).

## Entropy, Cross-Entropy, and Relative Entropy <a id="entropy-cross-entropy-and-relative-entropy"></a>

You can split the formula for KL divergence as a difference between cross entropy $H(p,q)$ and entropy $H(p)$.

Cross entropy between distributions $p$ and $q$ is measuring how surprised you are when you keep sampling from $p$ and using $q$ as your model of the data. If $p = q$, it's called entropy.

It is sometimes helpful to rewrite the formula for KL divergence as a difference of two expressions:

$$
\sum_i p_i \log \frac{p_i}{q_i} = \sum_i p_i \log \frac{1}{q_i} - \sum_i p_i \log \frac{1}{p_i}
$$

where we used the identity $\log p_i = -\log(1/p_i)$.

The left-hand side is the KL divergence (also called relative entropy). The term $\sum_i p_i \log \frac{1}{q_i}$ is called the cross-entropy, and $\sum_i p_i \log \frac{1}{p_i}$ is the entropy.

How do we interpret this equation? One way is to return to our initial example of coin flips.

Imagine a balance sheet with two columns: the left column lists the surprise (in bits) based on the true distribution $p$, and the right column lists the surprise based on the model $q$. For instance, if the model assigns a probability of 1% to heads, then whenever heads occurs the surprise is $\log(1/0.01) \approx 6.6$ bits—a significant surprise!

The KL divergence is the difference between these two surprises. Alternatively, we can consider the surprises separately. The first column, which gives the average surprise when data are generated by $p$ and modeled by $p$, is the entropy of $p$. It quantifies the inherent uncertainty in the distribution. For example, if $p$ is degenerate (100% heads), the surprise is $\log(1)=0$. In contrast, for a uniform distribution over 16 outcomes, each outcome gives 4 bits of surprise.

The second column represents the average surprise when data are generated by $p$ but modeled by $q$, which is the cross-entropy.

### Additivity <a id="additivity"></a>

Just like KL, the cross-entropy and entropy are additive for independent distributions.

## Application: Evaluating Predictions <a id="application-evaluating-predictions"></a>

As the next application of KL divergence and cross-entropy, let's solve the prediction riddle. Recall we asked about grading experts who predict future events.

To grade an expert's predictions, let $p$ denote the true probabilities of events and $q$ denote the expert's predicted probabilities. We define the expert's score as:

$$
KL(p,q)
$$

For example, if an expert's predictions are 30% and 40%, then $q$ is represented as the product distribution of $(0.3,0.7)$ and $(0.4,0.6)$. That is, we think of the expert's prediction as two distributions rather than two numbers. In this case, the formula for $KL(p,q)$ can be written as:

$$
KL(p,q) = \sum_i \left[p_i\log\frac{p_i}{q_i} + (1-p_i)\log\frac{1-p_i}{1-q_i}\right]
$$

However, there is a problem with this formulation—in reality, we do not know $p$! Instead, we can set up a grading scheme such that, in the long run, the expert's average score converges to the cross-entropy. Specifically, we charge the expert $\log(1/q_i)$ whenever an event occurs and $\log(1/(1-q_i))$ whenever it does not. This scoring rule, known as the log score, yields an expected score of:

$$
\sum_i \left[p_i\log\frac{1}{q_i} + (1-p_i)\log\frac{1}{1-q_i}\right]
$$

which is exactly the cross-entropy between the true distribution and the expert's model.

Note that we can only ensure correct grading in the long run. For instance, if you ask an expert about a single event—say, whether a random card from a deck is the ace of hearts (with true probability $1/52$) and the expert answers $1/52$—the best possible score would be 0. But since we do not have direct access to the true probabilities, we must rely on repeated experiments. Occasionally, an unlikely event (like drawing the ace of hearts) might occur, leading to a poor score for the expert. This is why it is important to ask many questions so that, by the law of large numbers, the computed score approximates the true cross-entropy.

As a further remark, we can rewrite our KL formula as:

$$
\text{cross-entropy}(p,q)=\text{entropy}(p)+KL(p,q)
$$

This shows that the log score consists of two parts: the inherent randomness (entropy) of the questions and the divergence $KL(p,q)$ that reflects how far the expert's prediction is from the truth. For example, if an expert always predicts 50/50 for fair coin tosses (so that $KL(p,q)=0$), her score will still be high because of the underlying entropy. On the other hand, if an expert predicts 50/50 for questions with a deterministic answer (e.g., 1+1=2), then $KL(p,q)$ will be large, indicating poor performance.

## Coding Theory Intuitions <a id="coding-theory-intuitions"></a>

One of the most powerful intuition pumps about entropy and related concepts comes from information theory, in particular coding theory. Let's explain it here very briefly.

If you look at Morse code, you might notice how the code for certain frequent letters like e or t are much shorter than the codes for rare letters like h or q.

To build the theory behind building codes like Morse's, let's imagine the following task. Consider the distribution over the English alphabet a,b,...,z where the probability of each letter is its approximate frequency in English text.

Our task is as follows. We should come up with some kind of binary encoding scheme so that if we get a long English text, represented as a large amount $n$ of independent samples from the above distribution, the number of bits we use is the least possible.

It turns out that the best possible solution to this problem satisfies that the average amount of bits we spend per letter -- called the rate of the code -- is $\sum p_i \log \frac{1}{p_i}$, the entropy of the distribution. For example, if the entropy of the distribution is about 4.17 bits, there is a code spending that many bits per letter on average, and there is no better code.

Instead of proving why the entropy is the answer to our question, let's see examples of (suboptimal) codes that suggest why the answer is entropy.

First, we could encode each letter using 5 bits -- a becomes 00000, b becomes 00001 and so on. The rate of this scheme is 5 bits per letter, and it can be improved quite a bit.

One way to improve it is to assign codes of different lengths to different letters, based on their frequency. The following turns out to be a good rule for the lengths: if a letter $i$ has frequency $p_i$, its code length should be about $\log \frac{1}{p_i}$.

As a specific example, if the alphabet were just a-d and the letter frequencies were $1/2, 1/4, 1/8, 1/8$, the best code is:

- a - 0
- b - 10
- c - 110
- d - 111

That is, "baba" is encoded as "100100".

You might notice how the letter with frequency $1/2^i$ has a code of length $i$; that is, frequency $p_i$ leads to a code of length $\log \frac{1}{p_i}$.

If all the probabilities in the distribution are $1/2^k$ for some number $k$, the generalization of the above code -- having $1/2^k$ probability letters have lengths $k$ -- is always possible.

If the probabilities in the distribution are not "round", you can't quite achieve a code mapping letters to binary strings as good as the entropy $H(p)$ of the distribution due to the rounding issues. You can only achieve a code of rate at most $1 + H(p)$. For example, the entropy of our English letter distribution is about $4.17$, but the best code mapping letters to bits (so-called Huffman code) has slightly worse rate of about $4.25$.

But that's not a big deal. If you allow codes that don't necessarily map single letters to bit strings, but that can also map pairs of letters and so on, then you can get codes with rate arbitrarily close to the entropy.

## Summary <a id="summary"></a>

In this section, we've introduced KL divergence and related concepts:

1. **KL Divergence** - A measure of how well a model distribution fits a true distribution
2. **Entropy** - The inherent uncertainty in a distribution
3. **Cross-Entropy** - The average surprise when using one distribution to model another
4. **Mutual Information** - A measure of dependence between two variables

We've also seen several applications:

- Training smaller models to mimic larger ones
- Measuring the distance from independence
- Evaluating predictions and experts
- Designing optimal coding schemes

These concepts form the foundation of information theory and provide powerful tools for understanding probability distributions. In the next section, we'll explore how KL divergence connects to belief updating and Bayesian reasoning.

## Next Steps <a id="next-steps"></a>

In the next part, we'll discuss how KL divergence relates to the process of updating beliefs in light of new evidence. We'll explore the connections between KL divergence, Bayes' theorem, and the Maximum Entropy principle.
