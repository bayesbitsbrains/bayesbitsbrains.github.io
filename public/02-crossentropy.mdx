# KL properties & (cross-)entropy

In this chapter, we'll check out some nice algebraic properties of KL divergence. The most useful one is that we can split it into two pieces called _cross-entropy_ and _entropy_. 

<KeyTakeaway>
![Formula for KL divergence](02-crossentropy/crossentropy_entropy_formula.png)
</KeyTakeaway>

## üîç Properties of KL divergence

Let's go through some key properties of KL divergence you should know about. Remember, KL divergence is algebraically defined like this:

<Math id="kl-definition" displayMode={true} math="D(p,q) = \sum_{i = 1}^n p_i \log \frac{p_i}{q_i}" />

Here's the biggest difference between KL and some more standard, geometrical, ways of mesuring distance like $\ell_1$ norm ($\sum |p_i - q_i|$) or $\ell_2$ norm (<Math math = "\sqrt{\sum (p_i - q_i)^2}" />). Take these two possibilities. 

1. $p_i = 0.5, q_i = 0.49$
2. $p_i = 0.01, q_i = 0.0$

Regular norms ($\ell_1, \ell_2$) think that the errors made by $q$ are about the same size. But KL knows better: The first situation is basically fine, but the second model $q$ is a total disaster! For example, letters "God" typically do not follow with "zilla", but any model of language should understand that this _may_ sometimes happen. If $q(\textrm{'zilla'} \mid \textrm{'God'}) = 0.0$, the model is going to be infinitely surprised once Godzilla arrives! 

Heads up: KL divergence can be infinite! Try to make it infinite in the following widget. Next level: Try to make it infinite while keeping $\ell_1$ and $\ell_2$ norm close to zero!

<DistributionComparisonWidget title="KL Divergence Explorer" />



### Asymmetry<a id = "asymmetry"></a>

The KL formula isn't symmetrical‚Äîin general, $D(p,q) \neq D(q,p)$. Some people see this as a disadvantage, especially when comparing KL to simple symmetric distance functions like $\ell_1$ or $\ell_2$. But I want to stress that the asymmetry is a feature, not a bug! KL measures how well a distribution $p$ is fitted by a model $q$. That's an asymmetrical thing by nature, so we need an asymmetrical formula‚Äînothing to be embarrassed about.

In fact, that's why we call it a [_divergence_](https://en.wikipedia.org/wiki/Bregman_divergence) instead of a distance‚Äîit acts kinda like a distance but isn't symmetric.

<Expand headline = "Example">
Imagine the true probability $p$ is 50%/50% (fair coin), but your model $q$ says 100%/0%. KL divergence is ... 
<Math displayMode={true} math = "\frac12 \cdot \log \frac{1}{1} + \frac12 \cdot \log \frac{1}{0} = \infty"/>

... infinite. Why? Well, you've got a 50% chance of flipping tails, which your model says should never happen. So there's a 50% chance you'll gain infinitely many bits of evidence towards $p$ (your posterior will jump to 100% fair, 0% biased).

Now flip it around: truth is 100%/0%, model is 50%/50%. Then 
<Math displayMode={true} math = "1 \cdot \log \frac{1}{1/2} + 0 \cdot \log \frac{1}{1/2} = 1"/>
Every flip gives you heads, so you gain one bit of evidence that the coin is biased. As you keep flipping, your belief in fairness drops exponentially fast, but it never hits zero. You've gotta account for the (exponentially unlikely) possibility that a fair coin just coincidentally came heads in all your past flips.
</Expand>

Here's a riddle for you! The following widget contains two distributions - one peaky and one broad. Which KL is larger? <Footnote>KL divergence also works for continuous distributions; just replace sum by integral. </Footnote>

<KLAsymmetryVisualizerWidget />


### Nonnegativity

If you plug the same distribution into KL twice, you get:

<Math displayMode={true} math="D(p, p) = \sum_{i = 1}^n p_i \cdot \log \frac{p_i}{p_i} = 0" />

because $\log 1 = 0$.
Makes sense‚Äîyou can't tell the truth apart from the truth. ü§∑

Even better, KL divergence is always nonnegative - this fact is sometimes called [Gibbs inequality](https://en.wikipedia.org/wiki/Gibbs%27_inequality). I think we built up a pretty good intuition for this in the last chapter. Just imagine sampling from $p$ but Bayes' rule somehow convinces you more and more that you are sampling from ... some other distribution $q$? That would be really messed up! 

This is not a proof though, just an argument that the world with possibly negative KL is not worth living in. Check out the formal proof if you're curious.

<Expand headline="Proof of nonnegativity">
We'll use natural logarithm to keep things short. We want to prove that <Math math = "D(p,q) =  \sum_{i = 1}^n p_i \cdot \ln \frac{p_i}{q_i}  \ge 0" /> for any $p,q$. 

Let's estimate what's inside the sum: $\ln \frac{p_i}{q_i}$. Since we know the inequality is tight when $p_i = q_i$, we need an estimate of logarithm that's tight around 1. The best linear approximation near 1 is $\ln (1+x) \le x$. We use it like this:

<Math displayMode={true} math="\begin{aligned}
-D(p,q)
&= \sum_{i = 1}^n p_i \cdot \ln \frac{q_i}{p_i}\\
&\le \sum_{i = 1}^n p_i \cdot  \left( \frac{q_i}{p_i} - 1 \right)\\
&= \sum_{i = 1}^n \left( q_i - p_i \right)\\
&= 1 - 1 = 0
\end{aligned}" />

</Expand>

### Additivity

Say you've got two distribution pairs $(p, q)$ and $(p', q')$. Then:

<Math displayMode={true} math="D(p \otimes p', q \otimes q') = D(p, q) + D(p', q')" />
where $p \otimes p'$ means the product distribution where $p,p'$ are independent.

We actually used this implicitly before‚Äîit's just saying that when you're computing KL divergence for many coin flips, you add up the divergences from each flip. 


<Expand headline="Chain rule & uniqueness" advanced={true}>
There's also a slightly fancier version of additivity called the _chain rule_. 


Say I've got distributions $p,q$ for how I get to work (\{üö∂‚Äç‚ôÄÔ∏è, üö≤, üöå\}). But when I take the bus, I also track which line (\{‚ë†, ‚ë°, ‚ë¢\}), with conditional distributions $p', q'$. Combining $p$ and $p'$ gives me an overall distribution <Math math = "p_{\textrm{overall}}" /> over \{üö∂‚Äç‚ôÄÔ∏è, üö≤, ‚ë†, ‚ë°, ‚ë¢\}. 

For example, if $p=$\{üö∂‚Äç‚ôÄÔ∏è: 0.3, üö≤: 0.3, üöå: 0.4\} and $p'=$\{‚ë†: 0.5, ‚ë°: 0.25, ‚ë¢: 0.25\}, then <Math math = "p_{\textrm{overall}}=" />\{üö∂‚Äç‚ôÄÔ∏è: 0.3, üö≤: 0.3, ‚ë†: 0.2, ‚ë°: 0.1, ‚ë¢: 0.1\}. 

The chain rule says:

<Math displayMode={true} math="
D(p_{\textrm{overall}}, q_{\textrm{overall}}) = D(p,q) + p_{\textrm{bus}} \cdot D(p',q')
"/>
where <Math math = "p_{\text{bus}}" /> is how often I take the bus according to $p$. 

This is pretty intuitive! First off, telling $p_{overall}$ apart from $q_{overall}$ is easier than just telling $p$ from $q$. 

ADD SOME MONOTONICITY

But the formula even tells us how much easier: the bus refinement helps by $D(p', q')$ whenever it comes up, which is <Math math = "p_{\textrm{bus}}" /> of the time. 

Try proving this yourself or see how it gives us additivity! 


Here's something cool: any reasonable function with monotonicity and chain rule properties [has to be KL divergence](https://blog.alexalemi.com/kl.html).

That's pretty awesome‚Äîit means KL divergence isn't some arbitrary formula someone cooked up. There's literally only one measure with these natural properties, and it's KL divergence.


</Expand>



## üß† Relative entropy = cross-entropy - entropy

Quick refresher: In the [previous chapter](01-kl_intro), we saw how KL divergence comes from repeatedly using Bayes' theorem with log-space updating:

<BayesSequenceLogWidget />

Each step adds surprisals ($\log 1/p$) to track evidence.
Last time, we focused on the differences between surprisals to see how much evidence we got for each hypothesis. Our Bayesian detective just keeps adding up these differences.

But we can also read the table column-by-column. The detective could add up the total surprisal for each hypothesis, then convert that to posterior probabilities. This is analogous to writing KL divergence like this:

<Math id="cross-entropy-decomp" displayMode={true} math='
\underbrace{\sum_{i = 1}^n p_i \log \frac{p_i}{q_i}}_{D(p,q)}
 =
\underbrace{\sum_{i = 1}^n p_i \log \frac{1}{q_i}}_{H(p,q)}
 -
\underbrace{\sum_{i = 1}^n p_i \log \frac{1}{p_i}}_{H(p)}
'/>

These two pieces on the right - $H(p,q)$ is called cross-entropy and $H(p)$ is entropy - are super important. Let's get a feel for what they mean. 

### Cross-entropy: <a id = "cross"></a>
Think of cross-entropy as _how surprised are you on average when you see data from $p$ and you're modeling it as $q$._ 

Check it out in the widget below. The widget shows what happens when our Bayesian detective from the previous chapter keeps flipping her coin, but now it shows both entropy and cross-entropy, instead of only showing the KL divergence, which is the difference between the two. 

<CrossEntropySimulator />

Analogously to KL divergence, cross-entropy is additive, so it grows linearly with the number of flips we make and we can thus represent it as a (dashed) line below. The red dashed line is telling us the expected surprise of the model $q$ as we keep flipping. The orange line is the expected surprise of $p$ - the best possible model that matches reality. KL divergence being nonnegative is equivalent to the red line _always_ being above the orange one. 

If you let the widget run, you will also see the actual surprise measured by our detective. This curve is also typically called cross-entropy - it is the cross-entropy between the _empirical distribution_ $\hat{p}$ (the actual outcomes of the flips) and the model $q$ (blue curve) or $p$ (green curve). The empirical crossentropies are tracking the dashed lines due to the [law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers). 


Bottom line: _better models are less surprised by the data and have smaller cross-entropy. KL divergence is how far our model is from the best one. _

### Entropy: 

The term $H(p) = H(p, p) = \sum_{i = 1}^n p_i \log 1 / p_i$ is a special case of cross-entropy called just plain _entropy_. It's the best possible cross-entropy you can get for distribution $p$‚Äîwhen you model it perfectly as itself.  

Intuitively, entropy tells you how much surprise or uncertainty is baked into $p$. Like, even if you know you're flipping a fair coin and hence $p = q$, you still don't know which way the coin will land. There's inherent uncertainty in that. This is what entropy measures: The fair coin's entropy is <Math math = "H(\{\textrm{H: }1/2, \textrm{T: }1/2\}) = \frac12\cdot \log2 +  \frac12\cdot \log2 = 1" /> bit. 

Entropy can get way smaller than 1 bit if say heads are very unlikely, as you can explore in the widget above and below. Makes sense! Sure, if $p(\textrm{heads} = 0.05)$, then seeing them is super surprising ($\log 1/0.05 \approx 4.32$), but it's so rare that the _average_ surprise stays tiny - you can check in the widget below that it's just $0.29$ bits per flip on average. That's because most flips are boringly predictable tails. Entropy hits zero when one outcome has 100% probability. 

But entropy can also get way bigger than 1 bit. Rolling a die has entropy $\log_2(6) \approx 2.6$ bits. In general, a uniform distribution over $k$ options has entropy $\log_2 k$‚Äîand that's the max for any distribution with $k$ options. Makes sense‚Äîyou're most uncertain when everything's equally likely.  

<EntropyWidget numCategories={6} title="Entropy of die-rolling" />

<Expand headline = "Example: correct horse battery staple">
Here's an example. Let's say there are about <Math math="2^{11}" /> English words that can be described as 'common'. If you generate uniformly four such common words and make your password the concatenation of them, the total entropy of your password is thus going to be $44$ bits. That's because entropy is a special case of cross-entropy and is thus additive. 

Having a uniform distribution with 44 bits of entropy is just a different way of saying that we have a uniform distribution with $2^{44}$ possible outcomes. 
The following comic wisely teaches us that this many possibilities make it a pretty secure password! Even if an adversary knows how we generated it, cracking it means they have to check about <Math math="2^{44}" /> passwords. 

<a href="https://xkcd.com/936/" target="_blank" rel="noopener noreferrer">
  <img src="02-crossentropy/password.png" alt="password" style={{cursor: 'pointer'}} />
</a>
</Expand>

<Expand headline = "Conditional entropy" advanced={true}>

Let's go back to the mutual information that we [encountered in the first chapter](01-kl_intro/information-theory). This is a formula that we can apply to a joint distribution $(X,Y)$:

<Math displayMode={true} math="I(X;Y) = D((X,Y), X \otimes Y)" />

Intuitively, mutual information tells us how many bits we learn about $X$ when we find out the value of $Y$ (or vice versa‚Äîit's symmetric). This can be formalized using entropy. 

First, recall the entropy formula $H(X) = \sum_{x} P(X = x) \log \frac{1}{P(X = x)}$. This formula still works if we condition on knowing that $Y$ takes a certain value $y$. We can write
<Math displayMode={true} math="H(X | Y = y) = \sum_{x} P(X = x | Y = y) \log \frac{1}{P(X = x | Y = y)}" />
The conditional entropy $H(X|Y)$ is defined as the entropy of $X$ after I sample $Y$ and learn its value, i.e.:
<Math displayMode={true} math="H(X|Y) = \sum_{y} P(Y = y) H(X | Y =y)" />

Conditional entropy has some nice properties. One of them is that $H(X|Y) \le H(X)$. That is, learning the value of $Y$ is on average only decreasing the uncertainty about $X$. In fact, the difference between the two is exactly the mutual information!

<Math displayMode={true} math="I(X;Y) = H(X) - H(X|Y)" />
It is a good exercise to write down all the definitions to check that this is true. To get some intuition about this, guess what happens if we make $P$(‚òÄÔ∏è AND üö∂‚Äç‚ôÄÔ∏è$) = P$(‚òÅÔ∏è AND üö≤$) = \frac{1}{2}$ in the widget below.  

<MutualInformationWidget />

The mutual information is then 1 bit. That's because learning the value of one distribution, say transport, makes the entropy of weather smaller by 1 bit - the weather distribution changes from a coin flip ($H(\textrm{weather}) = 1$) to either determined ‚òÄÔ∏è or determined ‚òÅÔ∏è ($H(\textrm{weather} | \textrm{transport}) = 0$). 

</Expand>


### üîê Relative entropy: 
KL divergence can be interpreted as the gap between cross-entropy and entropy. It tells us how far your average surprise (cross-entropy) is from the best possible (entropy). 
That's why in some communities, people call KL divergence the _relative entropy_ between $p$ and $q$. <Footnote>Way better name than 'KL divergence' if you ask me. But 'KL divergence' is what most people use, so I guess we're stuck with it. </Footnote>

### üéÜ So what's the big deal?

Splitting a sum into two parts isn't exactly rocket science‚Äîthe real win is that cross-entropy and entropy are super meaningful concepts on their own. <Footnote>Actually, entropy is way more famous than KL divergence. I'm building everything from KL divergence instead of entropy because I'm trying to balance between an excited brain dump about cool math that's hard to find elsewhere, and something resembling a coherent text. </Footnote>

Let's see this in action. 

<Expand headline = "üéµ Anthem battle"><a id="anthems"></a>
Let's solve our [anthem riddle](00-introduction#anthems). We have two text files, one is purely English text, the other one is a mix of all kinds of languages. 

Before clicking on the button, try to guess which of the two texts has larger entropy and which of the two KL divergences between them is larger. 
{/*If you want an even harder riddle: One of the two differences $|H(p_1) - H(p_2)|$ and $|D(p_1, p_2)- D(p_2, p_1)|$ is sizable and the other is pretty small. Which is which?*/}

<KLCalculatorWidget />

First, we can see that the mixed file has larger entropy. This makes sense! Every language uses different letters with different frequencies. For example, 'z' is quite uncommon in English, but pretty common in German. If we pool different languages together, the distribution of frequencies is becoming 'smoother', more uniform. Hence, larger entropy. Admittedly, it's only slightly larger, since increasing the probability of 'z' from 0.02 to 0.0003 is not really increasing the entropy that much.

For similar reasons, KL between $p_1$ and $p_2$ is smaller than vice versa. Remember, KL is all about probability _ratios_. If $p('e') = 0.12$ and $q('e') = 0.11$, $q$ is a good model of $p$. But if $p('z') = 0.02$ and $q('z')=0.0003$, $q$ is a crappy model of $p$. We have seen this above in the [asymmetry section](02-crossentropy#asymmetry). 

</Expand>

<Expand headline = "üîÆ How good are your predictions?"> <a id="application-evaluating-predictions"></a>

Time to solve our [prediction riddle](00-introduction#predictions). We asked experts to predict future events‚Äîhow do we score them?

### Idealized KL score

To grade predictions, let $p = (p_1, \dots, p_n)$ be the true probabilities of the events we ask our experts about, and $q$ be what the expert predicted. It for sure seems like a good idea to give the expert the following score:

<Math displayMode={true} math="S_{KL}(q) = D(p,q)" />

That is, we know that KL divergence is measuring how well a model $q$ matches the reality $p$, so we just use this a score for the expert - the lower score the better. 

If all $n$ events are independent,<Footnote>They of course never are, but here we will always assume they are. </Footnote> this formula becomes:

<Math displayMode={true} math="S_{KL}(q) = \sum_{i = 1}^n  \left(
    p_i\log\frac{p_i}{q_i} + (1-p_i)\log\frac{1-p_i}{1-q_i}
    \right)" />


But wait‚Äîthere's a huge problem with this score. Can you spot it?

### üéØ Cross-entropy score

The problem: we have no clue what the "true" probabilities are! <Footnote> We could go down a philosophical rabbit hole about whether "true" probability even is even a meaningful concept. But let's not. </Footnote>

All we know is what actually happened. This gives us an empirical distribution $\hat{p}$ where each $\hat{p}_i$ is either 0 or 1‚Äîit's all concentrated on the one outcome we saw.

What happens when we plug $\hat{p}$ into our KL score? Since $\hat{p}$'s entropy is zero (one outcome, no uncertainty), cross-entropy and relative entropy are the same: 

<Math displayMode={true} math="
S_{CE}(q) =
\sum_{i = 1}^n  \left(
    \hat{p}_i\log\frac{1}{q_i} + (1-\hat{p}_i)\log\frac{1}{1-q_i}
    \right)
"/>

This is why this formula is often called the _cross-entropy score_, though forecasting community calls it the [Log-score](https://forecasting.wiki/wiki/Log_score). 

### üîó Connection to the idealized score
Let's dig into how cross-entropy score relates to our idealized KL score. Technically, cross-entropy score is a random variable‚Äîit depends on which outcomes actually happen. Each $\hat{p}_i$ is 1 with probability $p_i$, otherwise 0.

So what's the expected cross-entropy score? Since $E_p[\hat{p}_i] = p_i$, linearity of expectation gives us:

<Math displayMode={true} math="
E_p[S_{CE}(q)] =
\sum_{i = 1}^n  \left(
    p_i\log\frac{1}{q_i} + (1-p_i)\log\frac{1}{1-q_i}
    \right)
"/>

In other words, <Math  math="E_p[S_{CE}(q)] = H(p,q)"/>. Nice! Give experts lots of questions, and by the law of large numbers, their score will approach the cross-entropy $H(p,q)$ between the true distribution and their guess. This is analogous to how [in the cross-entropy widget above](#cross), the blue line is different each time you run the experiment (it is random variable), but it tracks the dashed red line (the law of large numbers). 

Now remember:
<Math id="entropy-relation" displayMode={true} math="D(p,q) = H(p,q) - H(p)" />
We can't compute $D(p,q)$ directly, but here's the key insight: for two experts with predictions $q_1, q_2$, we have $D(p,q_1) < D(p, q_2)$ if and only if $H(p, q_1) < H(p, q_2)$. They only differ by $H(p)$, which doesn't depend on the experts' predictions. So:

_Comparing experts by cross-entropy is just as good as comparing by KL divergence in the long run!_

<Expand headline="Example: Coin flipping">
Let's make this concrete. We flip a fair coin $N$ times ($p_1 = \dots = p_N = 1/2$). Expert 1 nails it ($q_1 = \dots = q_N = 1/2$), while Expert 2 is a bit off ($q'_1 = \dots = q'_N = 0.6$).

The idealized KL scores: $KL(p, q) = N \cdot 0 = 0$ and $KL(p, q') = N \cdot D(p_1, q'_1) \approx 0.03 \cdot N$.

We can't see these scores, but we can compute cross-entropy. For large $N$, the law of large numbers says the scores will be roughly $H(p,q) = N$ and $H(p, q') \approx 1.03 \cdot N$. Both are bigger by $H(p) = N$‚Äîthe inherent uncertainty of coin flips. Even the perfect expert gets a high score! But since both shift by the same amount, cross-entropy still picks the best expert just like KL would. 

Key point: this only works _in the long run_. With just 8 predictions, any scoring is pretty noisy. 
</Expand>

Try the log-score on our example! Also, you can compare it with other popular score called brier score, which is just the so-called mean squared error or $\ell_2$ metric (i.e., if you predicted 0.8 probability and the event happens, your score is $(1-0.8)^2 = 0.04$). Brier score does not really care whether your failed prediction had probability of $0.9$ or $0.9999$. 

<ExpertRatingWidget
    title="Comparing Scoring Methods"
    showBrierScore={true}
  />

</Expand>


## üöÄ What's next? <a id="next-steps"></a>

We're getting the hang of KL divergence! Quick recap:

![Formula for KL divergence](02-crossentropy/crossentropy_entropy_formula.png)

If this really measures how well a model fits the truth, then making it small should give us good models, right? [Let's find out in the next chapter](03-minimizing)!