# KL properties & (Cross-)entropy

In this chapter, we will discuss a few nice algebraic properties of KL divergence. Also, it's often very useful to split the formula into two terms called _crossentropy_ and _entropy_. 


We will also go through a few basic properties that these things have.

## Properties of KL divergence

Let's go through several properties of KL divergence it's good to understand. Remember that KL divergence is algebraically defined like this:

$$
D(p,q) = \sum_{i = 1}^n p_i \log \frac{p_i}{q_i}
$$

KL divergence can be infinite: this happens whenever $q_i \not= 0$ and $p_i = 0$ for some $i$. We also use convention $0 \cdot \log 0/0 = 0$ so that KL is always a well-defined number or $+\infty$.

## Asymmetry

The KL formula is not symmetrical -- in general, we don't have $D(p,q) = D(q,p)$. This is sometimes seen as a drawback, especially when you compare KL with (symmetrical) distance function like the $\ell_1$ or $\ell_2$ metric<Footnote> <Math math='d_1(p,q) = \sum_{i = 1}^n |p_i - q_i|'/>, <Math math='d_2(p,q) = \sqrt{\sum_{i = 1}^n (p_i - q_i)^2}'/></Footnote>. I want to stress that the asymmetry is by design! KL measures how well a distribution $p$ is fitted by a model $q$. This is an asymmetrical notion, so we need an asymetrical formula, nothing to be ashamed for here.

In fact, the word _divergence_ is used for things like KL that behave kind of like distances but are not really symmetrical. <Footnote>More on that in [one of the last chapters](06-algorithms)</Footnote>

<Expand headline = "Example">
Imagine that the true probability distribution is 50\%/50\% (i.e., fair coin), and the model is $100\% / 0\%$. In that case, KL divergence is infinite. That's because if we flip a coin, we have 50\% chance of flipping tails, which the model distribution says should never happen. This means that we have 50\% chance of gaining infinitely many bits of information (whatever our prior on fair/biased was, the posterior should now be 100\% fair and 0\% biased) and hence the divergence is infinite.

On the other hand, if the truth is $100\% / 0\%$ and the model is $50\% / 50\%$, then $D(p,q) = 1$. That's because each flip, we are going to flip heads and thus gain one bit of evidence towards the coin being fair. One bit is a lot and our probability of the coin being fair goes down really fast, but it's never zero. We simply have to account for the (exponentially unlikely) event that even if the coin is fair, all flips may have happenned to be heads.
</Expand>

## Nonnegativity

If you plug in the same distribution twice into KL, you get

$$
D(p, p) = \sum_{i = 1}^n p_i \cdot \log \frac{p_i}{p_i} = 0
$$

because $\log 1 = 0$.
This makes sense, you can never distinguish the truth from the truth. 🤷

Even better, KL divergence is always nonnegative. I think we got a pretty strong intuition why in the previous chapter. Just imagine that you have two distributions $p$ and $q$, but as you keep sampling from $p$, you gradually keep believing more and more that the distribuition actually is ... $q$? The world would be really, really messed up in that case! Feel free to check the formal proof below. 


<Expand headline="Proof of nonnegativity">
We will use the natural logarithm to make the proof a bit shorter, i.e., we want to prove that <Math math = "D(p,q) =  \sum_{i = 1}^n p_i \cdot \ln \frac{p_i}{q_i}  \ge 0" /> holds for any $p,q$. 

We will estimate the expression $\ln \frac{p_i}{q_i}$ inside the summation. Since we already know that the inequality is tight if all $p_i = q_i$, we should choose an estimate of logarithm which is tight around 1. The best approximation of logarithm around $1$ is $\ln (1+x) \le x$. We can use it like this:

$$
-D(p,q)
= \sum_{i = 1}^n p_i \cdot \ln \frac{q_i}{p_i}
\le \sum_{i = 1}^n p_i \cdot  \left( \frac{q_i}{p_i} - 1 \right)
= \sum_{i = 1}^n q_i - p_i
= 1 - 1 = 0
$$

</Expand>


A bit stronger version of this fact is also called monotonicity. We won't really use it, so feel free to skip it. 

<Expand headline="Monotonicity explanation">
Imagine that we have two distributions $p,q$, let's say they are over countries (\{🇺🇸,🇨🇦,🇲🇽,🇯🇵,🇬🇧,🇮🇳,...}). Then, we zoom out a little bit and consider a set of larger geographical areas (\{North America, Europe, ...}). We can convert $p,q$ to this rougher set to get distributions $p', q'$ -- e.g. we set <Math math = "p'(\textrm{North America}) = p(🇺🇸) + p(🇨🇦) + p(🇲🇽)" />. Then it's the case that

$$
D(p, q) \ge D(p', q')
$$

This makes sense! Remember that KL divergence tells us how hard it is to separate $p$ from $q$ using Bayes' theorem. This just says that for our Bayesian reasoner, it gets harder to distinguish $p$ from $q$, as we keep grouping different outcomes together. In the special case, we could group all the outcomes to a single outcome SOMETHING_HAPPENED -- the inequality would than reduce to $D(p,q) \ge 0$, i.e., nonnegativity is a special case of this.
</Expand>

## Additivity

Let's say that we have two distribution (the "truth" and the "model") $p, q$ and then another two distributions $p', q'$. It's true that

$$
D(p \otimes p', q \otimes q') = D(p, q) + D(p', q')
$$
where $p \otimes p'$ stands for the product distribution with marginals $p,p'$, i.e. the joint distribution where $p,p'$ are independent.

We have used this property implicitly in the previous chapter -- it allowed us not to compute the overall KL divergence as the sum for individual flips. 

<Expand headline="Proof of additivity">
The proof is to just write it down. Here' it is (by Claude)':
Let's start with the definition of KL divergence for the joint distributions:

<Math displayMode={true} math="
D(p \otimes p', q \otimes q') = \sum_{i=1}^{n}\sum_{j=1}^{m} p(i)p'(j) \log\left(\frac{p(i)p'(j)}{q(i)q'(j)}\right)
"/>

Using properties of logarithms, we can rewrite this as:

<Math displayMode={true} math="
D(p \otimes p', q \otimes q') = \sum_{i=1}^{n}\sum_{j=1}^{m} p(i)p'(j) \left[\log\left(\frac{p(i)}{q(i)}\right) + \log\left(\frac{p'(j)}{q'(j)}\right)\right]
"/>

This expands to:

<Math displayMode={true} math="
D(p \otimes p', q \otimes q') = \sum_{i=1}^{n}\sum_{j=1}^{m} p(i)p'(j) \log\left(\frac{p(i)}{q(i)}\right) + \sum_{i=1}^{n}\sum_{j=1}^{m} p(i)p'(j) \log\left(\frac{p'(j)}{q'(j)}\right)
"/>

Now, we can factor out terms:

<Math displayMode={true} math="
D(p \otimes p', q \otimes q') = \sum_{i=1}^{n} p(i) \log\left(\frac{p(i)}{q(i)}\right) \sum_{j=1}^{m} p'(j) + \sum_{j=1}^{m} p'(j) \log\left(\frac{p'(j)}{q'(j)}\right) \sum_{i=1}^{n} p(i)
"/>

Since probability distributions sum to 1:
$$\sum_{i=1}^{n} p(i) = 1 \text{ and } \sum_{j=1}^{m} p'(j) = 1$$

This gives us:

<Math displayMode={true} math="
D(p \otimes p', q \otimes q') = \sum_{i=1}^{n} p(i) \log\left(\frac{p(i)}{q(i)}\right) + \sum_{j=1}^{m} p'(j) \log\left(\frac{p'(j)}{q'(j)}\right)"/>

Which is exactly:

$$D(p \otimes p', q \otimes q') = D(p, q) + D(p', q')$$
</Expand>


In fact, a slightly stronger property called _chain rule_ is true. We won't really use it, so feel free to skip. 

<Expand headline="Chain rule explanation">
Imagine that we have two distributions $p,q$ for how I commute to work (\{🚶‍♀️, 🚲, 🚌}). Later, I decide that when I take bus, I will also keep track of what line I took (\{①, ②, ③}); I again have two (so-called conditional) distributions $p', q'$ for that. Now, $p$ and $p'$ can be combined into an overall distribution <Math math = "p_{\textrm{overall}}" /> over the set \{🚶‍♀️, 🚲, ①, ②, ③}. For example, if $p=$\{🚶‍♀️:0.3, 🚲:0.3, 🚌:0.2\} and $p'=$\{①: 0.5, ②:0.25, ③:0.25\}, then <Math math = "p_{\textrm{overall}}=" />\{🚶‍♀️:0.3, 🚲:0.3, 🚌①: 0.1, 🚌②:0.05, 🚌③:0.05\}. 

The chain rule property of KL divergence is the fact that

<Math displayMode={true} math="
D(p_{\textrm{overall}}, q_{\textrm{overall}}) = D(p,q) + p_{\textrm{bus}} \cdot D(p',q')
"/>
Here, <Math math = "p_{\text{bus}}" /> is the probability of 🚌 according to the (true) distribution $p$. 

This property makes sense! It says that distinguishing the truth $p_{overall}$ from the model $q_{overall}$ can't be harder than distinguishing $q$ from $p$. In fact, the additional refinement of the option 🚌  is helping us whenever it comes up. Algebraically, we just add the two relevant divergences together, with $D(p', q')$ weighted by the frequency <Math math = "p_{\textrm{bus}}" />. 

Try to derive this property or see how it implies additivity! 
</Expand>


## Relative entropy = Crossentropy - Entropy

A quick recap: The [previous chapter](01-kl_intro), we've seen how KL divergence is really the result of repeatedly applying Bayes' theorem, using this table:

![Surprisal](01-kl_intro/repeated_bayes_surprisals.png)

Each cell in this table holds a value of type $\log 1/p$ that's also sometimes called the _surprisal_.

Last time, we have been parsing this table "row-by-row". That is, we always subtracted two surprisals to compute how the given flip changes our posterior. The Bayesian reasoner then keeps summing up all those surprisal differences.

We can also try to understand the table "column-by-column". That is, the Bayesian reasoner can compute the total surprisal for both hypothesis, then convert those into the final posterior probability. This corresponds to rewriting the KL divergence as follows:

<Math displayMode={true} math='
\underbrace{\sum_{i = 1}^n p_i \log \frac{p_i}{q_i}}_{D(p,q)}
 =
\underbrace{\sum_{i = 1}^n p_i \log 1 / q_i}_{H(p,q)}
 -
\underbrace{\sum_{i = 1}^n p_i \log 1 / p_i}_{H(p)}
'/>

$H(p,q)$ is called _crossentropy_. You should think of this expression as telling you how surprised you are, on average, if you keep seeing data from $p$ and try to model them by $q$.

$H(p) = H(p, p)$ is an important special case called _entropy_. Intuitively, it's telling you how much surprise there is, if you keep seeing data sampled from $p$ (and you know they are coming from $p$).

Finally, $D(p,q)$ is the difference between these two values. That's also why in some communities, it's known as the _relative entropy_ between $p$ and $q$. <Footnote>Much better name than KL divergence, if you ask me. The name KL divergence seems to be much more common, though, so we stick with it. </Footnote>

### So what is this good for?

Splitting a summation into two summations is not some kind of hard-core mathematical trick; the main advantage of adding crossentropy & entropy to our vocabulary is that they are simply very meaningful concepts. Let's solve a few more riddles and see what I mean by that!

## Two important properties of (cross-, relative-) entropy

All our entropies have bunch of intuitive properties. Here are a few of them.

### Additivity

Let's say that you have two distribution (the "truth" and the "model") $p, q$ and then another two distributions $p', q'$. It's true that


$$
D(p \otimes p', q \otimes q') = D(p, q) + D(p', q')
$$
where $\otimes$ stands for the product distribution, i.e. the joint distribution where $p,p'$ are independent.

<Expand headline="Proof of additivity">
Here's the proof (by Claude):
Let's start with the definition of KL divergence for the joint distributions:

<Math displayMode={true} math="
D(p \otimes p', q \otimes q') = \sum_{i=1}^{n}\sum_{j=1}^{m} p(i)p'(j) \log\left(\frac{p(i)p'(j)}{q(i)q'(j)}\right)
"/>

Using properties of logarithms, we can rewrite this as:

<Math displayMode={true} math="
D(p \otimes p', q \otimes q') = \sum_{i=1}^{n}\sum_{j=1}^{m} p(i)p'(j) \left[\log\left(\frac{p(i)}{q(i)}\right) + \log\left(\frac{p'(j)}{q'(j)}\right)\right]
"/>

This expands to:

<Math displayMode={true} math="
D(p \otimes p', q \otimes q') = \sum_{i=1}^{n}\sum_{j=1}^{m} p(i)p'(j) \log\left(\frac{p(i)}{q(i)}\right) + \sum_{i=1}^{n}\sum_{j=1}^{m} p(i)p'(j) \log\left(\frac{p'(j)}{q'(j)}\right)
"/>

Now, we can factor out terms:

<Math displayMode={true} math="
D(p \otimes p', q \otimes q') = \sum_{i=1}^{n} p(i) \log\left(\frac{p(i)}{q(i)}\right) \sum_{j=1}^{m} p'(j) + \sum_{j=1}^{m} p'(j) \log\left(\frac{p'(j)}{q'(j)}\right) \sum_{i=1}^{n} p(i)
"/>

Since probability distributions sum to 1:
$$\sum_{i=1}^{n} p(i) = 1 \text{ and } \sum_{j=1}^{m} p'(j) = 1$$

This gives us:

<Math displayMode={true} math="
D(p \otimes p', q \otimes q') = \sum_{i=1}^{n} p(i) \log\left(\frac{p(i)}{q(i)}\right) + \sum_{j=1}^{m} p'(j) \log\left(\frac{p'(j)}{q'(j)}\right)"/>

Which is exactly:

$$D(p \otimes p', q \otimes q') = D(p, q) + D(p', q')$$
</Expand>

This is super important! In fact, in the precedening chapter, it was important that the flips of the coin were independent, so to make it more formal, we would really rely on this additive property. Crossentropy and entropy are also additive for independent distributions.

In fact, a slightly stronger property called _chain rule_ is true.

<Expand headline="Chain rule explanation">
Imagine that we have two distributions $p,q$ for how I commute to work (\{🚶‍♀️, 🚲, 🚌}), but later I decide that when I take bus, I will also keep track of what line I took (\{①, ②, ③}); I again have two distributions $p', q'$ for that. Now, $p$ and $p'$ can be combined into an overall distribution $p_{overall}$ over the set \{🚶‍♀️, 🚲, ①, ②, ③}; we combine $q,q'$ into $q_{overall}$ as well. The chain rule property says that

<Math displayMode={true} math="
D(p_{overall}, q_{overall}) = D(p,q) + p_{\text{bus}} \cdot D(p',q')
"/>

One can prove the chain rule property similarly to how additivity is proven.
</Expand>

### Nonnegativity

We have already seen in the [previous chapter](01-kl_intro) that if KL were negative for some two distributions, something would be severely wrong with the universe. It is true that for any two $p,q$, we have



$$
D(p,q) \ge 0
$$

Because $D(p,q) = H(p,q) - H(p)$, we can also view this fact as saying that the crossentropy $H(p,q)$ for fixed $p$ is minimized if we choose $q = p$. That is, our Bayesian friend adding surprisals is going to be surprised the least if her model matches the data.

A bit stronger version of this fact is also called monotonicity

<Expand headline="Monotonicity explanation">
Imagine that we have two distributions $p,q$, let's say they are over countries (\{🇺🇸,🇨🇦,🇲🇽,🇯🇵,🇬🇧,🇮🇳,...}). Then, we zoom out a little bit and consider a smaller set of geographical areas (\{North America, Europe, ...}). We can convert $p,q$ to this rougher set to get distributions $p', q'$ -- e.g. we set $p'(North America) = p(🇺🇸) + p(🇨🇦) + p(🇲🇽)$. Then it's the case that

$$
D(p, q) \ge D(p', q')
$$

This makes sense! Remember that KL divergence means how hard is it to separate $p$ from $q$ using Bayes' theorem. This just says that for our Bayesian reasoner, it gets harder to distinguish $p$ from $q$, as we keep grouping different outcomes together. In the special case, we could group all the outcomes to a single outcome SOMETHING_HAPPENED -- the inequality would than reduce to $D(p,q) \ge 0$.

Also, monotonicity is pretty similar to the so-called [Data processing inequality](https://en.wikipedia.org/wiki/Data_processing_inequality) you might have heard of.
</Expand>

### Uniqueness

As it turns out, some version of additivity and monotonicity can be used to uniqely pinpoint KL divergence and entropy. By that I mean that any reasonable function of one parameter that has these two properties already [has to be the entropy](<https://en.wikipedia.org/wiki/Entropy_(information_theory)#Characterization>), and any reasonable function of two parameters with these properties [has to be KL divergence](https://blog.alexalemi.com/kl.html).

This is pretty cool and it helps us to appreciate that our definitions are not arbitrary at all. There's just one measure with these natural properties, and this measure is KL divergence.

## Application: Evaluating Predictions <a id="application-evaluating-predictions"></a>

As the next application of KL divergence and cross-entropy, let's solve our [prediction riddle](00-introduction/predictions). Recall that we asked about a few experts to predict future events and recorded their probabilities -- how do we measure their performance?

### Idealized KL score

To grade an expert's predictions, let $p$ denote the true probabilities of events and $q$ denote the expert's predicted probabilities. It sure sounds like a good idea to define the expert's score as:

$$
KL score = D(p,q)
$$

We will for simplicity assume that all the events are independent, where this simplifies to the following formula:

$$
KL score = \sum_{i = 1}^n  \left(
    p_i\log\frac{p_i}{q_i} + (1-p_i)\log\frac{1-p_i}{1-q_i}
    \right)
$$

For example, if we ask an expert about two independent events that happen with probabilities $30\%$ and 90\%, and she estimates the probabilities as $50\%$ and $80\%$, her score would be

$$
KL score = 0.3 \cdot \log 0.3/0.5 + 0.7 \cdot \log 0.7/0.5 \\
+ 0.9 \cdot \log 0.9 / 0.8 + 0.1 \cdot 0.1 / 0.2
\approx ??.
$$

There's just one problem with our approach -- can you see it?

### Crossentropy score

The problem is that when we are grading, we have typically absolutely no idea what the "true" probabilities are! <Footnote> We could now have a very long philosophical discussion about whether something like "true" probability even exists. Fortunately, nothing in this text really depends on these philosophical subtleties. </Footnote>

The only fact we know is whether each event happenned or not. Fortunately, this also corresponds to a valid probability distribution $\hat{p}$ -- it's simply the case that $\hat{p}_i = 0$ or $\hat{p}_i = 1$ for each $i$.

We can now define a new, feasible to compute, score function:

<Math displayMode={true} math="
crossentropy score =
\sum_{i = 1}^n  \left(
    \hat{p}_i\log\frac{\hat{p}_i}{q_i} + (1-\hat{p}_i)\log\frac{1-\hat{p}_i}{1-q_i}
    \right)
"/>

Why do we call it the _crossentropy score_? At this point, I should mention that to define KL divergence, we use the convention that $0 \cdot \log 0 = 0 \cdot \log 0/0 = 0$. This means that our equation actually simplifies to

$$
crossentropy score =
\sum_{i = 1}^n  \left(
    \hat{p}_i\log\frac{1}{q_i} + (1-\hat{p}_i)\log\frac{1}{1-q_i}
    \right)
$$

That is, the score is both equal to the KL divergence between $\hat{p}$ and $q$, and to crossentropy between them. <Footnote> Another way to see that is via the equation $D(\hat{p}, q) = H(\hat{p}, q) - H(\hat{p})$; the distribution $\hat{p}$ is a trivial distribution supported only on one outcome, so its entropy is 0 and we thus have $D(\hat{p}, q) = H(\hat{p}, q)$. </Footnote> So we might as well call this crossentropy score. In the forecasting community, this score is typically called [Log-score](https://forecasting.wiki/wiki/Log_score), the crossentropy score notation is closer to how this is called in machine learning.

Before we compute the score, let's understand more deeply how it connects to the idealized KL score. Technically speaking, crossentropy score is a random variable because it depends on the "true" randomness of $p$. More concretely, each $\hat{p}_i$ is actually a random variable equal to 1 with probability $p_i$, and otherwise equal to 0.

So what's the expected value of the crossentropy score, i.e., $E_p[crossentropy score]$? Well, since $\hat{p_i}$ is on average equal to $p_i$, i.e., $E_p[\hat{p}_i] = p_i$, we simply have

$$
E_p[crossentropy score] = H(p, q)
$$

This is pretty cool! If we give our experts many questions, then in the long run, we can use the law of large numbers to argue that the score we measure is close to the crossentropy $H(p,q)$.

At this point, let's remember the equation

$$
D(p,q) = H(p,q) - H(p)
$$

Although we can't compute the value of $D(p,q)$, notice that for two expert predictions $q_1, q_2$, we have $D(p,q_1) < D(p, q_2)$ if and only if $H(p, q_1) < H(p, q_2)$: KL divergence and crossentropy only differ by $H(p)$ which is in this case just some constant. That is, _if we compare the experts based by their cross entropy score, it's in the long run the same as comparing them by their KL score!_

### Example

<Block headline="Example: Coin flipping">
Let's see an example of this. Let's say we flip a fair coin $N$ times ($p_1 = \dots = p_N = 1/2$) and ask two experts for their probabilities. Let's say that the first expert predicts correctly ($q_1 = \dots = q_N = 1/2$) and the other one is a bit off ($q'_1 = \dots = q'_N = 0.6$).

Then, the idealized KL score happens to be $KL(p, q) = N \cdot 0 = 0$ and $KL(p, q') = N \cdot D(p_1, q'_1) \approx 0.03 \cdot N$.

We don't have access to this score, but if we compute their crossentropy scores, the law of large numbers tells us that for large enough $N$, the scores are going to be roughly $H(p,q) = N$ and $H(p, q') \approx 1.03 \cdot N$. Both crossentropies are shifted by the entropy of the underlying process which is $N$. So we are penalizing both experts for something they can't know. But that's OK since both values are shifted by the same amount, so if we use the score to e.g. select the best experts, it behaves the same as using the ideal KL score.

We will not bother computing the cross entropy score for the example data. The reason is that, as we have seen, our score is only reasonable if we can argue via the law of large numbers that the value we compute is going to be close to the crossentropy $H(p,q)$. But that's very fishy assumption for just 8 predictions, especially if some of the events happen with very small or very large probability.
</Block>

## Coding Theory Intuitions <a id="coding-theory-intuitions"></a>

One of the most powerful intuition pumps about entropy and related concepts comes from information theory, in particular coding theory. Let's explain it here very briefly.

If you look at Morse code, you might notice how the code for certain frequent letters like e or t are much shorter than the codes for rare letters like h or q.

To build the theory behind building codes like Morse's, let's imagine the following task. Consider the distribution over the English alphabet a,b,...,z where the probability of each letter is its approximate frequency in English text.

Our task is as follows. We should come up with some kind of binary encoding scheme so that if we get a long English text, represented as a large amount $n$ of independent samples from the above distribution, the number of bits we use is the least possible.

It turns out that the best possible solution to this problem satisfies that the average amount of bits we spend per letter -- called the rate of the code -- is $\sum p_i \log \frac{1}{p_i}$, the entropy of the distribution. For example, if the entropy of the distribution is about 4.17 bits, there is a code spending that many bits per letter on average, and there is no better code.

Instead of proving why the entropy is the answer to our question, let's see examples of (suboptimal) codes that suggest why the answer is entropy.

First, we could encode each letter using 5 bits -- a becomes 00000, b becomes 00001 and so on. The rate of this scheme is 5 bits per letter, and it can be improved quite a bit.

One way to improve it is to assign codes of different lengths to different letters, based on their frequency. The following turns out to be a good rule for the lengths: if a letter $i$ has frequency $p_i$, its code length should be about $\log \frac{1}{p_i}$.

As a specific example, if the alphabet were just a-d and the letter frequencies were $1/2, 1/4, 1/8, 1/8$, the best code is:

- a - 0
- b - 10
- c - 110
- d - 111

That is, "baba" is encoded as "100100".

You might notice how the letter with frequency $1/2^i$ has a code of length $i$; that is, frequency $p_i$ leads to a code of length $\log \frac{1}{p_i}$.

If all the probabilities in the distribution are $1/2^k$ for some number $k$, the generalization of the above code -- having $1/2^k$ probability letters have lengths $k$ -- is always possible.

If the probabilities in the distribution are not "round", you can't quite achieve a code mapping letters to binary strings as good as the entropy $H(p)$ of the distribution due to the rounding issues. You can only achieve a code of rate at most $1 + H(p)$. For example, the entropy of our English letter distribution is about $4.17$, but the best code mapping letters to bits (so-called Huffman code) has slightly worse rate of about $4.25$.

But that's not a big deal. If you allow codes that don't necessarily map single letters to bit strings, but that can also map pairs of letters and so on, then you can get codes with rate arbitrarily close to the entropy.

## Next Steps <a id="next-steps"></a>

We are getting comfortable with KL divergence! A quick recap:

![Formula for KL divergence](02-crossentropy/crossentropy_entropy_formula.png)

If it is really the right way to measure how well a model fits the truth, it sure seems that making it small is a good way to build good models!? [Let's see in the next chapter](03-minimizing)!



