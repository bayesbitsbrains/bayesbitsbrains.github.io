# All those riddles <a id="introduction"></a>

This site contains an introduction to various topics at the intersection of probability, information theory, and machine learning. 
{/*Our tour guide through this corner of the world is a formula called _Kullback-Leibler (KL) divergence_. */}

Check out [this page](about) for logistics. Here, I've got several riddles. I hope that some of them will [nerd-snipe](https://xkcd.com/356/) you! üòâ You will understand all of them at the end of this minicourse. 


<Expand headline = "üß† Intelligence test">
Test your intelligence with the following widget! You will be given a bunch of text snippets cut from Wikipedia cut at a random place. Your task is to predict the next letter. You can compare your performance with some neural nets! 

<LetterPredictionWidget/>

Why are neural nets so good in this? Why should this game measure intelligence? And why did Claude Shannon - the information theory [GOAT](https://www.merriam-webster.com/dictionary/goat) - make this experiment in 1940's? 
</Expand>

<Expand headline="üîÆ How good are your predictions?"> <a id="predictions"></a>

It'd be awesome to know the future‚Äîor at least know someone who does. So you've gathered some experts and came up with five questions (Q1 to Q5). You ask the experts to give you probabilities for all 5 questions.
A year later, you know what actually happened and are ready to find the best expert.  

![questions](00-introduction/questions.png)

Question is: who's the best here?

<MultipleChoiceQuestion
  options={["üßë Expert 1", "üëµüèø Expert 2", "üë∂ Expert 3"]}
  correctIndices={[0, 1, 2]}
  feedbackType="all-show"
  explanation={<>This is actually hard to tell with just 5 questions, we would need many more of them. But I am not a big fan of üßë though, with his failed 99%-confidence prediction! Later, we'll see how KL divergence gives us the log-score that forecasting tournaments use; this score penalizes failed 99%-confidence predictions quite heavily!   
  <ExpertRatingWidget
    title="Log score"
    showBrierScore={false}
  />  </> }
  />
</Expand>



<Expand headline="üìà S&P shape"> <a id="financial-mathematics"></a>

{/* ![Financial Data Distribution](00-introduction/sap_graph.png) */}

Every day, the S&P index price jumps around a bit. <Footnote>If you don't know what the S&P is, just think Bitcoin, Apple stock, or how many euros you can buy with a dollar</Footnote>

I grabbed some historical data, calculated the daily price changes, normalized them <Footnote>I.e., I am plotting the so-called log-return <Math math = "\ln P_t/P_{t-1} \approx 1 + \frac{P_t - P_{t-1}}{P_{t-1}}" /> for each consecutive daily prices $P_{t-1}, P_t$. </Footnote> and threw them in a histogram. The x-axis shows how big the change was (positive about half the time, negative the other half) anad the y-axis shows how often that size change happens. Before I show you the plot, take a guess‚Äîwhat's it gonna look like? 

![shapes](00-introduction/shapes.png)

<MultipleChoiceQuestion
  options={["A", "B", "C"]}
  correctIndices={[1, 2]}
  feedbackType="all-show"
  explanation={<>This is hard to say, try it below yourself. I fitted the data by two curves - Gaussian distribution (looks like the shape in C), and Laplace distribution (looks like the shape in B). 
  <FinancialDistributionWidget showBTC={false} showSAP={true} />
  What's happening here? We will discuss this example in the [chapter about the max-entropy principle](05-max_entropy). This principle is extremely useful in situations like ours, when we want to model some kind of data. <br/><br/></>}
/>

</Expand>





<Expand advanced={false} headline="üåê How large is Wikipedia?"><a id="wikipedia"></a>
Marcus Hutter, an AI researcher, started <a href="http://prize.hutter1.net/">this challenge</a>: Take a 1GB file of Wikipedia text. How small can you compress it? <a href="https://en.wikipedia.org/wiki/ZIP_(file_format)">Zipping it</a> gets you about 300MB. But we can do way better. What's the current record? 

<MultipleChoiceQuestion
  options={["around 1MB", "around 10MB", "around 100MB"]}
  correctIndices={[2]}
  explanation={<>It's around 100MB. But wait‚Äîwhy does an <em>AI</em> researcher care about compression? We'll see the connection with <a href="02-crossentropy/coding">entropy</a>, <a href = "02-crossentropy/hutter">next token prediction</a>, and <a href="#">Kolmogorov complexity</a>. In the meantime, you can try to guess how well can various other texts be compressed by various algorithms. 
  <CompressionWidget />
</>}
/> 
</Expand>

<Expand headline="ü¶∂ Average foot"> <a id="statistics"></a>

![rod](00-introduction/rod.png "Determining the length of the foot in 1522; taken from https://commons.wikimedia.org/wiki/File:Determination_of_the_rute_and_the_feet_in_Frankfurt.png")

Back in the day, people measured stuff in feet. Sure, we all kinda know how long a foot is, but eventually you need to nail it down precisely. One way (shown above) is to grab 16 random people and average their foot lengths:
<Math displayMode={true} math="\bar X = \frac{1}{16} (X_1 + \dots + X_{16})" />

This gives you a pretty stable number that shouldn't change much if you do it again. To measure how good your estimate is, you'd usually calculate the standard deviation. Remember the formula? Which one's right:

<MultipleChoiceQuestion
  options={[
    <Math math="\bar{\sigma}^2 = \frac{1}{15} \sum_{i=1}^{16} (X_i - \bar{X})^2" />,
    <Math math="\bar{\sigma}^2 = \frac{1}{16} \sum_{i=1}^{16} (X_i - \bar{X})^2" />,
    <Math math="\bar{\sigma}^2 = \frac{1}{17} \sum_{i=1}^{16} (X_i - \bar{X})^2" />
  ]}
  correctIndices={[0, 1, 2]}
  feedbackType="all-show"
  explanation={<>In a sense, they're all correct! Well, more like they're all defensible options from the viewpoint of frequentist statistics. Using <Math math="1/(n-1)" /> gives you the <a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">unbiased estimate</a>, <Math math="1/n" /> gives the <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood estimate</a>, and <Math math="1/(n+1)" /> minimizes <a href="https://en.wikipedia.org/wiki/Mean_squared_error">the mean squared error</a> between your guess and the truth.<br/><br/>But here's the thing: out of all these ways to estimate stuff, only maximum likelihood became the rockstar of machine learning. Using KL divergence, <a href="04-minimizing#mle">we'll see</a> why maximum likelihood is so special and what makes it tick.</>}
/>
</Expand>


{/*
<Expand headline="üß† LLM training"> <a id="deep-learning"></a>

So you wanna train a large language model (LLM) like GPT/Gemini/Claude. These beasts take text, do some crazy computations, and spit out the next letter (Well, actually the next [token](https://en.wikipedia.org/wiki/Large_language_model#Tokenization), which is like a little chunk of letters).
LLMs don't just guess a single letter‚Äîthey predict the whole distribution $p$ of what might come next. 

Training LLMs is complicated as hell, but one super important bit is picking the _loss function_. Here's the deal: take some text that shows up everywhere online, like "My name is A". We know what usually comes next‚Äîmaybe "l" is common (all those Alexes), but "a" not so much. Call this the ground-truth distribution $p$.

Your LLM tries to guess this distribution with its own guess $q$. We need to measure how close $q$ is to $p$ using a loss function, then make that number as small as possible during training. So given distributions $p = \{p_1, \dots, p_k\}$ and $q = \{q_1, \dots, q_k\}$, which loss function should we pick?

<MultipleChoiceQuestion
  options={[
    <><Math math="\mathcal L(p,q) = \sum_{i = 1}^k |p_i - q_i|" /></>,
    <><Math math="\mathcal L(p,q) = \sum_{i = 1}^k (p_i - q_i)^2" /></>,
    <><Math math="\mathcal L(p,q) = \sum_{i = 1}^k p_i \cdot \log (p_i / q_i)" /></>
  ]}
  correctIndices={[2]}
  explanation={<>The first two work okay, but practioners typically use KL divergence. Here's why it's nice: if <Math math="p_i = 0.5" /> and <Math math="q_i = 0.49" />, KL thinks that's basically fine. But if <Math math="p_i = 0.01" /> and <Math math="q_i = 0.0" />, KL freaks out (the ratio <Math math="p_i/q_i" /> is infinite). On the other hand, the other two loss functions think both situations are similar problem. <br/><br/>Check out this widget with two distributions and all three loss functions. <br/><br/><DistributionComparisonWidget title="KL Divergence Explorer" /></>}
/>
</Expand>
*/}

<Expand headline="üîó Distance from independence"> <a id="information-theory"></a>

Independence is huge in probability. Quick refresher: you've got two distributions $p_1$ and $p_2$. Maybe $p_1$ is about weather (‚òÄÔ∏è or ‚òÅÔ∏è) and $p_2$ is how I get to work (üö∂‚Äç‚ôÄÔ∏è, üö≤, or üöå). A joint distribution is a table showing the probability of each combo. Here are three possible joint distributions for three different people:

![Which table is the "most" independent?](00-introduction/independence.png)

All three have the same _marginals_: 70% good weather, and 20%/30%/50% for walk/bike/bus.

Two distributions are independent if their joint distribution equals the product of the marginals. Here's what independence looks like:

![Independent distributions](00-introduction/independence2.png)

Which of our three tables is "closest" to being independent?

<MultipleChoiceQuestion
  options={["Table 1", "Table 2", "Table 3"]}
  correctIndices={[1]}
  explanation={<><a href="01-kl_intro#information-theory">We'll see</a> this is measured by something called mutual information, which is super important in information theory and‚Äîsurprise!‚Äîit's just KL divergence in disguise.<br/><br/>Try it yourself with the interactive widget below:<br/><br/><MutualInformationWidget /></>}
/>
</Expand>

<Expand headline="ü§ì Understanding XKCD jokes"> <a id = "xkcd"></a>

<a href="https://xkcd.com/1159/" target="_blank" rel="noopener noreferrer">
  <img src="00-introduction/countdown.png" alt="Are the odds in our favor?" style={{cursor: 'pointer'}} />
</a>

So... are the odds in our favor?

<MultipleChoiceQuestion
  options={["Yes", "No", "depends"]}
  correctIndices={[1, 2]}
  explanation={<>
  The guy in the hat seems to be confident that at least one digit behind the picture is not zero. If those digits are uniformly random and independent, it's a safe assumption. However, we will develop the maximum entropy principle that will explain our gut feeling: Given all the zeros we can see, it's actually very likely that all other hidden digits are zeros too. 

  More about that later, we will test it out with the calculator below. <br/><br/><XKCDCountdownWidget /></>}
/> 
</Expand>


<Expand headline="ü§Ø Machine Learning mess"> <a id="machine-learning"></a>

When you first dive into machine learning, it looks like total chaos‚Äîjust a bunch of random tricks and optimization problems. Like, say you wanna understand how DALLE or Midjourney work. The standard setup for image generation is called a variational autoencoder. You train it by optimizing this absolute monster: 

<Math displayMode={true} math = "\frac{1}{N} \sum_{i = 1}^N \left( \sum_y p'(y | x) \frac{\| X_i - \textrm{Dec}(y)\|^2}{2d} \,+\, \left( \frac12 \sum_{j = 1}^d \textrm{Enc}_{\mu, j}(X_i)^2 + \textrm{Enc}_{\sigma^2, j}(X_i) - \log \textrm{Enc}_{\sigma^2, j}(X_i) \right)\right)"/>

Where the hell does this come from?? [We'll see](06-machine_learning) how KL divergence makes sense of this mess‚Äîand tons of other standard ML algorithms too. 

![Examples](06-machine_learning/table.png)

</Expand>





## üöÄ What's next? <a id="next-steps"></a>

As we go through the mini-course, we'll revisit each puzzle and understand what's going on. 
There are three parts to it. 

1. First, we will understand how Bayes' rule leads to the notions of KL divergence, cross-entropy and entropy (first three chapters). 
2. Next, we will see that minimizing KL divergence is a powerful principle to build good probabilistic models. We will use it to explain where the machine-learning loss function are coming from. (next three chapters)
3. Finally, we will discuss how entropy and KL divergence are related to coding theory, and what kind of intuition it gives us about large language models. 

See you in [the first chapter](01-kl_intro)!