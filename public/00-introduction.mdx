# All those riddles <a id="introduction"></a>

This web contains an introduction to Kullback-Leibler (KL) divergence. Specifics of what we cover are [at the bottom](what-this-course-covers) together with [what we assume you know](what-we-assume), but let's start with something more exciting. We want to tell you several riddles and hope that you get [nerd-sniped](https://xkcd.com/356/) by some of them!

## Polling <a id="polling"></a>

The next US elections are coming and you want to estimate which of the two parties is going to win the popular vote. Here's how: You will sample a few random US citizens and ask them who they vote for, hoping that the estimate you compute from this sample is a good approximation of reality.

If we assume a simplified model where every citizen votes for one party, they are truthful, and won't change their opinions in the future<Footnote>Of course, those assumptions are entirely unrealistic but bear with us.</Footnote>, it is possible to compute that asking 1000 people is enough to get an estimate that is probably within one percent of the right answer. [TODO check] This is astonishing! Regardless of how many people live in the country, 1000 random citizens are enough to get the answer within one percent.

But in reality, all US elections are incredibly close; we already know that both Democrats and Republicans are going to get around 50%. So we should perhaps get a bigger sample sufficient to estimate within $0.1\%$. The question is, roughly how many people should we sample to get this improved precision?

1. about 3000
2. about 10000
3. about 100000

The correct answer is about 100000. In fact, if we want to be close up to an error of $\epsilon$, we need about $1/\epsilon^2$ samples. This means that getting better estimates become very expensive very soon! This is one reason why most polling estimates don't bother asking more than about 1000 people - even in the most idealistic scenarios, you asking sufficient amount of people to bring your error bars below ~1% gets too costly.

The question is: why $1/\epsilon^2$? We'll explore this in later sections. KL divergence measures how well one distribution matches another, which is why understanding its value for distributions (50%, 50%) and (50%+$\varepsilon$, 50%-$\varepsilon$) is going to shed a lot of light on this question! We will also use this as a jumping bridge to explain a bit more on how KL divergence is crucial in statistics.

## Financial Mathematics <a id="financial-mathematics"></a>

![Financial Data Distribution](00-introduction/sap_graph.png)

This picture shows the price of S&P<Footnote>If you don't know what that is, think of the price of Bitcoin, Apple, or any other stock</Footnote>.

Every day, the price jumps a little bit. I downloaded the historical data, computed the daily jumps, and plotted them in a histogram plot -- the x-axis is the size of the jump (positive roughly 50%, otherwise negative) and the y-axis is the frequency of that jump. Before I show you the plot, I want you to guess how this is going to look like:

1. Laplace
2. normal
3. střecha

This was a trick question because both answers are kind of true! The shape of the histogram depends on something I did not tell you -- how far in the past did I look to collect the data. Try it for yourself both for S&P and Bitcoin:

[TODO widget]

![Financial Data Distribution](00-introduction/financial.png)

At the beginning, it seems that distribution looks a lot like the familiar bell-shaped curve $e^{-x^2}$ of [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution). This makes sense! The price of a stock slightly increases whenever somebody buys it, and slighlty decreases whenever somebody sells. The daily change is a summation of bazilions of buys and sells, and this is the setup in which we can apply the so-called [Central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem), that says that in this case, the resulting distribution should be normal.

So how come that as we keep zooming out, the histogram gets more and more "pointy"? The other distribution is the so-called Laplace distribution with shape $e^{-|x|}$ and it starts to be a better fit after around ?? months for S&P, and ?? for Bitcoin.

There are two ways for how this connects to KL divergence. First of all, KL divergence measures how well a ground truth distribution (empirical data we collected) is fitted by a model (normal or Laplace distribution), so you can use it to make claims such as "for more than ?? months, Laplace fits the data better than normal".

Second, we will use KL divergence to understand a deep principle in probability called maximum entropy principle. This will help us gain a lot of intuition for what both normal and Laplace distributions stand for.

## Statistics <a id="statistics"></a>

![Determining the length of the foot in 1522](00-introduction/rod.png)

In the ye olden days, people measured lengths in feet. Different people of course have differently-sized body parts, so at some point we have to be a bit more precise. One way is to measure the foot of the local warlord, but then you have to change it every now and then when the warlord is replaced.

Another way (as in the engraving above) is to ask 16 random people and compute their mean foot length as
$$\bar X = \frac{1}{16} (X_1 + \dots + X_{16})$$
This will be a fairly stable estimate that will hopefully remain similar the next time you perform this experiment.

How good is this estimate? Typically, one would gauge it by estimating the standard deviation. Do you remember the formula for that? Try to guess the correct one:

1. $$\bar \sigma^2 = \frac{1}{15} \sum_{i=1}^{16} (X_i - \bar X)^2$$
2. $$\bar \sigma^2 = \frac{1}{16} \sum_{i=1}^{16} (X_i - \bar X)^2$$
3. $$\bar \sigma^2 = \frac{1}{17} \sum_{i=1}^{16} (X_i - \bar X)^2$$

Another trick question since all formulas are correct! Or a bit more precisely, all of them are defensible in the framework of (frequentist) statistics.
The coefficient $1/(n-1)$ corresponds to the so-called unbiased estimate, $1/n$ gives the maximum likelihood estimate, and $1/(n+1)$ minimizes the mean squared error between the guess and the correct value.

But out of all these philosophies of how to estimate values, only one of them -- maximum likelihood estimation -- became the main workhorse of machine learning. Using KL, we will see why the maximum likelihood philosophy stands out.

## How to train your LLM <a id="deep-learning"></a>

[TODO image: "my name is A", two distributions]

So you want to train a good large language model (LLM) like GPT/Gemini/Claude. Such a model is given some text as input, does some computation inside, and outputs the next letter<Footnote>In fact, it outputs the next token. But a token is just a small group of letters -- think of it as a syllable or a short word. </Footnote> in the text.
LLMs are in fact designed so that they don't output a single letter, but try to predict the distribution $p$ of the next letter. This is why it's simple to make LLMs output different answers on the same text.

{/* We want to train a new model that is at least as good as, say, GPT-4. In practice, this would involve collecting a lot of text and running both GPT-4 and our LLM on them -- for each predicted letter, we would look at the distribution outputted by GPT-4 and try to nudge our model to make its distribution look similar. */}

There are many complicated things going on in training of LLMs, but one particularly important one is the choice of the so-called _loss function_. To understand this, let's say that for some text that appears all over the place on the internet, like "My name is A", we know the distribution of the next letter. Perhaps "l" is frequent because of Alex, but "a" not so much. Let's call this ground-truth distribution $p$.

We also know the distribution that our LLM outputs, let's call it $q$. We need to measure how well $q$ matches $p$. This way, we derive the loss function -- the function that we try to minimize during the LLM training. So which one would you choose?

1. $L(p,q) = \sum_{i = 1}^n |p_i - q_i|$ (_$\ell_1$ norm_)
2. $L(p,q) = \sum_{i = 1}^n (p_i - q_i)^2$ (_$\ell_2$ norm_)
3. $L(p,q) = \sum_{i = 1}^n p_i \cdot \log (p_i / q_i)$ (_KL divergence_)

There is nothing wrong with the first two functions, in practice, people usually opt for KL divergence. An important feature of it is that according to KL, if $p_i = 0.5$ and $q_i = 0.51$, it's smaller problem than if $p_i = 0.01$ and $q_i = 0.02$. Sounds reasonable!

Here's a widget with two distributions and all three loss functions. Try to make KL infinitely large!

[TODO widget]

## Predictions <a id="predictions"></a>

It would be great to know what the future holds—or at least to know someone who does. To this end, you have gathered a bunch of experts and asked them to assign probabilities to a list of events for the coming year. Each expert provides an estimate for each event.
When the year ends, you know for each event whether it occurred or not.

[TODO table]

The question is, which expert is the best one?

1. Expert 1
2. Expert 2
3. Expert 3

Answer: It depends! In practice, people use one of two scoring rules, the Brier score or log-score. The two scores penalize you differently if you claim that an event probably won't happen, but it does -- see e.g. the ?? column. Brier does not care much whether your guess was 1%, 0.1%, or even 0% but log score does. In fact, if you estimate that an event has 0% probability and then it happens, log-score says you are infinitely bad!

We will see how KL divergence explains why log-score is natural and what it stands for.

## Distance from independence <a id="information-theory"></a>

Independence is one of the most important probability concepts. Here's a recap: think of two distributions $p_1$ and $p_2$. Maybe $p_1$ is over tomorrow's weather -- $\{\text{good}, \text{bad}\}$, and $p_2$ is over how I am going to commute to work -- $\{\text{walk}, \text{bike}, \text{bus}\}$. A joint distribution is a table where we put a probability for each pair of options. For example, here are three possible joint distributions for three different people:

[todo]

All three above distribution have the same _marginals_; all correspond to 70% of good weather, and 20%/30%/50% for walk/bike/bus.

Two distributions are independent, if the joint distribution is a "product" of the two marginal distributions, as below:

[todo]

Clearly, none of our three examples correspond to independent distribution. But the question is, which table is the "closest" to being independent?

1. Table 1
2. Table 2
3. Table 3

[TODO posunout dál, irelevantní]
Of course, to answer this question, we first have to come up with a suitable function that measures distance from independence. In practice, this is often done by computing [correlation](https://en.wikipedia.org/wiki/Correlation), but that's not a super satisfactory solution. First, it works only for random variables<Footnote>I.e., distribution over numbers</Footnote> so it's unclear how to apply it to our case. Also, dependent random variables can have zero correlation, which is weird.

The hard part is to come up with a good definition of how far a joint distribution is from being independent. A good way of measuring that is a so-called mutual information. It intuitively measures how much we learn about the value of one distribution, if we learn the value of the other one. In our case, it tells us that the ?? example is closest to being independent.

We will see how we can come up with the definition of mutual information using KL, and we will understand what the definition stands for.

## Machine Learning <a id="machine-learning"></a>

Have you noticed that certain functions appear frequently in machine learning?

For example, consider $x^2$: The most basic clustering algorithm, $k$-means, minimizes the sum of squared distances, and the same holds for linear regression. Why use $x^2$? What would it mean to use $|x|$ or $x^4$ instead?

Or consider the logarithmic function. Standard machine learning tools like logistic regression involve logarithms, and neural nets are often optimized using the so-called cross-entropy loss—which takes logarithms of probabilities.

Some formulas in machine learning are true beasts. For example, the loss function for logistic regression or the complex expressions behind variational autoencoders (the architecture behind early image generation models like DALL-E). Where do these seemingly complex expressions come from? What's their meaning?

## Modeling <a id="modeling"></a>

A customer comes to a restaurant. Each meal on the menu has a specific tastiness value, which we conveniently assume is known. If the customer spends enough time considering all the meals, she will pick the one with the highest tastiness. But if she is extremely hungry, she might simply choose a random meal. The question is: How should we model the situation in between—when the customer spends some time looking at the menu but not enough to always pick the tastiest item?

More mathematically speaking, given $n$ numbers $a_1, \dots, a_n$, can we find a parameter $\lambda$ and a natural family of probability distributions such that $\lambda=0$ corresponds to the uniform distribution and $\lambda=+\infty$ corresponds to picking the maximum value (i.e., $\argmax_{i} a_i$)?

You sometimes encounter this type of question when constructing a probabilistic model. There are many degrees of freedom in constructing a probability distribution, so how do we pick the "most natural" one that interpolates between complete order and total randomness?

This quandary is typically resolved by what's known as the maximum entropy principle. In this case, the principle dictates that we should choose the so-called softmax distribution. How does this principle work, and where does it come from?

### Random Pi Program <a id="random-pi-program"></a>

Here's a riddle: Consider the set of all C programs of length 1000 characters (i.e., the file has 1000 bytes if saved in ASCII) that print the first million digits of $\pi$. Let's sample one such program uniformly at random. How would it probably look?

## Preview: The Power of KL Divergence <a id="preview-the-power-of-kl-divergence"></a>

Each of the puzzles above might seem disconnected, but they share a common thread: they can all be approached, understood, or solved using KL divergence and related concepts from information theory.

KL divergence is a fundamental measure that quantifies how one probability distribution differs from another. Its formal definition is:

$$KL(p, q) = \sum_{i = 1}^n p_i \log \frac{p_i}{q_i}$$

Where $p$ represents the "true" distribution and $q$ represents our model or approximation.

In the upcoming sections, we'll explore this measure in depth, develop intuitions for why it works, and see how it connects to fundamental principles of reasoning under uncertainty. We'll also examine how this single measure provides a unifying perspective on diverse problems across statistics, machine learning, information theory, and more.

As we progress through the course, we'll revisit these puzzles and show how they can be solved or better understood through the lens of information theory and KL divergence.

## Next Steps <a id="next-steps"></a>

In the next part, we'll begin our exploration of information theory by learning how to quantify information. We'll define entropy, cross-entropy, and KL divergence, and develop intuitions for what these measures tell us about probability distributions.

Ready to begin? Let's start with [Quantifying Information](/01-quantifying).

## What This Course Covers <a id="what-this-course-covers"></a>

This course takes a problem-driven approach. Rather than starting with abstract mathematical concepts, we begin with real-world puzzles and challenges, introducing the theoretical tools only as they become necessary for solving these problems.

Throughout the course, you'll see how concepts like entropy, KL divergence, cross-entropy, mutual information, and the maximum entropy principle connect seemingly disparate fields like:

- Election polling and statistical sampling
- Financial modeling and market predictions
- Machine learning algorithms and loss functions
- Deep learning architectures
- Information theory and coding
- Algorithmic complexity and prediction

By the end of this course, you'll understand not just how to apply these tools, but why they work and how they connect to fundamental principles of reasoning under uncertainty.

## What we assume <a id="what-we-assume"></a>
