# 📚 About This Mini-Course <a id="what-this-course-covers"></a>

This mini-course starts with [a few puzzles and challenges](00-riddles), all of them at the intersection of probability, information theory, and machine learning. They motivate the next five chapters that build up the relevant theory to solve them. 

Here are some keywords we will explore: KL divergence, entropy, cross-entropy, max likelihood & max entropy principles, loss functions, coding theory, compression, and more. 

## 📖 How to read this

Skip stuff you find boring, especially expanding boxes & footnotes. Skip boxes labeled with ⚠️, unless you are really into the topic. Follow links, get nerdsniped, and don't feel the need to read this linearly. 

This mini-course does not contain many formal theorem statements or proofs since the aim is to convey intuition in an accessible way. The downside is that some discussions are necessarily a bit imprecise. If you miss this, copy-paste the relevant chapter to your favorite LLM and ask it. 

The total length of the main text is about 2 chapters of Harry Potter and the Philosopher's Stone (from the beginning to Harry's first experience with magic). About 5 chapters if you read all footnotes, expand boxes, and bonus content (Harry learns he's a wizard). In either case, I can't promise you will feel the same as Harry.  

{/*
If you want to gain experience and level up your probability stats, you can't take Harry-Potter-reading approach, though. 
See [resources](resources) for some links. 
*/}

## 🧠 What is assumed <a id="what-we-assume"></a>

I assume probability knowledge after taking a typical introductory course at university.  
You should be familiar with the basic language of probability theory: probabilities, distributions, random variables, independence, expectations, variance, and standard deviation. 

[Bayes' rule](https://www.lesswrong.com/w/bayes-rule) is going to be especially important.<Footnote>There are always more levels of understanding. I am a professional mathematician / computer scientist and still didn't appreciate Bayes' rule enough until reading [Yudkowsky's](https://www.lesswrong.com/w/bayes-rule) [explanations](https://www.lesswrong.com/w/test-2). </Footnote>
I also assume that you get the gist of [the law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers) <Footnote>If you keep flipping a coin with bias $p$ and $X_i$ is the $i$-th outcome, the so-called sample mean $\hat{p} = 1/n \cdot \sum_{i = 1}^n X_i$ is likely to be very close to the true bias $p$. </Footnote> and maybe even the [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem). <Footnote>If you keep flipping a coin and $X_i$ is the $i$-th outcome, the distribution of the sample mean is going to look like the normal distribution, with standard deviation about $1/\sqrt{n}$ for $n$ flips. </Footnote> While Little Prince is cool, it helps if you associate the [slender, bell-shape curve of normal distribution](https://en.wikipedia.org/wiki/Bell-shaped_function#/media/File:Normal_Distribution_PDF.svg) with $e^{-x^2}$ rather than a [devoured elephant](https://www.amazon.com/KIUB-Postcard-Little-elephant-10x15cm/dp/B0CKSVVWDL). 

Knowing example uses of statistics and machine learning helps a good bit to appreciate the context.


## 🎯 About This Project

I like to think about _open exposition problems_<Footnote>The phrase comes from one of my [favorite exposition papers](https://timothychow.net/forcing.pdf). </Footnote> - what are topics in computer science that are already understood by the scientific community, but we still did not work out the best ways to teach them? 

I see two pretty interesting problems here: 

-- How to adapt our teaching of computer science theory to convey more about neural networks? 
-- How can we use the current capabilities of LLM to teach better, in general? 

This text takes a stab at both problems. First, I try to collect various ideas at the intersection of probability, statistics, and information theory that we are typically not teaching in standard undergrad courses, and repackage them. The result hopefully gives some useful intuitions behind current ML in particular, and behind using probability to understand the world around us in general. 

Second, with current LLM capabilities, some ways of presenting math got doable. The current text is full of widgets that will hopefully help you learn by playing with stuff, instead of proving theorems. Let's see how it works! 


## 👥 Authors

<div style={{display: 'flex', gap: '3rem', justifyContent: 'center', alignItems: 'flex-start', flexWrap: 'wrap', marginTop: '2rem'}}>
  <div style={{textAlign: 'center', maxWidth: '250px'}}>
    <img src="fig/claude.png" alt="claude" style={{width: '200px', height: '200px', objectFit: 'cover', borderRadius: '8px', marginBottom: '1rem'}} />
    <p><a href="https://claude.ai/">Claude</a> is a model from Anthropic.</p>
  </div>

  <div style={{textAlign: 'center', maxWidth: '250px'}}>
    <img src="fig/gavento.jpg" alt="Tom Gavenčiak" style={{width: '200px', height: '200px', objectFit: 'cover', borderRadius: '8px', marginBottom: '1rem'}} />
    <p><a href="https://gavento.cz/">Tom Gavenčiak</a> is a researcher in AI alignment and applied rationality at Charles University in Prague.</p>
  </div>

  <div style={{textAlign: 'center', maxWidth: '250px'}}>
    <img src="fig/vasek.jpg" alt="Vašek Rozhoň" style={{width: '200px', height: '200px', objectFit: 'cover', borderRadius: '8px', marginBottom: '1rem'}} />
    <p><a href="https://vaclavrozhon.github.io/">Vašek Rozhoň</a> is a computer science researcher and an assistant professor at Charles University in Prague. With friends, he runs the [polylog channel](https://www.youtube.com/@PolylogCS). </p>
  </div>
</div>

## ✉️ Feedback

We would be grateful if you leave feedback [here](https://forms.gle/YourFormID) [todo], write a comment, or reach out to us directly. 

All three of Claude, Gemini, and GPT have been used massively to create this mini-course. 
_Thanks to Petr Chmel, Vojta Rozhoň, Robert Šámal, Pepa Tkadlec, and others for feedback. _

