# 📚 What This Mini-Course Covers <a id="what-this-course-covers"></a>

This mini-course on KL divergence starts with [a few puzzles and challenges](00-introduction). This motivates the next five chapters that build up the relevant theory that solves the puzzles. 

Here are some concepts we will explore: KL divergence, entropy, cross-entropy, maximum likelihood principle, maximum entropy principles, machine-learning loss functions, and more.

The applications of KL are in probability, statistics, machine learning, information and coding theory, financial math, etc. etc. This mini-course is building up towards applications to machine learning, but on our journey, we will see a little bit of everything.

# 📖 How to read this

Skip stuff you find boring, especially expanding boxes and footnotes. Follow links and don't feel the need to read this linearly. 

This mini-course does not contain formal theorem statements or proofs since the aim is to convey intuition in an accessible way. However, if you want to gain experience and level up your probability stats, you have to grind through exercises and formality at some point. See [resources](resources) for some links and ask your favorite LLM / Google whenever something's unclear or you miss an argument.  

The total length of the main text is about 1.5 chapters of Harry Potter and the Philosopher's Stone (from the beginning to Dudley's tantrum during his birthday). About 4-5 chapters if you read all footnotes and expanding boxes (up to when Harry first meets Hagrid and learns he's a wizard). 

# 🧠 What we assume <a id="what-we-assume"></a>

You should be familiar with the basic language of probability theory: probabilities, distributions, random variables, independence, expectations, variance, and standard deviation. You should definitely understand the [Bayes' rule](https://www.lesswrong.com/w/bayes-rule). 

We also assume that you get the gist of [the law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers) <Footnote>If you keep flipping a coin and $X_i$ is the $i$-th outcome, the so-called sample mean $1/n \cdot \sum_{i = 1}^n X_i$ is likely to be very close to the true bias of the coin. </Footnote> and even the [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem). <Footnote>If you keep flipping a coin and $X_i$ is the $i$-th outcome, the distribution of the sample mean is going to look like the normal distribution, with standard deviation about $1/\sqrt{n}$ for $n$ flips. </Footnote> While Little Prince is cool, it helps if you associate the [slender, bell-shape curve of normal distribution](https://en.wikipedia.org/wiki/Bell-shaped_function#/media/File:Normal_Distribution_PDF.svg) with $e^{-x^2}$ rather than a devoured elephant. 

Knowing example uses of statistics and machine learning helps a good bit to appreciate the context.


# 🎯 About This Project

We (Tom and Vašek) are both academics, but also fans of LessWrong-style rationality. What pieces of probability theory are the most important to understand the world around us? Which rationalist insights can be conveyed during lectures? 

These are all amazing [open exposition problems](https://mathoverflow.net/questions/281852/important-open-exposition-problems)! We took a stab at it on a probability seminar at Charles University at spring 2025; this text contains a part of what we discussed there. 

Our hope is that by the end of this mini-course, you will not only remember the formula for  KL divergence, but also get a better language and intuitions for all kinds of problems that involve reasoning under uncertainty and modelling the world around us. 

# 👥 Authors

<div style={{display: 'flex', gap: '3rem', justifyContent: 'center', alignItems: 'flex-start', flexWrap: 'wrap', marginTop: '2rem'}}>
  <div style={{textAlign: 'center', maxWidth: '250px'}}>
    <img src="fig/gavento.jpg" alt="Tom Gavenčiak" style={{width: '200px', height: '200px', objectFit: 'cover', borderRadius: '8px', marginBottom: '1rem'}} />
    <p><a href="https://gavento.cz/">Tom Gavenčiak</a> is a researcher in AI alignment and applied rationality at Charles University in Prague.</p>
  </div>
  
  <div style={{textAlign: 'center', maxWidth: '250px'}}>
    <img src="fig/vasek.jpg" alt="Vašek Rozhoň" style={{width: '200px', height: '200px', objectFit: 'cover', borderRadius: '8px', marginBottom: '1rem'}} />
    <p><a href="https://vaclavrozhon.github.io/">Vašek Rozhoň</a> is a computer science researcher and an assistant professor at Charles University in Prague.</p>
  </div>
</div>

# 💬 Feedback
We would be greateful if you leave it [here](). 