# What This Mini-Course Covers <a id="what-this-course-covers"></a>

This mini-course on KL divergence starts with [a few puzzles and challenges](00-introduction). This motivates the next five chapters that build up the relevant theory that solves the puzzles. 

Here are some concepts we will explore: KL divergence, entropy, cross-entropy, maximum likelihood principle, maximum entropy principles, machine-learning loss functions, and more.

The applications of KL are in probability, statistics, machine learning, information and coding theory, financial math, etc. etc. As our guiding light, we took the applications of KL to machine learning -- this is what we are building up towards during the five chapters. But on our journey, we will see a little bit of everything.

# About This Project

We (Tom and Vašek) are both academics, but also fans of LessWrong-style rationality. What pieces of probability theory are the most important to understand the world around us? Which rationalist insights can be conveyed during lectures? 

These are all amazing [open exposition problems](https://mathoverflow.net/questions/281852/important-open-exposition-problems)! We took a stab at it on a probability seminar at Charles University at spring 2025; this text contains a part of what we discussed there. 

Our hope is that by the end of this mini-course, you will not only remember the formula for  KL divergence, but also get a better language and intuitions for all kinds of problems that involve reasoning under uncertainty and modelling the world around us. 

# About The Authors

[Tom Gavenčiak](https://gavento.cz/) is a researcher in AI alignment and applied rationality at Charles University in Prague.

[Vašek Rozhoň](https://vaclavrozhon.github.io/) is a computer science researcher and an assistant professor at Charles Universtiy in Prague.
