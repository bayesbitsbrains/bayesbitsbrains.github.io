# Fisher Information

Fisher information is one of the most fundamental concepts in statistics, providing a measure of how much information a random sample carries about an unknown parameter.

## Definition

For a probability distribution $p(x|\theta)$ parameterized by $\theta$, the **Fisher information** is defined as:

<Math displayMode={true} math="I(\theta) = E_{x \sim p(x|\theta)} \left[ \left( \frac{\partial}{\partial \theta} \log p(x|\theta) \right)^2 \right]" />

This can be equivalently written as:

<Math displayMode={true} math="I(\theta) = -E_{x \sim p(x|\theta)} \left[ \frac{\partial^2}{\partial \theta^2} \log p(x|\theta) \right]" />

## Connection to KL Divergence

One of the most beautiful results in information theory is that Fisher information emerges naturally as the second-order term in the Taylor expansion of KL divergence.

Consider two distributions with parameters $\theta$ and $\theta + \epsilon$. For small $\epsilon$:

<Math displayMode={true} math="D_{KL}(p(x|\theta) || p(x|\theta + \epsilon)) = \frac{\epsilon^2}{2} I(\theta) + O(\epsilon^3)" />

This shows that Fisher information measures the "curvature" of the KL divergence around a parameter value.

## The Cramér-Rao Bound

Fisher information provides a fundamental limit on how well we can estimate parameters. The **Cramér-Rao bound** states that for any unbiased estimator $\hat{\theta}$ of parameter $\theta$:

<Math displayMode={true} math="\text{Var}(\hat{\theta}) \geq \frac{1}{n \cdot I(\theta)}" />

where $n$ is the sample size. This tells us:
- Higher Fisher information → lower variance bound → better estimation possible
- The bound scales with $1/n$, explaining why more data helps

## Examples

### Gaussian Distribution
For a Gaussian with unknown mean $\mu$ and known variance $\sigma^2$:
<Math displayMode={true} math="I(\mu) = \frac{1}{\sigma^2}" />

This makes intuitive sense: smaller variance means more information about the mean.

### Bernoulli Distribution
For a Bernoulli distribution with parameter $p$:
<Math displayMode={true} math="I(p) = \frac{1}{p(1-p)}" />

Information is maximized when $p = 0.5$ (maximum uncertainty).

## Information Geometry

Fisher information defines a natural metric on the space of probability distributions, called the **Fisher-Rao metric**:

<Math displayMode={true} math="g_{ij}(\theta) = E_{x \sim p(x|\theta)} \left[ \frac{\partial \log p(x|\theta)}{\partial \theta_i} \frac{\partial \log p(x|\theta)}{\partial \theta_j} \right]" />

This turns the parameter space into a Riemannian manifold, where:
- Geodesics represent optimal paths between distributions
- The metric captures the "difficulty" of distinguishing nearby distributions

## Applications

Fisher information appears throughout statistics and machine learning:

1. **Efficient Estimation**: Maximum likelihood estimators achieve the Cramér-Rao bound asymptotically
2. **Experimental Design**: Choosing experiments to maximize Fisher information
3. **Natural Gradient Descent**: Using the Fisher metric for optimization
4. **Information Bottleneck**: Trading off compression vs. preservation of relevant information

## Connection to Entropy

While entropy measures uncertainty in a single distribution, Fisher information measures how that uncertainty changes with parameters. In fact:

<Math displayMode={true} math="\frac{\partial H(p_\theta)}{\partial \theta} = -E_{x \sim p_\theta} \left[ \frac{\partial \log p_\theta(x)}{\partial \theta} \right]" />

The derivative of entropy involves the score function, whose variance is the Fisher information!

## Takeaways

Fisher information bridges several fundamental concepts:
- It's the local curvature of KL divergence
- It bounds the accuracy of parameter estimation
- It defines a natural geometry on probability spaces
- It connects to how entropy changes with parameters

Understanding Fisher information provides deep insight into the limits of statistical inference and the geometry of information.