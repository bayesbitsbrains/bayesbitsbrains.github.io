# Resources <a id="resources"></a>

This page compiles high-quality resources for further exploring KL divergence, information theory, and their applications. Resources are organized by difficulty level from introductory to advanced.

## Introductory Resources <a id="introductory-resources"></a>

These resources focus on building intuition and providing accessible explanations of the core concepts.

### Kullback-Leibler Divergence Explained <a id="kl-divergence-explained"></a>

[**"Kullback-Leibler Divergence Explained" – _Count Bayesie blog_ (Will Kurt, 2017)**](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)

This blog post introduces KL divergence in an accessible way, framing it as a measure of how much **information is lost** when using one distribution to approximate another. It starts with a fun "space worms" example, comparing empirical data to simple models, and defines KL divergence with the basic formula.

The author explains entropy as the "information" in a distribution and shows KL divergence as the extra surprise or inefficiency when using the wrong model. He also demonstrates KL divergence calculations for two model distributions and emphasizes that KL is **not symmetric** (so not a true distance).

The post provides an intuitive feel for KL divergence and even shows that choosing model parameters by **maximum likelihood corresponds to minimizing KL divergence** (the blog's example finds the best-fit Binomial model by minimizing KL).

### Intuitive Explanation of the Kullback-Leibler Divergence <a id="intuitive-explanation-kl"></a>

[**"Intuitive Explanation of the Kullback-Leibler Divergence" – _Johannes Schusterbauer's blog_ (2021)**](https://johfischer.com/2021/12/31/intuitive-explanation-of-the-kullback-leibler-divergence/)

This article builds KL divergence from the ground up using basic information theory concepts. It introduces **self-information** (surprisal) and **entropy** before KL, using a biased coin toss example to illustrate why unlikely events carry more information.

KL divergence is then presented as the extra number of bits needed to encode samples from a true distribution $P$ when using a code optimized for an alternative distribution $Q$. In simple terms, if you assume the wrong distribution, your descriptions of data will be longer – KL divergence quantifies that penalty, and it's zero only if $P=Q$ (no extra bits needed).

The blog provides a step-by-step derivation with a coin-toss sequence example, helping the reader develop an **intuitive grasp** of the KL formula by comparing the likelihood of data under two distributions. This resource is very accessible, focusing on **intuition and examples** rather than heavy math.

### Six (and a Half) Intuitions for KL Divergence <a id="six-intuitions-kl"></a>

[**"Six (and a Half) Intuitions for KL Divergence" – _LessWrong post_ (Callum McDougall, 2022)**](https://uli.rocks/p/kl-divergence/)

This essay offers multiple complementary intuitions for KL divergence, making it a great bridge between basic understanding and deeper insight. For example, one intuition is that **KL divergence measures the "expected additional surprise"** when using an incorrect model ("map") for the true distribution ("territory").

It relates KL to the difference in code lengths (cross-entropy minus entropy) – essentially the extra unpredictability introduced by a wrong assumption. The post also discusses how KL behaves like a "metric" on probability distributions (even though it's not symmetric): it can be seen as a kind of squared distance in the limit of small differences (relating to Fisher information geometry).

Moreover, it explains the practical difference between $D_{KL}(P\|Q)$ and $D_{KL}(Q\|P)$, echoing the ideas of "inclusive" vs "exclusive" KL in an intuitive way. Overall, this resource compiles **several viewpoints** – from seeing KL as information gain to a tool for updating beliefs – in a reader-friendly manner.

## Intermediate Resources <a id="intermediate-resources"></a>

These resources connect KL divergence to important principles in machine learning and statistics.

### Machine Learning: A Probabilistic Perspective <a id="machine-learning-probabilistic"></a>

[**"Machine Learning: A Probabilistic Perspective" – _Kevin P. Murphy_ (2012, Textbook)**](https://probml.github.io/pml-book/book0.html)

Chapter 2 of this popular ML textbook introduces KL divergence in the context of probability and information theory. Murphy defines KL divergence and explains its core properties (non-negativity, asymmetry) as a way to measure dissimilarity between distributions.

Importantly, the book connects KL divergence to machine learning techniques: it notes that **maximizing likelihood is equivalent to minimizing KL divergence** between the empirical data distribution and the model distribution. In fact, as Oxford lecture notes (inspired by Murphy's content) show, the **MLE solution can be viewed as the model that minimizes $D_{KL}(p_{\text{true}} \;\|\; p_{\theta})$** in the limit of infinite data.

Murphy's text also covers how KL divergence relates to **cross-entropy loss** used in classification and how it underpins measures like **mutual information** (which is a KL between joint and factorized distributions). The presentation is mathematical but geared toward readers with basic probability background, making it a solid resource to see how KL divergence bridges statistical inference and information theory in ML.

### Information Theory, Inference, and Learning Algorithms <a id="information-theory-inference"></a>

[**"Information Theory, Inference, and Learning Algorithms" – _David J. C. MacKay_ (2003, Textbook)**](https://www.inference.org.uk/itprnn/book.html)

MacKay's renowned book offers a comprehensive treatment of entropy and KL divergence and weaves them into the fabric of Bayesian inference and machine learning. It provides a rigorous definition of KL divergence (also called _relative entropy_) and discusses its fundamental properties (e.g., non-negativity from Gibbs' inequality).

What sets this book apart is its insight into _why_ KL divergence matters: for instance, MacKay shows how **Bayesian updating can be interpreted in information–theoretic terms**, and how designing coding schemes or learning models both involve minimizing KL divergence.

The book covers the **maximum entropy principle** in depth – showing that maximizing entropy subject to constraints leads to exponential-family distributions – and relates this to minimizing KL to a prior. It also introduces the idea of **Occam's razor and model complexity** in Bayesian model comparison using KL (through the concept of _Occam factors_ and how much the posterior compresses the prior, i.e., a KL term).

With numerous examples and exercises, this textbook helps readers develop both a theoretical and practical understanding of KL divergence.

### Probability Theory: The Logic of Science <a id="probability-theory-science"></a>

[**"Probability Theory: The Logic of Science" – _E. T. Jaynes_ (2003, Book)**](https://bayes.wustl.edu/etj/prob/book.pdf)

This classic text by Jaynes lays the philosophical and mathematical groundwork for the **Principle of Maximum Entropy** and connects it to Bayesian probability theory. Jaynes uses relative entropy (essentially KL divergence) as a tool for deriving probability distributions from incomplete information – arguing that the distribution which **maximizes entropy** (least informative beyond given constraints) is the objectively correct choice.

In the book, KL divergence appears as the cross-entropy one minimizes when updating from a prior to a posterior with new data (this is effectively an information-theoretic derivation of Bayes' rule). The material is theoretical: Jaynes shows, for example, that **Bayesian updating yields the distribution that maximizes entropy relative to the prior**, meaning the posterior is the least biased distribution consistent with the new evidence.

This unification of MaxEnt and Bayes was ahead of its time and provides deep insight into why KL/entropy is central in inference. While the book is dense, it's a **highly regarded reference** for understanding KL divergence in the broader context of statistical physics, information theory, and Bayesian inference.

## Advanced Resources <a id="advanced-resources"></a>

These resources dive deeper into the theory and advanced applications of KL divergence.

### Elements of Information Theory <a id="elements-information-theory"></a>

[**"Elements of Information Theory" – _Thomas M. Cover & Joy A. Thomas_ (2nd ed. 2006, Textbook)**](https://www.wiley.com/en-us/Elements+of+Information+Theory%2C+2nd+Edition-p-9780471241959)

This authoritative textbook covers KL divergence in detail as part of the foundation of information theory. It formally defines **relative entropy $D(p\|q)$** and proves key facts like non-negativity and convexity (via Jensen's inequality).

The book places KL divergence at the heart of many topics: it shows that **mutual information** is a KL divergence (between the joint distribution and the product of marginals), and that **data processing can only reduce KL divergence** (an aspect of the Data Processing Inequality). It also uses KL in coding theorems – for instance, analyzing how far an arbitrary code or distribution is from the optimal by a KL term.

Additionally, Cover & Thomas highlight that the principle of maximum entropy and minimum KL are two sides of the same coin for deriving distributions. This text is mathematically rigorous and is often used in graduate courses, making it ideal for those who want a deep theoretical understanding and aren't afraid of formal proofs.

### Why KL? <a id="why-kl"></a>

[**"Why KL?" – _Alex Alemi's blog_ (2020)**](https://blog.alexalemi.com/kl.html)

In this blog post, a research scientist takes an **axiomatic approach** to KL divergence, asking why it's so special among divergence measures. The post derives KL divergence as the _unique_ quantity (up to scale) that satisfies a set of desirable properties for an "information gain" measure.

It imagines updating a prior distribution $q$ to a posterior $p$ and asks: how can we quantify "how much we learned"? From a few natural requirements (continuity, symmetry under reparameterization, non-negativity with 0 iff $p=q$, additivity for independent updates), Alemi shows that the answer must be the KL divergence.

The blog explains that KL divergence goes by many names – _relative entropy, information gain, expected weight of evidence_ – hinting at its fundamental role in information theory. It also clarifies units (bits, nats) and interpretation: for instance, KL can be seen as the number of bits of surprise added by switching beliefs from $q$ to $p$.

By referencing an information theory theorem (Hobson's theorem) and walking through intuitive examples (like narrowing down suspects in a puzzle), this post gives a **deep theoretical insight** into KL divergence while remaining conversational in tone.

### Maximum Entropy and Bayes <a id="maximum-entropy-bayes"></a>

[**"Maximum Entropy and Bayes" – _Rising Entropy blog_ (2020)**](https://risingentropy.com/maximum-entropy-and-bayes/)

This article explores the profound connection between the Maximum Entropy principle and Bayesian inference using KL divergence. Citing research by Giffin & Caticha, it points out that the **method of Maximum _Relative_ Entropy (ME)** generalizes both vanilla MaxEnt and Bayes' rule.

The blog demonstrates that when you receive new evidence, updating your prior to the posterior by **maximizing relative entropy $S_{\rm rel}(P, P_{\text{old}}) = -\int P \log(P/P_{\text{old}})\,dx$** (i.e., minimizing KL divergence to the prior) yields exactly the Bayesian posterior.

In other words, Bayes' theorem can be derived as the distribution change that **minimizes KL divergence from the prior while fitting the data**. The post provides an intuitive interpretation: relative entropy $D(P\|P_{\text{old}})$ is the _additional information required_ to go from the old distribution to the new.

It underscores that both Bayes and MaxEnt are driven by the same information-theoretic principle of keeping your distribution as "broad" (high entropy) as possible until forced to sharpen it by new constraints. This resource is valuable for readers interested in the theoretical unity of inference and information theory.

### KL-Divergence as an Objective Function <a id="kl-objective-function"></a>

[**"KL-Divergence as an Objective Function" – _Tim Vieira's ML blog_ (2014)**](https://timvieira.github.io/blog/post/2014/10/06/kl-divergence-as-an-objective-function/)

This blog post is aimed at practitioners who want to understand how KL divergence is used in **model fitting and variational inference**. It provides a **"cheat sheet" for KL's two directions**, often denoted $D_{KL}(P\|Q)$ versus $D_{KL}(Q\|P)$.

Vieira explains that minimizing $D_{KL}(P\|Q)$ (also called the **inclusive KL**) tries to make $Q$ cover all high-probability regions of $P$ – it is "mean-seeking" and will use a broad $Q$ to avoid missing any mass that $P$ has.

In contrast, minimizing $D_{KL}(Q\|P)$ (the **exclusive KL**) is "mode-seeking" – it will focus $Q$ on one mode of $P$ if that yields lower divergence. He gives a memorable mnemonic: _"When the truth comes first, you get the whole truth"_, meaning $KL(P\|Q)$ forces the approximation to account for the true distribution's support.

The post then delves into the calculus: deriving gradients for each KL direction and discussing computational issues. For instance, inclusive KL requires expectation under $P$ (often intractable if $P$ is the true distribution), whereas exclusive KL requires expectation under $Q$ (easier if $Q$ is your model).

This has practical consequences: **variational inference** typically minimizes $D_{KL}(Q\|P)$ (e.g., in VAEs, where $Q$ is the approximate posterior) because it's easier to sample from $Q$, while **maximum likelihood** can be seen as minimizing $D_{KL}(P\|Q)$ in the limit of infinite data.

## Applications in Modern Machine Learning <a id="applications-ml"></a>

These resources focus on practical applications of KL divergence in modern machine learning models.

### Variational Inference & Deep Learning <a id="variational-inference"></a>

[**"Variational Autoencoders" – _John Aslanides's weblog_ (2019)**](https://aslanides.io/blog/vae)

KL divergence is a key ingredient in modern deep learning algorithms, especially those involving latent-variable models. For example, **Variational Autoencoders (VAEs)** use KL divergence in their loss function.

This tutorial on VAEs clearly derives the Evidence Lower Bound (ELBO) objective, which includes a **KL term $D_{KL}(q(z|x)\,\|\,p(z))$** measuring how the learned encoder ($q$) diverges from the prior over latent variables. This term serves as a regularizer, encouraging the model's latent distribution to stay close to a simple prior while the reconstruction term encourages fidelity to the data.

The inclusion of KL divergence in ELBO connects **variational inference** to deep learning: effectively, training a VAE is an application of **Bayesian inference via KL minimization**. The tutorial provides both theoretical understanding and practical examples with code and visualizations.

### Reinforcement Learning Applications <a id="reinforcement-learning"></a>

[**"Trust Region Policy Optimization" – _OpenAI Spinning Up documentation_**](https://spinningup.openai.com/en/latest/algorithms/trpo.html)

In reinforcement learning, KL divergence is used to ensure stable updates and to incorporate prior knowledge. A prominent example is **Trust Region Policy Optimization (TRPO)**, an algorithm that improves a policy gradually by constraining the KL divergence between the new policy and the old policy at each step.

The intuition is that policies are probability distributions (over actions), and TRPO limits how "far" the new policy can deviate in distribution space, measured by KL – preventing large, destabilizing changes.

This use of KL as a regularizer or constraint is also seen in **policy gradient methods with entropy/KL bonuses** (to encourage exploration) and in **reinforcement learning from human feedback**, where a KL penalty keeps the learned policy close to an original model to avoid drifts.

In effect, KL divergence in RL acts as an **information-theoretic trust measure**, connecting to the idea of keeping updates **small in information space**.

## Next Steps <a id="next-steps"></a>

These resources provide a wealth of information for deepening your understanding of KL divergence and its applications. We recommend starting with the introductory resources if you're new to the topic, or diving into the advanced resources if you're ready to explore the theoretical foundations more deeply.
