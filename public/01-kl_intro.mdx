# KL divergence & Bayes rule <a id="quantifying-information"></a>

In this chapter, we will explain what KL divergence is and how it all goes back to Bayes theorem. Also, we'll discuss [one of our riddles](mutual-information).

## Definition of KL Divergence <a id="definition-of-kl-divergence"></a>

Here's what you need to know about KL divergence. <Footnote>KL stands for Kullback and Leibler, two clever guys who defined it in 1950's, very soon after Claude Shannon's [super important paper](https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication) in which he defined a closely related notion of entropy.</Footnote>

<div>KL divergence measures how well a distribution $p$ is fitted by a model distribution $q$.</div>

This means that the function is something like a distance function measuring how two distributions are close to each other.

The formula for discrete distributions $p_1, \dots, p_n$ and $q_1, \dots, q_n$ is:<Footnote> People typically use $D_{KL}(p||q)$ instead of $D(p,q)$. The two vertical lines are there to emphasize how $D_{KL}(p||q) \not= D_{KL}(q||p)$. We will use the simplified, less cluttered, notation since we are going to talk about this function a lot. 🙂 </Footnote>

$$
D(p,q) = \sum_{i = 1}^n p_i \log \frac{p_i}{q_i}
$$

The meaning behind the formula is that if you keep sampling from the true distribution $p$, KL divergence tells you how quickly you are able to tell that you indeed sample from $p$, and not from some other distribution $q$. If it is small, this means that $q$ is a good, hard-to-distinguish model of $p$.

I will try to explain this more precisely in this chapter.

## Bayes' theorem

Before we dive into our KL formula, a short recap of Bayes' theorem. This is a theorem about how to change your probabilities in face of evidence. Here's an example:

You have a coin which may or may not be biased. For simplicity, let's say there are only two options: either the coins is fair -- heads/tails with probabilities 50\%, 50\% -- or it gives heads with 40\% and otherwise tails with 60\% probability.

To apply Bayes' theorem, you need a _prior_ on how likely each possibility is. Let's say we choose 50\% fair, and 50\% biased.

To gain evidence, you flip a coin; let's say you got Heads. This is an evidence towards the coin being fair, as the other case has smaller heads-probability. The question is, how to compute the new, _posterior_ probability that your coin is fair?

Bayes' theorem can do that. The formula for it is typically written down as follows:

<Math displayMode={true} math={'P(fair | flipped heads) = \\frac{P(flipped heads | fair) \\cdot P(fair)}{P(flipped heads)}'} />

This is not rocket science -- if you multiply both sides by $P(flipped heads)$, both sides are just a different way of writing down the definition of $P(fair \cap flipped heads)$.

But I like to think of it a bit differently and in my head, I write it as this:

<Math displayMode={true} math={'\\frac{P(fair | flipped heads)}{P(biased | flipped heads)} = \\frac{P(fair)}{P(biased)} \\cdot \\frac{P(flipped heads | fair)}{P(flipped heads | biased)}'} />

You can check that the two formulas are the same by yourself. <Footnote>To see this, use the first formula both for $P(fair | flipped heads)$ and $P(biased | flipped heads)$, and divided them. </Footnote> To understand this rephrasing, it's good to think in terms of [odds](https://en.wikipedia.org/wiki/Odds). This is how people in the gambling industry sometimes use probabilities: instead of saying $1/3$ probability of this and $2/3$ of that, they say that odds are 1:2. Odds are like probabilities, but they don't sum up to one -- odds 1:2 are the same as 2:4. This way of thinking is actually very useful: Often, you don't want to bother thinking about how to normalize probabilities, so you just think in terms of odds.

Using odds, the Bayes' formula from equation ?? now makes a plenty of sense! You just have to multiply two ratios:

![Bayes' theorem using odds](01-kl_intro/bayes_theorem.png)

That is, to compute the posterior odds, we multiply two things. First, the prior odds $P(fair) : P(biased)$. Second, the ratio $P(flipped heads | fair) : P(flipped heads | biased)$ of how likely heads are under each hypothesis. Probability expressions of the type $P(something happens | hypothesis)$ are called _likelihoods_, so this is also called the likelihood ratio.

Bayes' theorem simply says we should do the simplest thing possible with the two relevant ratios -- multiply them -- to arrive at the posterior odds $P(fair | flipped heads) : P(biased | flipped heads)$ .

## Go forth and multiply

KL divergence is about what happens if we keep flipping the coin. That is, what happens if our Bayesian hero keeps flipping a coin and after each flip, she applies Bayes' theorem to update her belief? The picture you should have in mind is this one:

![Repeated application of Bayes' theorem](01-kl_intro/repeated_bayes.png)

As our hero keeps flipping, she keeps multiplying her posterior by one of the two likelihood ratios, $5/4$ for each head, and $5/6$ for each tail.

For example, in our picture, three tails out of five flips point slightly towards the biased hypothesis. So, after we normalize the posterior odds to probabilities, we find out that the original prior of about $67\%$ for the coin being fair dropped by $3\%$ to about $64\%$. Clearly, we need much more flips to find out what the truth is.

## Measuring evidence in bits

Before we dive into what's going on in here, it will turn out to be easier to use the language of bits. That is, we will take the logarithm of all our numbers and talk about adding stuff instead of multiply stuff.

First, let's rewrite our table like this:

![Repeated application of Bayes' theorem in log scale](01-kl_intro/repeated_bayes_log2.png)

We really just rewrote all numbers as $2$ to the power of something, so that multiplications can be thought of as summations in the exponent.

In fact, let's remove all the clutter and just write down the important stuff:

![Surprisal](01-kl_intro/repeated_bayes_surprisal.png)

Basically, instead of writing down probabilities $p$ in each cell, we wrote down $\log 1/p$ instead. The formula $\log 1/p$ is called a _[surprisal](https://en.wikipedia.org/wiki/Information_content)_. That's because the intuition behind this number is that whenever an event happens, this tells you how surprised you are: If you flip heads and thought the probability of that is $1\%$, you are more surprised ($\log 1/0.01 \approx 6.6$) then if you though the probability is $50\%$ ($\log 1/0.5 = 1$).

Our table now becomes a kind of a balance sheet where we keep track of all the surprisals.
If we subtract two surprisals in the same row, the result tells us how much evidence we gained towards one of the hypotheses. In our case, whenever we flip heads, we gain $0.32$ bits of evidence that the coin is fair, and whenever we flip tails, we gain $0.26$ bits of evidence that the coin is biased.

We get the final posterior probability by computing the total suprisal for both hypothesis and exponentiating it (we also have to add the prior to the equation).

In our case, the total evidence we accumulated towards the coin being biased is

$$
3 \cdot 0.26 - 2 \cdot 0.32 \approx 0.15,
$$

We started with one bit of evidence towards the fair hypothesis, so our posterior is $0.85$ bits towards the fair hypothesis. We can convert this number back to probabilities and get the number of $64\%$ for the fair hypothesis.

## Expected Distinguishing Evidence <a id="expected-distinguishing-evidence"></a>

Let's say that in our scenario, the coin happens to be biased. To compute how quickly our Bayesian hero learns this fact, we can compute the expected number of bits she learns per flip. We've computed that heads means $0.26$ bits towards the fair hypothesis and tails means $0.32$ bits towards the biased hypothesis. So, on average, each flip we accumulate

$$
0.4 \cdot (-0.32) + 0.6 \cdot 0.26 \approx 0.03
$$

bits of evidence towards the true hypothesis. This number is called KL divergence between the (true) hypothesis of 40\% / 60\% and the (model) hypothesis of 50\% / 50 \%.

What should we take from this number? It means that on average, we need about 33 flips to gain one bit of evidence. So, if we started with a prior of $2:1$ towards the coin being fair, after about 33 flips we will be about $1:1$. After another 33 flips, we will be $2:1$ towards the coin being biased. Then $4:1$ after another 33 flips and so on.

The actual odds that our Bayesian hero keeps computing are going to fluctuate around this expected behavior. However, due to the law of large numbers, we know that as the number of flips $N$ gets larger, with high probability, the total number of accumulated bits is going to be around $0.03 \cdot N$. <Footnote> More precisely, it's around $0.03N \pm O(\sqrt{N})$. The fundamental reason why we took the logarithm of probabilities and keep talking about bits is that the law of large numbers holds if you keep summing stuff, but not if you keep multiplying stuff. </Footnote>

Try what happens yourself in the widget!

<EvidenceAccumulationSimulator />

## KL divergence, in general

KL is simply the general formula for expected accumulated evidence. In the general case, you have two distributions $p$ and $q$. Let's say $p$ is the truth. Whenever we see the $i$-th outcome, we compare the two likelihoods of this outcome, $p_i$ and $q_i$, and update our posterior. Since we took the logarithm, this means adding $\log p_i$ bits to column counting the total evidence towards the truth, and $\log q_i$ bits to the other column. <Footnote> Of course, although I use $p$ to denote the truth, the Bayesian reasoner that applies the rules does not what the truth is. </Footnote>

On average, each sample from the true distribution $p$ gives us

$$
D(p,q) = \sum_{i = 1}^n p_i \cdot \log \frac{p_i}{q_i}
$$

bits of evidence towards the truth.

In case this number is smaller than $1$, you can think intuitively of $1/D(p,q)$ as "how many samples are needed to gain one bit of evidence towards the truth". Here, one bit of evidence means that true hypothesis got multiplied by $2$ in your posterior odds. This is not the same as saying that your posterior probability got multiplied by $2$. But it is the case that if you have unbalanced odds like $1:1000$, getting a bit of evidence towards the first hypothesis (it's now 2:1000) is almost the same as multiplying the corresponding probability by 2. And getting a bit of evidence towards the second hypothesis (it's now 1:2000) is almost the same as dividing the probability of the first hypothesis by 2.

Notice how KL divergence is about the evidence, not the prior. It tells you the rate (speed) of how your prior shifts, regardless of where you started.
I am mentioning this because if you like to divide statistics into Bayesian and frequentist one (don't worry if you don't know what that means), our framing might suggest that we are firmly on the Bayesian-statistics ground. That's not the case, KL does not care about our petty divisions and is useful in both approaches.

## Asymmetry

It's important to understand that the KL formula is not symmetrical -- in general, we don't have $D(p,q) = D(q,p)$. This is sometimes seen as a drawback, especially when you compare KL with (symmetrical) distance function like the $\ell_1$ or $\ell_2$ metric<Footnote> <Math math='d_1(p,q) = \sum_{i = 1}^n |p_i - q_i|'/>, <Math math='d_2(p,q) = \sqrt{\sum_{i = 1}^n (p_i - q_i)^2}'/></Footnote>. I want to stress that the asymmetry is by design! KL measures how well a distribution $p$ is fitted by a model $q$. This is an asymmetrical notion, so we need an asymetrical formula, nothing to be ashamed for here.

In fact, the word _divergence_ is used for things like KL that behave kind of like distances but are not really symmetrical. <Footnote>More on that in [one of the last chapters](06-algorithms)</Footnote>

One important special case of the asymetry is this: Imagine that the true probability distribution is 50\%/50\% (i.e., fair coin), and the model is $100\% / 0\%$. In that case, KL divergence is infinite! That's because if we flip a coin, we have 50\% chance of flipping tails, which the model distribution says should never happen. This means that we have 50\% chance of gaining infinitely many bits of information (whatever our prior on fair/biased was, the posterior should now be 100\% fair and 0\% biased) and hence the divergence is infinite.

On the other hand, if the truth is $100\% / 0\%$ and the model is $50\% / 50\%$, then each flip we are going to flip heads and gain one bit of information towards the coin being fair. One bit is a lot and our probability of the coin being fair goes down exponentially fast, but it's never zero. We simply have to account for the (exponentially) unlikely event that even though the coin is fair, all flips happened to be heads.

## Wait a minute -- Can KL be negative?

If you plug in the same distribution twice into KL, you get

$$
D(p, p) = \sum_{i = 1}^n p_i \cdot \log \frac{p_i}{p_i} = 0
$$

because $\log 1 = 0$.

This makes sense, you can never distinguish the truth from the truth. 🤷

Even better, KL divergence is always nonnegative. We have not proved that, but I think we got a pretty strong intuition that if it were negative, something would be really, really messed up. Just imagine that you have two distributions $p$ and $q$, but as you keep sampling from $p$, you gradually keep believing more and more that the distribuition actually is ... $q$? We'll discuss the nonnegativity in the next chapter.

## Application: How to train your LLM <a id="application-training-smaller-models"></a>

Let's solve [one of our riddles](00-introduction/deep-learning) from the introduction. We asked about how to train a smaller language model to mimic a larger one like GPT-4. In particular, we have a (true) distribution $p$ and a (model) distribution $q$ and want to come up with a function that measures how well the model fits the truth.

That's exactly what KL divergence measures!

What about other options, like minimizing the $\ell_1$ norm, i.e., $\sum |p_i - q_i|$? or $\ell_2$ norm, i.e., $\sum (p_i - q_i)^2$? Doing that would perhaps also work reasonably well, but there is something very uneasy about those approaches. Consider these two scenarios:

1. $p_i = 0.5, q_i = 0.49$
2. $p_i = 0.01, q_i = 0.0$

The standard geometrical norms ($\ell_1, \ell_2$) treat both of these situations as having similar magnitude of error. KL, though, understands that while in the first situation, the model approximates the truth pretty well, the second situation is total failure!

## Application: Measuring distance from independence <a id="information-theory"></a>

Let's solve one of our riddles: [How to measure the distance from independence](00-introduction/information-theory)?

Remember, we have three possible joint distributions:

![Which table is the "most" independent?](00-introduction/independence.png)

All of them have the same marginal distributions, i.e., the row and column sums are the same. If the two marginal distributions were independent, the table would look like this:

![Independent distributions](00-introduction/independence2.png)

Which one of our three distributions is the "most" independent?

To solve this, we need to find a good definition of how to measure the disimilarity between one of the tables and the idealized, independent table. There are all kinds of formulas that we could try<Footnote>To give one example, in practice, this is often done by computing [correlation](https://en.wikipedia.org/wiki/Correlation). Unfortunately, that's not a super satisfactory solution. First, correlation can only be computed for random variables, i.e., distribution over numbers and not general distributions over \{☀️, ☁️\}, so it's unclear how to apply it to our case. Also, zero correlation does not imply that the two variables are independent. </Footnote>, but KL divergence really begs to be used here. Remember, to use KL, we have to have two distributions. One of them is the "truth" -- in our case, one of the three tables. The second one is the "model" -- in our case, it's the idealized independent table. Computing the KL between the two intuitively tells us how well the model match the reality. More technically, it tells us, how long it takes a Bayesian reasoner to find out that the data are really coming from the actual distribution, and not the model one.

The KL divergences turns to be as follows:

$$
D(table 1, independent table) \approx 0.40
D(table 2, independent table) \approx 0.04
D(table 3, independent table) \approx 0.21
$$

In particular, the second table is "closest" to being independent.

This computation can be done whenever you have a joint distribution $(p_1, p_2)$. The KL divergence between $(p_1, p_2)$ and the independent distribution $p_1 \otimes p_2$ is called [mutual information](http://en.wikipedia.org/wiki/Mutual_information) between $p_1$ and $p_2$ and it's a crucial quantity in e.g. information and coding theory. You should think about it as the most natural function that measures how dependent two distributions $p_1$ and $p_2$ are. More technically, it tells you how many bits on average you learn about $p_2$ if you learn the outcome of $p_1$ <Footnote> Or, symmetrically, how many bits you learn about $p_1$ if you learn the outcome of $p_2$. </Footnote>

## What's next?

In the [next section](02-crossentropy), we will look more closely at the formula for KL divergence and see how it connects to entropy and crossentropy of distributions.
