# KL divergence & Bayes rule <a id="quantifying-information"></a>

In this chapter, we will explain what KL divergence is and how it all goes back to Bayes theorem. Also, we'll discuss [one of our riddles](mutual-information).

## Definition of KL Divergence <a id="definition-of-kl-divergence"></a>

Here's what you need to know about KL divergence.

<div>KL divergence measures how well a distribution $p$ is fitted by a model distribution $q$.</div>

This means that the function is something like a distance function measuring how two distributions are close to each other.

The formula for discrete distributions $p_1, \dots, p_n$ and $q_1, \dots, q_n$ is:<Footnote> People typically use $D_{KL}(p||q)$ instead of $D(p,q)$. The two vertical lines are there to emphasize how $D_{KL}(p||q) \not= D_{KL}(q||p)$. We will use the simplified, less cluttered, notation since we are going to talk about this function a lot. ðŸ™‚ </Footnote>

$$
D(p,q) = \sum_{i = 1}^n p_i \log \frac{p_i}{q_i}
$$

The meaning behind the formula is that if you keep sampling from the true distribution $p$, KL tells you how quickly you are able to tell that you indeed sample from $p$, and not from some other distribution $q$. If it is small, this means that $q$ is a good, hard-to-distinguish model of $p$.

I will try to explain this more precisely in this chapter.

## Asymmetry

It's important to understand that the KL formula is not symmetrical -- in general, we don't have $D(p,q) = D(q,p)$. I want to stress that this is by design! KL measures how well a distribution $p$ is fitted by a model $q$. This is something asymmetrical, so we need an asymetrical formula, nothing to be ashamed for.

In fact, the word _divergence_ is used for things like KL that behave kind of like distances but are not really symmetrical. <Footnote>More on that in [one of the last chapters](06-algorithms)</Footnote>
KL stands for Kullback and Leibler, two clever guys who defined it in 50's.

Below, you can play with two distributions and see how different measures of distance behave. I want you to see how KL behaves differently than simple distance functions like $\ell_1$-distance<Footnote> $d_1(p,q) = \sum_{i = 1}^n |p_i - q_i|$</Footnote>. If $p_i > 0$ and $q_i = 0$, KL divergence becomes infinite! We will understand why in a minute, but intuitively, it's because if you sampled an item $i$ from an unknown distribution, you can now be 100\% sure that the distribution is not $q$ -- you got infinitely many bits of evidence by that sample.

[TODO widget s KL, l1, l2 -- ]

## Bayes' theorem

Before we dive into our KL formula, a short recap of Bayes' theorem. This is a theorem about how to change your probabilities in face of evidence. Here's an example:

You have a coin which may or may not be biased. For simplicity, let's say there are only two options: either the coins is fair -- heads/tails with probabilities 50\%, 50\% -- or it gives heads with 40\% and otherwise tails with 60\% probability.

To apply Bayes' theorem, you need a _prior_ on how likely each possibility is. Let's say we choose 50\% fair, and 50\% biased.

To gain evidence, you flip a coin; let's say you got Heads. This is an evidence towards the coin being fair, as the other case has smaller heads-probability. The question is, how to compute the new, _posterior_ probability that your coin is fair?

Bayes' theorem can do that. The formula for it is typically written down as follows:

$$
P(fair | flipped heads) = \frac{P(flipped heads | fair) \cdot P(fair)}{P(flipped heads)}
$$

This is not rocket science -- if you multiply both sides by $P(flipped heads)$, both sides are just a different way of writing down the definition of $P(fair \cap flipped heads)$.

But I like to think of it a bit differently and in my head, I write it as this:

$$
\frac{P(fair | flipped heads)}{P(biased | flipped heads)}
= \frac{P(fair)}{P(biased)} \cdot \frac{P(flipped heads | fair)}{P(flipped heads | biased)}
$$

You can check that the two formulas are the same by yourself. <Footnote>To see this, use the first formula both for $P(fair | flipped heads)$ and $P(biased | flipped heads)$, and divided them. </Footnote> To understand this rephrasing, it's good to think in terms of [odds](https://en.wikipedia.org/wiki/Odds). This is how people in the gambling industry sometimes use probabilities: instead of saying $1/3$ probability of this and $2/3$ of that, they say that odds are 1:2. Odds are like probabilities, but they don't sum up to one -- odds 1:2 are the same as 2:4. This is actually sometimes useful, if you understand implicitly that probabilities have to be normalized, but don't want to bother writing it in your equations.

Using odds, the Bayes' formula now makes a plenty of sense if you ask me! It's just this:

![Bayes' theorem using odds](01-kl_intro/bayes_theorem.png)

That is, we write the prior odds $P(fair) : P(biased)$. Then we write down the ratio $P(flipped heads | fair) : P(flipped heads | biased)$ of how likely heads are under both hypotheses. Probability expressions of the type $P(something happens | hypothesis)$ are called _likelihoods_, so this is also called the likelihood ratio.

These are the two relevant parts of the equation. Bayes' theorem says we should do the simplest thing possible with them -- multiply them -- to arrive at the posterior $P(fair | flipped heads) : P(biased | flipped heads)$ .

## Keep flipping

KL divergence is about what happens if we keep flipping the coin. That is, what happens if our Bayesian hero keeps flipping a coin and after each flip, she applies Bayes' theorem to update her belief? The picture you should have in mind is this one:

![Repeated application of Bayes' theorem](01-kl_intro/repeated_bayes.png)

As our hero keeps flipping, she keeps multiplying her posterior by one of the two likelihood ratios, $5/4$ for each head, and $5/6$ for each tail.

For example, in our picture, three tails out of five flips point slightly towards the biased hypothesis. So, after we normalize the posterior odds to probabilities, we find out that the original prior of about $67\%$ for the coin being fair dropped by $3\%$. Clearly, we need much more flips to find out what the truth is.

## Measuring evidence in bits

Before we dive into what's going on in here, it will turn out to be easier to take logarithm of all numbers. Our picture then becomes a kind of a balance sheet where we keep adding and subtracting the evidence.

![Repeated application of Bayes' theorem in log scale](01-kl_intro/repeated_bayes_log.png)

Concretely: Whenever we flip heads, instead of multiplying the posterior odds by $0.5$ and $0.4$, we think of adding $-1$ to the sum of evidence towards fair hypothesis, and adding $-1.32$ to the sum of evidence towards biased hypothesis. In the end, the difference is what counts, so we can also think of this as the fair hypothesis gaining $0.32$ bits of evidence against the biased hypothesis.

In the end, the fair hypothesis gained $0.32$ bits of evidence to its favor twice, but the biased hypothesis got $0.26$ bits three times. $3 \cdot 0.26 - 2 \cdot 0.32 \approx 0.15$, so the total evidence we accumulated is $0.15$ bits towards the coin being biased. We started with one bit of evidence towards the fair hypothesis, so our posterior is $0.85$ bits towards the fair hypothesis; this turns out to correspond to $64\%$ probability.

## Expected Distinguishing Evidence <a id="expected-distinguishing-evidence"></a>

Let's say that in our scenario, the truth is that the coin is biased. To compute how quickly our hero learns, we can compute the expected number of bits she learns per flip. We've computed that heads means $-0.15$ bits towards the biased hypothesis and tails means $+0.32$ bits. So, on average we accumulate

$$
0.4 \cdot (-0.15) + 0.6 \cdot 0.32 \approx 0.03
$$

bits per flip. What should we take from this number? It means that on average, we need about 33 flips to gain one bit of evidence. So, if we started with a prior of $2:1$ towards the coin being fair, after about 33 flips we will be about $1:1$. After another 33 flips, we will be $2:1$ towards the coin being biased. Then $4:1$ after another 33 flips and so on.

The actual odds that our Bayesian hero keeps computing are going to fluctuate around this expected behavior. But due to the law of large numbers, we can be sure that with high probability, the total number of bits accumulated by $N$ flips is going to be around $0.03 \cdot N$. <Footnote> More precisely, it's around $0.03N \pm O(\sqrt{N})$. The reason why we took the logarithm of everything is that the law of large numbers holds if you keep summing stuff, but not if you keep multiplying stuff. </Footnote>

Try what happens yourself! It could be interesting to try some special cases like $50\%$ vs $51\%$, to get some intuition about how long it make take until the law of large numbers kicks in.

<EvidenceAccumulationSimulator />

## Finally, KL

KL is simply the general formula for expected accumulated evidence. In the general case, you have two distributions $p$ and $q$. Let's say $p$ is the truth. Whenever we see the $i$-th outcome, we compare the two likelihoods of this outcome, $p_i$ and $q_i$, and update our posterior. Since we took the logarithm, this means adding $\log p_i$ bits to column counting the total evidence towards the truth, and $\log q_i$ bits to the other column. <Footnote> Of course, although I use $p$ to denote the truth, the Bayesian reasoner that applies the rules does not what the truth is. </Footnote>

On average, each sample from the true distribution $p$ gives us

$$
D(p,q) = \sum_{i = 1}^n p_i \cdot \log \frac{p_i}{q_i}
$$

bits of evidence towards the truth.

In case this number is smaller than $1$, you can think intuitively of $1/D(p,q)$ as "how many samples are needed to gain one bit of evidence towards the truth". Here, one bit of evidence means that true hypothesis got multiplied by $2$ in your posterior odds. This is not the same as saying that your posterior probability got multiplied by $2$. But it is the case that if you have unbalanced odds like $1:1000$, getting a bit of evidence towards the first hypothesis (it's now 2:1000) is almost the same as multiplying the corresponding probability by 2. And getting a bit of evidence towards the second hypothesis (it's now 1:2000) is almost the same as dividing the probability of the first hypothesis by 2.

Notice how KL divergence is about the evidence, not the prior. It tells you the rate (speed) of how your prior shifts, regardless of where you started.
I am mentioning this because if you like to divide statistics into Bayesian and frequentist one (don't worry if you don't know what that means), our framing might suggest that we are firmly on the Bayesian-statistics ground. That's not the case, KL does not care about our petty divisions and is useful in both approaches.

## Wait a minute -- Can KL be negative?

If you plug in the same distribution twice into KL, you get

$$
D(p, p) = \sum_{i = 1}^n p_i \cdot \log \frac{p_i}{p_i} = 0
$$

because $\log 1 = 0$.

This makes sense, you can never distinguish the truth from the truth. ðŸ¤·

Fortunately, KL divergence is always nonnegative. We have not proved that, but I think we got a pretty strong intuition that if it were negative, something would be really, really messed up. Just imagine that you have two distributions $p$ and $q$, but as you keep sampling from $p$, you gradually keep believing more and more that the distribuition actually is ... $q$?

There's also a short algebraic proof that KL is always nonnegative. Let me show it, although it's perhaps not as exciting.

To prove that for any $p,q$ we have <Footnote>I've used the natural logarithm here to make the proof a bit shorter. Of course, the base of the logarithm does not really change the formula, it's more about in which units do we want to measure information. Computer scientists tend to like bits, physicists tend to like [nats](<https://en.wikipedia.org/wiki/Nat_(unit)>), whatever. </Footnote>

$$
D(p,q) =  \sum_{i = 1}^n p_i \cdot \ln \frac{p_i}{q_i}  \ge 0,
$$

we will estimate the expression $\log \frac{p_i}{q_i}$ inside the summation. Since we already know that the inequality is tight if all $p_i = q_i$, our estimate should be tight in this case. The best approximation of logarithm around $1$ is $\log (1+x) \le x$<Footnote>picture</Footnote>. We can use it like this:

$$
-D(p,q)
= \sum_{i = 1}^n p_i \cdot \ln \frac{q_i}{p_i}
\le \sum_{i = 1}^n p_i \cdot  \left( \frac{q_i}{p_i} - 1 \right)
= \sum_{i = 1}^n q_i - p_i
= 1 - 1 = 0
$$

## Application: Measuring distance from independence

## What's next?
