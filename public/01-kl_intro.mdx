# KL divergence & Bayes rule <a id="quantifying-information"></a>

In this chapter, we'll introduce KL divergence and see how it's tied to Bayes' theorem. Plus, we'll solve some riddles.

<KeyTakeaway>
KL divergence measures how well a distribution $p$ is fitted by a model distribution $q$.
</KeyTakeaway>

## üìù Definition of KL divergence <a id="definition-of-kl-divergence"></a>

Here's the KL divergence formula <Footnote>KL stands for Kullback and Leibler, two guys who came up with this in the 1950s, right after Claude Shannon dropped his [game-changing paper](https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication) about entropy. Here's the first exercise: Try saying "Kullback-Leibler divergence" fast three times in a row. </Footnote> for two discrete distributions $p_1, \dots, p_n$ and $q_1, \dots, q_n$:<Footnote> About notation: Most people write $D_{KL}(p||q)$ instead of $D(p,q)$. The double bars are there to remind you that $D_{KL}(p||q) \not= D_{KL}(q||p)$. We'll keep it simple with $D(p,q)$ since we'll be using this a lot. üôÇ </Footnote>

$$
D(p,q) = \sum_{i = 1}^n p_i \log_2 \frac{p_i}{q_i}
$$

As a computer scientist, I always use $\log_2$, so from now on I'll drop the subscript and just write $\log$. 

You can think of KL divergence as a wonky distance measure between two distributions. But a better intuition goes like this: When you keep sampling from the true distribution $p$, KL divergence tells you how fast you can figure out you're sampling from $p$ and not some other (model) distribution $q$. Small KL means that $q$ is a pretty good imposter in pretending to be $p$.

We'll break this down in the rest of the chapter, bit by bit. üôÇ

## üéØ Bayes' theorem

Before we dive into KL, let's refresh on Bayes' theorem‚Äîit's how you update your beliefs when you get new info. 

Example: You've got a coin that might be rigged. To keep it simple, let's say there are only two options: it's either fair (50/50 for heads/tails) or it's biased toward tails (25/75 for heads/tails).

To use Bayes, you need a starting guess (a _prior_) about which it is. Let's say you think there's a 2/3 chance it's fair and 1/3 chance it's biased.

You flip the coin‚ÄîHeads! This is evidence for the hypothesis that the coin is fair since heads are more likely with a fair coin. 

Bayes' rule calculates the new probability (the _posterior_) that it's fair. The standard way to write the rule goes like this:

<Math id="bayes-formula" displayMode={true} math='P(\textrm{fair} | \textrm{heads}) = \frac{P(\textrm{heads} | \textrm{fair}) \cdot P(\textrm{fair})}{P(\textrm{heads})}' />

This is no rocket science ‚Äî if you multiply both sides by $P(\textrm{heads})$, both sides express the quantity <Math math = "P(\textrm{fair AND heads})" /> using conditional probabilities.

## üîÄ Divide and conquer

I like to think about Bayes' rule a bit differently:

<Math displayMode={true} math='\underbrace{\frac{P(\textrm{fair} | \textrm{heads})}{P(\textrm{biased} | \textrm{heads})}}_{\textrm{Posterior odds}} = \underbrace{\frac{P(\textrm{fair})}{P(\textrm{biased})}}_{\textrm{Prior odds}} \cdot \underbrace{\frac{P(\textrm{heads} | \textrm{fair})}{P(\textrm{heads} | \textrm{biased})}}_{\textrm{Likelihood ratio}}' />

This formula is just another way of writing down the Bayes' rule. <Footnote>Why? Use formula <EqRef id="bayes-formula" /> for both $P(\textrm{fair} | \textrm{heads})$ and $P(\textrm{biased} | \textrm{heads})$, then divide them. </Footnote> To understand the message of this version, let's think in [odds](https://en.wikipedia.org/wiki/Odds) instead of probabilities. Gamblers love this‚Äîinstead of "1/3 chance of this, 2/3 chance of that," they say "odds are 1:2." Odds don't need to add to one (1:2 is the same as 2:4), which actually often comes pretty handy - it allows you to focus on the more important parts of the picture.

With odds, Bayes' formula is super clean! You just have to multiply your prior odds by <Math math = "\frac{P(\textrm{heads} | \textrm{fair})}{P(\textrm{heads} | \textrm{biased})}" />‚Äîthat's how much more likely heads are under each hypothesis. These conditional probabilities <Math math = "P(\textrm{event} | \textrm{hypothesis})" /> are called [_likelihoods_](https://en.wikipedia.org/wiki/Likelihood_function), so this ratio is the likelihood ratio.

Here's how it works in practice: <Footnote>Getting Bayes' rule is super important. If this explanation is too fast, check out [this explainer](https://www.lesswrong.com/w/bayes-rule). </Footnote>

<BayesCalculatorWidget />

We increased our probability of fair coin from $66.7\%$ to $80\%$. Our suspicion seems to be confirmed but it could also be a fluke. Let's flip the coin a few more times. 

{/*Bayes' theorem simply says we should do the simplest thing possible with the two relevant ratios -- multiply them -- to arrive at the posterior odds $P(\textrm{fair} | \textrm{heads}) : P(\textrm{biased} | \textrm{heads})$ .*/}

## ‚úñÔ∏è Go forth and multiply

KL divergence is about what happens when we keep flipping. Picture our Bayesian hero flipping over and over, updating her beliefs each time using Bayes. If she gets $\{H, T, T, H, T\}$, here's what's happening:

<BayesSequenceWidget />

Every flip, she multiplies her odds by a likelihood ratio: $2/1$ for heads, $2/3$ for tails.

In this example, three tails out of five slightly favor the fair-coin hypothesis. After converting back to probabilities, her initial 66.7% belief in fairness increased to about 70%. Gonna need way more flips to know for sure!

## üìä A bit better summary

Before we see what happens long-term, let's improve our calculator a bit. We'll get a bit more clarity if we take logs of everything. Instead of multiplying odds and likelihood, we will be adding so-called _log-odds_. 

Here's the same step-by-step calculator after taking logarithm. For example, "prior 1:0" in the calculator means that the prior is $2^1 : 2^0$. 

<BayesSequenceLogWidget />

Notice that all the likelihoods (numbers in yellow rows) are now negative numbers. That makes sense - probabilities are smaller than one, so after taking the log, they become negative numbers. It's often useful to talk about absolute values of those numbers, which is why people define a quantity called _[surprisal](https://en.wikipedia.org/wiki/Information_content)_: Whenever you have something happening with probability $p$, the expression $\log 1/p$ can be called a surprisal and its values are bits. 

This is a logarithmic viewpoint on how surprised you are when something happens. Getting heads when you thought it was 1% likely? Super surprising ($\log 1/0.01 \approx 6.6 \textrm{bits}$). Getting heads on a fair coin? Meh ($\log 1/0.5 = 1 \textrm{bit}$).

When we subtract surprisals for the same outcome under different hypotheses, we get how much evidence that outcome provides. For our coin example, heads give us 1 bit of surprisal given the fair hypothesis and two bits of surprisal given the biased hypothesis, so we get 1 bit of evidence towards the fair hypothesis. Analogously, tails give 0.58 bits of evidence towards the biased hypothesis.

To get the final probability, add up all the surprisals for each hypothesis, exponentiate, and don't forget the prior.
In our example, the total evidence for the fair hypothesis is:

$$
1 - 0.58 - 0.58 + 1 - 0.58 \approx 0.25. 
$$

We started with one bit favoring the fair hypothesis, so we end up with 1.25 bits for fair. Convert that back and you get about 70% probability the coin is fair.

## Expected evidence <a id="expected-distinguishing-evidence"></a>

Let's say the coin actually is biased. How fast will our Bayesian hero figure this out? We can calculate the average number of bits that she learns per flip. Heads give -1 bit (negative because it points the wrong way), tails give +0.58 bits. On average, each flip gives:

$$
0.25 \cdot (-1) + 0.75 \cdot 0.58 \approx 0.19
$$

bits of evidence toward the truth. This is the KL divergence between the true 25%/75% distribution and the model 50%/50% distribution!

What does this mean in practice? After about 5 flips, you get one bit of evidence on average. So if you start thinking 2:1 the coin is fair, after ~5 flips you'll be at 1:1. Another 5 flips gets you to 2:1 it's biased, then 4:1, and so on.

The actual odds bounce around this average. But thanks to the law of large numbers, after $N$ flips the total evidence will be close to $0.19 \cdot N$. <Footnote> More precisely, it's $0.19N \pm O(\sqrt{N})$. Ultimately, we use logs and talk about bits because the law of large numbers works for adding stuff, not multiplying. </Footnote>

Try it yourself! I recommend checking edge cases like 50% vs 51% (to get  intuition about when the law of large numbers kicks in) or what's the difference between 50% vs 1% and 1% vs 50%. 

<EvidenceAccumulationSimulator only_kl_mode={true} />

## üîÑ KL divergence, in general

KL divergence is just the general formula for expected evidence accumulation. Say you've got two distributions $p$ and $q$, where $p$ is what's really happening, but you only know it's either $p$ or $q$. 

You can keep sampling from the unknown distribution and play the Bayesian game: Whenever you sample outcome $i$, compare the likelihoods $p_i$ and $q_i$ and update your beliefs. This means adding $\log 1/p_i$ bits to $p$'s surprise total and $\log 1/q_i$ bits to $q$'s. 

On average, each sample from the true distribution $p$ gives you:

$$
D(p,q) = \sum_{i = 1}^n p_i \cdot \log \frac{p_i}{q_i}
$$

bits of evidence toward the truth.

When this number is small (less than 1), you can think of $1/D(p,q)$ as "how many samples until I get one bit of evidence." One bit means the odds for the true hypothesis doubled. <Footnote>This isn't the same as doubling the probability. Going from 1:1 to 2:1 odds means 50% ‚Üí 66.7% probability. But with lopsided odds like 1:1000, gaining a bit toward the underdog (making it 2:1000) almost doubles its probability. And gaining a bit the other way (1:2000) almost halves the underdog's probability. The truth's probability shoots up exponentially until it's comparable to the alternative, then the alternative's probability tanks exponentially. </Footnote>

Notice KL divergence is about the evidence, not your starting beliefs. It tells you how fast beliefs change, no matter where you start. Sometimes, we like to divide statistics into Bayesian and frequentist; KL is useful for both. 


## üß© Cracking some riddles

<RiddleSolution riddle="independence" id="information-theory">

Back to our riddle: [How do you measure distance from independence](00-introduction#information-theory)?

Remember our three joint distributions $p_1, p_2, p_3$:

![Which table is the "most" independent?](00-introduction/independence.png)

They all have the same marginals (same row and column totals). If the marginals were independent, we'd get this product distribution $q$:

<img src="00-introduction/independence2.png" style={{width: '66%', align: 'center'}} />

Which table is the "most" independent?

We need to measure how different each table is from the ideal independent one. There are more reasonable ways to do this<Footnote>People often use [correlation](https://en.wikipedia.org/wiki/Correlation), but that has problems. First, correlation only works for numbers, not general stuff like \{‚òÄÔ∏è, ‚òÅÔ∏è\}. Plus, zero correlation doesn't mean independence. </Footnote>, but using KL divergence is a very principled way to do this. We've got two distributions: the "truth" (one of our tables) and the "model" (the ideal independent table). The KL between them tells us how well the model matches reality‚Äîbasically, how long until a Bayesian detective figures out the data isn't coming from the independent model.

The KL divergences for our tables:

$$
D(p_1, q) \approx 0.40
$$
$$
D(p_2, q) \approx 0.04
$$
$$
D(p_3, q) \approx 0.21
$$

So table 2 is "closest" to independence!

This works for any joint distribution $(r, s)$. The KL divergence between $(r, s)$ and the independent version $r \otimes s$ is called [mutual information](http://en.wikipedia.org/wiki/Mutual_information) between $r$ and $s$‚Äîit's a super-important quantity in information theory.

<MutualInformationWidget /> 

</RiddleSolution>

## üöÄ What's next?

In the [next section](../02-crossentropy), we'll dig deeper into the KL formula and see how it connects to entropy and cross-entropy.