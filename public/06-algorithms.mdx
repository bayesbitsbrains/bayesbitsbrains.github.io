# More on algorithms

In the [previous chapter](05-machine_learning), we have seen how KL divergence guides to go all the way from having a rough guess of how our data look like to a well-defined problem of optimizing a concrete loss function. This part is where KL divergence shines and shows us the way.

But there's one more step missing -- how do we solve the final optimization problem? This is the place where understanding KL is not necessarily too important. In a few fortunate cases (estimating mean/variance, linear regression), we have explicit formulas for how the solution look like, so we solve the problem by plugging into those formulas. <Footnote>Although, in the case of linear regression, the formula involves inverting a matrix, so in practice it's faster to solve the problem by running gradient descent. </Footnote>

Most machine learning problems are however NP hard ($k$-means, neural net optimization) and we try to solve them using some kind of locally-improving algorithm.

In case of $k$-means, it would be what's called Lloyd's algorithm, a special version of a more general algorithm called EM. We will discuss in the section on [Variational methods](variational-methods) how this is connected to a large class of methods called variational Bayesian methods, and a technique called ELBO.

In case of neural network and many other machine learning problems, we solve them using gradient descent. There are several variants of this algorithm and one very useful one is called _multiplicative weights update_. This is a super important algorithm, but it's not usually explained as a variant of gradient descent. We will understand the connection in [the section on multiplicative weights](multiplicative-weights), using KL divergence.

## Variational Bayesian methods <a id="variational-methods"></a>

## Multiplicative weights update <a id="multiplicative-weights"></a>

Let's recall one of our riddles:

Let's say that we want to get rich by investing at the stock market. Fortunately, there are $n$ investors that are willing to share their advice with us: Each day $t$, they give us some advice, and at the end of the day, we learn how good the advice was -- for the $i$-th expert, we will have loss $\ell_i^{(t)}$ <Footnote>If this number is positive, the advice was bad, if it is negative, the advice was good. </Footnote>

Our general investing strategy is this: We start with a uniform distribution $p_1^{(0)}, \dots, p_n^{(0)}, p_i^{(0)} = 1/n$ over the experts. At the beginning of each day, we sample an expert from this distribution and follow her advice. At the end of the day, we look at the losses $\ell_i^{(t)}$ and we update $p^{(t)}$ to $p^{(t+1)}$.

The question is: how should we update?

One way to do that is using gradient descent:

$$
p_i^{(t+1)} = p_i^{(t)} - \varepsilon \cdot \ell_i^{(t)}
$$

This algorithm has some problems, like that the probabilities can become negative and they don't sum up to one. These are not so crucial, we can always round negative numbers to zero and normalize the distribution afterwards. But even after that, the algorithm does not feel alright -- it does not distinguish between changing $50\%$ to $49\%$ and changing $1\%$ to $0\%$. We are going back to the issue that probability should be often handled "multiplicatively", not "additively".

Hence multiplicative weights update algorithm:

$$
p_i^{(t+1)} \propto p_i^{(t)} \cdot e^{- \varepsilon \cdot \ell_i^{(t)}}
$$

The $\propto$ just means that we normalize $p^{(t+1)}$ after the multiplicative update. This turns to be a better way to approach the problem. The math behind this is a long story<Footnote> Check out e.g. ... Multiplicative weights are used in machine learning (the keyword: bandit problems and recommendation systems), algorithm design, optimization, game theory etc. etc. </Footnote>, but I want to give you two intuitions why this algorithm is a good idea.

The first intuition is pretty down-to-earth and uses what we already learnt. The second intuition is more abstract, but I think it nicely explains how gradient descent and multiplicative weights are connected.

### Softmax intuition

Let's think about all the information that we learn in the first $t$ steps of our investing game. Each expert $i$ accumulated a total loss $L_i^{(t)} = \ell_i^{(1)} + \dots + \ell_i^{(t)}$. How should this total loss translate to the probability that we sample her in the $t+1$-th round?

We understand that! In the chapter on [max entropy distributions](04-max_entropy), we have seen how softmax function is the right generic way of converting numbers to probabilities. So, we should have $p_i^{(t+1)} \propto e^{\lambda \cdot L_i^{(t)}}$. The constant $\lambda$ is a proportionality constant that we have to decide on by some other means.

But look, this is exactly what multiplicative weights algorithm is doing. It updates $p_i$ by multiplying it by $e^{\varepsilon \cdot \ell_i^{(t)}}$ in each step, so $p_i^{(t)}$ is proportional to $e^{\varepsilon \cdot \ell_i^{(1)}} \cdot \dots \cdot e^{\varepsilon \cdot \ell_i^{(t)}} = e^{\varepsilon \cdot L_i^{(t)}}$. So what we call $\lambda$ in softmax is called the learning rate $\varepsilon$ in multiplicative weights.

### Geometric intuition

As we have seen in the [preceding chapter](05-machine-learning), it's really nice to think in terms of optimization problems that optimize loss functions. In fact, one step of gradient descent or multiplicative weights algorithm can be seen as optimizing a certain loss function. Let's see how.

In our optimization problem, we start with the following data: the current distribution $p^{(t)}$ and loss function $\ell^{(t)}$. Notice how we try to balance two things:

1. We want the new probability distribution $p^{(t+1)}$ to be close to the original one $p^{(t)}$, whatever the word "close" means.

2. If we sample a random expert from $p^{(t+1)}$, we want her to be as good as possible for the current loss function. That is, we want to minimize $\sum_{i = 1}^n p^{(t+1)}_i \cdot \ell_i$.

Having this in mind, here's an equivalent formulation of gradient descent and multiplicative weights:

$$
p^{(t+1)}_{\text{gradient descent}} = \argmin_{p} \left(
   \sum_{i = 1}^n \ell_i p_i + \eps \cdot \frac12 \sum_{i = 1}^n (p_i - p_i^{(t)})^2\right).
$$

$$
p^{(t+1)}_{\text{multiplicative weights}} = \argmin_{p, \sum_{i=1}^n p_i = 1} \left(
   \sum_{i = 1}^n \ell_i p_i + \eps \cdot \frac12 \sum_{i = 1}^n p_i \cdot \log \frac{p_i}{p_i^{(t)}} \right).
$$

I will leave to you to check that the solution of the first optimization problem is $p_{i}^{(t+1)} = p_i^{(t)} - \eps \cdot \ell_i^{(t)}$<Footnote>Hint: If you differentiate the loss function with respect to $p_i$, you get $\frac{\partial \mathcal{L}}{\partial p_i} = \ell_i + \eps \cdot (p_i - p_i^{(t)})$. </Footnote>, whereas the solution to the second optimization problem is $p_{i}^{(t+1)} \propto p_i^{(t)} \cdot e^{-\eps \cdot \ell_i^{(t)}}$. <Footnote>Hint: You can use Lagrange multipliers to replace the hard constraint of $\sum_{i = 1}^n p_i = 1$ with the soft constraint of $\lambda (\sum_{i = 1}^n p_i - 1)$ that goes to the loss function. After differentiating with respect to $p_i$, you get $\frac{\partial \mathcal{L}}{\partial p_i} = \ell_i + \eps \cdot ( \log \frac{p_i}{p_i^{(t)}} + 1) + \lambda$. </Footnote>

What I want to focus on is how both algorithms try to balance two constraints (stay close to original solution, make new one good on current loss), and their difference is pretty much only in how they measure distance in the space of all probability distributions.

Gradient descent corresponds to the assumption that we are supposed to use the standard Euclidean distance (or $\ell_2$ distance) to measure the distance between distributions. Of course, this is an amazing distance and if you are optimizing in a complicated multidimensional space, it's a natural choice!

But in the context of our getting-rich riddle, we are optimizing probabilities -- after all, even the gradient descent would have to be altered to keep projecting its solutions to the probability simplex to make any sense. In this space, the most meaningful way of measuring distances is not Euclidean distance, but KL. Hence, multiplicative weights.

But wait a minute! KL can't measure distance. We have emphasised how it's important that $D(p,q) \not= D(q,p)$, which seems pretty bad for measuring distance! Fortunately, we are talking about a setup where both distributions $p^{(t)}, p^{(t+1)}$ are going to end up being very close to each other, and in that case, $D(p,q) \approx D(q,p)$ (more about that in the [next section](07-fisher_info)!).

The general algorithm that generalizes both gradient descent and multiplicative weights is called the mirror descent. The algorithm needs to be supplied with something-like-distance-function. For example, KL divergence works for mirror descent, even though its asymmetric. It's beyond the scope to analyze what properties a function needs to satisfy so that we can plug it inside mirror descent. The class of functions that work is called Bregman divergences. Finally we understand why the word divergence in _KL divergence_!

### The important idea

What I want to stress is this: In the case of multiplicative weights, KL guides us not to come up with the right formulation of the problem to solve (as in [the previous chapter](05-machine_learning)). It guides us to come up with the right algorithm to approach a problem!

## What's next

Next, we will delve a bit deeper into the geometric question of how to derive a distance function between distributions from KL divergence which is asymmetric. This way, we will derive Fisher information -- a key tool in statistics. [Jump to the next section](07-fisher_info)
