# Coding Theory

Entropy is deeply connected to the topic of compression. In this chapter, we will discuss the connection and see how LLMs can be interpreted as compressing algorithms. 

<KeyTakeaway>
Entropy is the unbeatable lower bound for lossless compression, if we treat text symbols independently.  
</KeyTakeaway>


## üìù Code intro
Say you've got a long DNA string built from letters $\{\mathsf{A},\mathsf{C},\mathsf{G},\mathsf{T}\}$. You want to store it using as little disk space as possible.  

Here's the most basic plan: We assign a binary code for each letter and encode the string using that code. 

For example, we can use <Math math = "\mathsf{A} \rightarrow \textsf{00}, \mathsf{C} \rightarrow \textsf{01}, \mathsf{G} \rightarrow \textsf{10}, \mathsf{T} \rightarrow \textsf{11} " />. The string gets stored using just 2 bits per letter. Done! 

Can we do better? Sometimes, yes! Here's a riddle: try to build a code that encodes the following string with 1.75 bits per letter on average. 

While playing, notice that when you e.g. use the code $\mathsf{A} \rightarrow \textsf{0}$, no other letter can get a code-name starting with $\textsf{0}$. The reason is that such a code may not be decodable. E.g., if you use codes <Math  math = "\textsf{0}" /> and <Math  math = "\textsf{00}" />, how do you decode <Math  math = "\textsf{00}" />?

<BuildYourOwnCodeWidget /> 



.

.

SPOILER


.

.

Here's the solution: <Math math = "\mathsf{A} \rightarrow \textsf{0}, \mathsf{C} \rightarrow \textsf{10}, \mathsf{G} \rightarrow \textsf{110}, \mathsf{T} \rightarrow \textsf{111} " />. 
Since the letter frequencies in the text are <Math math = "\frac12, \frac14, \frac18, \frac18"/>, this encoding only uses 
<Math displayMode={true} id = "code-example" math = "\frac12 \cdot 1 + \frac14 \cdot 2 + \frac18 \cdot 3 + \frac18 \cdot 3 = 1.75" />
bits per letter.  

OK, using shorter codes for more frequent letters makes sense, but what's the best way to do this? Coding theory knows the answer. Roughly speaking, it tells us to __try to give a letter with frequency $p_i$ a code-name of length $\log 1/p_i$__. 

For example, looking at <EqRef id = "code-example"/>, you can rewrite the left-hand side as: 
<Math displayMode={true} id = "code-example2" math = " \frac12 \cdot \log \frac{1}{1/2} + \frac14 \cdot \log \frac{1}{1/4} + \frac18 \cdot \log \frac{1}{1/8} + \frac18 \cdot \log\frac{1}{1/8}. " />
Every letter with frequency $p$ got code-name of length exactly $\log 1/p$! Notice that for general strings of length $N$, the length of codes satisfying the $p \rightarrow \log 1/p$ rule is this: 
<Math displayMode={true} math = "N \cdot \sum_{i = 1}^k p_i \cdot \log \frac{1}{p_i} = N \cdot H(p)." />
That is, if you manage to construct a code with $p \rightarrow \log 1/p$, you spend $H(p)$ bits per letter on average. 

If we implement our $p \rightarrow \log 1/p$-rule-of-thumb with the most direct algorithm possible, we get what's known as Shannon's code. This code sorts the letter-frequencies down from the largest one, and assigns to each letter of frequency $p$ a code of length $\lceil \log \frac{1}{p} \rceil$ (we round up because we can't have a code name with 1.5 bits). Here it is for English alphabet:

<ShannonCodeWidget />

You can notice that the number of bits per letter (also called _rate_) is a bit north of the entropy of the underlying distribution. This is because of the rounding $\log 1/p \rightarrow \lceil \log 1/p \rceil$. For example, if $p = 0.49$, we would like to give the letter a code name of length $\log \frac{1}{0.49} \approx 1.03$. Too bad that code names have to be integers and Shannon's code assigns it a code name of length $2$. As a general rule, Shannon's code can use up to 1 bit per letter more than what's the entropy. <Footnote>Try to work out the details of what's happening in Shannon's code. Especially: Why does Shannon's algorithm of assigning letters never runs out of available code names? </Footnote>

Unfortunately, there are situations where Shannon's code is really almost 1 bit worse than the entropy. Take a string that uses just two characters $\mathsf{A}, \mathsf{B}$, but the frequency of $\mathsf{B}$ is close to zero. Then, the entropy is close to zero, but Shannon can't do better than 1 bit per letter. 


### üìú The source coding theorem <a id="source-coding"></a>

The [source coding theorem](https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem) is the most important theorem in coding theory. It basically says that on one hand, there are codes that get very close to the entropy bound. On the other hand, you can't beat that bound. 

Here's the setup of the theorem more precisely. We model texts like this: There is a source that has a distribution over letters. This source keeps sampling letters independently, and sends them to us. This goes on for a long time, say $n$ steps; our goal is to save some binary string to the memory so that later on, we can recover from that string what $n$ letters the source sent. 

This setup with the source sampling independent letters models that we only want to think about the problem of giving code names to letters based on frequencies. We are not trying to use the _correlations_ like '$\mathsf{th}$ often follows by $\mathsf{e}$' to get better compression.  

The source coding theorem says that first, there's a way to store the emitted string, using close to $H(p)$ bits per letter. We have come close to that bound above - Shannon's code is only 1 bit worse than entropy. But with more complicated codes, we can even get rid of this 1-bit slack (at the expense that the code is no longer as simple as mapping single letters to code names). 

<Expand advanced={true} headline="More on better codes">

There are two simple ways to get rid of the 1-bit slack. 

### Shannon's code on tuples
Think of constructing Shannon's code not for every letter, but for every _pair of letters_ (i.e., the alphabet grows from $26$ to $26^2$). If Shannon's code loses 1 bit per encoded letter-pair, it means it loses $1/2$ bits per actual letter! Doing this trick not for a pair of letters but a tuple of several letters can bring us arbitrarily close to a code that spends $H(p)$ bits per letter. 

This is a theoretical argument why we can reach the entropy limit. It's not very practical, though, since the alphabet gets very larger pretty soon ($26^2$ for pairs, $26^3$ for triples, ...). 

### Arithmetic coding
[Arithmetic coding](https://en.wikipedia.org/wiki/Arithmetic_coding) is a much better way to encode strings into binary, and it's widely used in practice. 

[todo drop or widget]

</Expand>

The second part of the theorem says that we can't beat the rate $H(p)$. Details for that in the expand box. 

<Expand advanced = {true} headline = "Details of Shannon's source coding theorem">
To see why we can't do better than $H(p)$ bits per letter on average, we will have to understand [Kraft's inequlaity](https://en.wikipedia.org/wiki/Kraft%E2%80%93McMillan_inequality). This inequality says that if we work with alphabet with $k$ letters and we build a code with words of lengths $\ell_1, \dots, \ell_k$, then it has to be the case that 
<Math id = "kraft" displayMode = {true} math = "\sum_{i = 1}^k 2^{- \ell_i} \le 1. " />

To understand this inequality, go back to widgets above - whenever we give a code-name to a letter, like <Math math="\mathsf{e} \rightarrow \textsf{000}" />, we can no longer use codewords that start with <Math math="\textsf{000}" /> for other letters, since that would create clashes. The node with <Math math="\textsf{000}" /> has to be a _leaf_. Some leaves 'take more space' than others - for example, using code name of <Math math="\textsf{0}" /> intuitively kills off half of the space of possibilities. 

A bit more formally, imagine continuing the full binary tree up to some very large depth $N$. Then a code word of length $\ell_i$ is above <Math math="2^{N - \ell_i}" /> of nodes at depth $N$. Since different code words cover disjoint intervals of depth-$N$ nodes, and there are $2^N$ nodes at depth $N$, it has to be the case for any valid code that 
<Math displayMode={true} math="\sum_{i = 1}^k 2^{N - \ell_i} \le 2^N" />
Divide by $2^N$ and you get Kraft's inequality <EqRef id = "kraft"/>. 

I actually like to think about Kraft's inequality as equality. Why? Well, if your code is such that the left-hand side is really smaller than $1$, then you are stupid. <Footnote>You can notice that Shannon's code above is 'stupid' since for English, it leaves some space at the right of the binary tree. In fact, Shannon's code is useful mostly for didactical reasons and in practice you would construct the code by [Huffman's algorithm](https://en.wikipedia.org/wiki/Huffman_coding). </Footnote> In the widget below, you can see how codes can be iteratively improved to get equality. 

<KraftInequalityWidget />


The reason why this is helpful is that I like to think about the numbers <Math math="q_i = 2^{-\ell_i}" /> as some kind of idealized probabilities. Widget above shows that we can pretty much assume that the numbers $q_i$ always sum up to 1. You can think about the numbers $q_i$ as the probability distribution _implied_ by your code-name lengths $\ell_i$. Intuitively, the code is optimized for the distribution $q$, not $p$. 

This setup with $p$ and $q$ is little bit like our discussions in the first two chapters. In fact, let's write down the fact that KL divergence between $p$ and $q$ is always nonnegative. Let's write it down in the form $H(p, q) \ge H(p)$:
<Math displayMode = {true} math="\sum_{i = 1}^k p_i \log \frac{1}{q_i} \ge \sum_{i = 1}^k p_i \log \frac{1}{p_i}" />
Plugging in <Math math="q_i = 2^{-\ell_i}" />, we get
<Math displayMode = {true} math="\sum_{i = 1}^k p_i \ell_i \ge H(p)." />
That is, the average code-name length is at least $H(p)$. This works for any code, hence the second part of [Shannon's source coding theorem](https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem). 
</Expand>

## üß† Entropy intuition

Using the fact that good codes can achieve a rate close to entropy can give us a lot of intuition! Remember [how we defined](/02-crossentropy) entropy as the "average surprisal" of a distribution? We could also ask: if we keep sampling from the distribution, how many bits per flip do we need to store the results? 

For example, if we are flipping a fair coin and want to keep the results, there is nothing smarter then writing one bit <Math math="\textsf{0}" />/<Math math="\textsf{1}" /> for each heads/tails that we flipped. This is 1 bit per flip. 

But if we are flipping a biased coin where heads have only $0.01$ probability, there are better ways to store the results! For example, we could split the outcomes into batches of 10, and give a code-name to each of the $2^{10}$ possibilities in a batch. If we use short code-names for very probably outcomes - e.g., $\mathsf{TTTTTTTTTT} \rightarrow \textsf{0}$, we can spare a lot of disk space! <Footnote>This goes back to our discussion of how to improve the slack of one bit in Shannon's code. </Footnote>

The entropy of the coin flip is the rate of the best code, it's the rate at which we generate 'relevant information' with our flips.  

With coding theory intuitions, cross-entropy also becomes very natural: it's how many bits you need when data comes from $p$ but you are using the code that's optimized for a different distribution $q$. For example, in the top widget, you can construct the best code for $\mathsf{AACATACG}$, but then run it on $\mathsf{AAAAAAAA}$. 

Finally, KL divergence (relative entropy) is how much worse your mismatched code is compared to the optimal one. 

We will now see more examples of how entropy and compression clicks. 

## üì¶ Predictive coding and LLMs

So far, all our discussion was encoding the text letter-by-letter. This is not too hard. The real fun starts when we want to compress a given text _without the assumption that letters are independent_. We now want to use the fact that $\textsf{th}$ usually follows by $\textsf{e}$ to achieve the best compression possible. 

Here is a generic approach of how most compressions algorithm work, called [predictive coding](). A generic predictive-coding algorithm goes through the text letter by letter and tries to build some kind of understanding of it. For each letter, it tries to predict it using the past text. As we know, once you have a distribution over the next letter, we can convert it to a code, e.g. Shannon's code. This way, we can encode the whole text.  

I want to explain two examples of predictive coding algorithms. The first example is how LLMs can be used to compress text, the second example would be a typical implementation of ZIP. 

### ü§ñ LLMs 

LLMs are exactly fitting the description of our next-letter predictor. A small technicality - to make the prediction task easier, the text is first split into so-called tokens (think short words, syllables). The net then predicts the next token, not the next letter. 

We have discussed LLMs in the main text; they are trained by reading the Internet, minimizing the cross-entropy loss. But we can now understand that equivalently, __they are trained to compress Internet as much as possible__!  

In the next widget, you can see how it works. 

<GPT2CompressionWidget />

This way of compressing test is extremely efficient, since LLMs can predict the next token even more efficiently than us, people. The catch is that to use this to compression, we would have to send the LLM with the compressed text, so that the receiver can decode. Modern LLMs are huge, so this is not really practical. 

### üóÅÔ∏è ZIP 
Typical algorithm used in ZIP works like this: It reads the text and constructs a dictionary -- list of common patterns (syllables, words) that were frequent in the past. When it encounters a word it has in its dictionary, instead of writing 'divergence', it writes something like [now comes entry 42 in the dictionary]. The algorithm does not juggle with any probabilities. However, due to the connection of probabilties and code lengths, we can now understand that if the ZIP algorithm can at some position encode the word 'divergence' in 10 bits, we can interpret it as the ZIP algorithm __predicting__ that the probability of 'divergence' is $2^{-10}$. 

### üï∞Ô∏è Prediction = Compression

These two examples of predictive coding algorithms go back to our discussion of coding intuition for entropy. We defined entropy in the language of surprise, probabilities, prediction. Coding theory uses the language of codes, and compression. But since the probability $p$ corresponds both to surprise of $\log 1/p$ and ideal-code-length of $\log 1/p$, the math is the same for both. 

Hence, whenever there is a good prediction, there's a good compression, and vice versa. This is really super important and we will return to it in the [Kolmogorov complexity chapter](09-kolmogorov). 


## üß© Cracking riddles <a id = "compression"></a>

<RiddleExplanation id="wikipedia">
We can now revisit [our Wikipedia riddle](00-riddles#wikipedia) about Hutter's compression challenge. Remember, the question is about how much can we compress a single 1GB file of English Wikipedia. Hutter is an AI researcher and he started his challenge because he understands very well the topic of this chapter - if you can compress text, you can also predict it, and that implies you have to have some kind of knowledge/intelligence.  

The honest answer is that we don't know (see the [Kolmogorov complexity](09-kolmogorov) chapter), but w

We broadly understand how well different algorithms compress English text. 

There are more approaches to compress text files. Let's go through them and you can see in the next widget how they fare for various types of data. 

- _Baseline_: The stadard way to store text files is UTF-8. Simplifying a bit, this format store each letter using 8 bits. <Footnote>Why is it lying? [UTF-8](https://en.wikipedia.org/wiki/UTF-8) itself is a beautiful example of good engineering inspired by coding theory. There are around 100 characters (English letters, digits) that are stored using 8 bits, fancier letters from reasonable alphabets are stored using 16 bits, and emojis like üòÄ or hieroglyphs like ìÄÄ take 32 bits. Classic coding theory‚Äîrare stuff gets longer codes. But English Wiki is mostly standard English letters, so 8 bits per character it is. </Footnote>

- _Optimal code_: Treat letters as independent, we can encode them using $H(p)$ bits per letter on average, where $p$ is the distribution of English letter frequencies. [In the Shannon's coding widget above](../02-crossentropy#construction), we saw that the entropy is around 4.2 bits. So, we can shrink the file by almost __2x__ just by using that different letters have different frequencies. 

- _Zipping_: Standard compression algorithms like those used in ZIP implement dictionaries to catch patterns that are often reused in the text. They can compress English text up to a factor around __3x__.  

- _LLMs_: Large Language Models are trained to predict text, hence they compress it. If we disregard the fact that technically speaking, LLM size should count toward the size of the compressed text, they compress English text around __7x__ for old LLMs like GPT-2 and up to perhaps __10x__ for state-of-the-art networks. 

I encourage you to play with the following widget, especially making GPT-2 compress your texts. üôÇ

<CompressionWidget />

The best algorithms in Hutter's competition have compress by a factor of about 9x. But unfortunately, while the challenge is backed by a very deep understanding of the nature of intelligence, the precise parameters of the challenge make it hard to relate to the winning algorithms. Since the file size is 'only' 1GB, the compression-by-LLM approach doesn't easily work. You have to append the LLM to the compressed text and it becomes huge.<Footnote>Or you have to train it 'inside' the compression/decompression algorithm, and for that the file is too small. </Footnote> The winning algorithms are thus used on more 'combinatorial' approaches, like those used in ZIP or stuff like [multiplicative weights](07-algorithms). 

Finally, let me tell you about one of the coolest experiments I know of. Claude Shannon, the GOAT who invented information theory in the late 40s, did the following a few years after his invention. He measured the prediction ability of people: he showed them partial sentences and asked them to guess the next letter. This way, he figured that people can compress English to about 0.5 - 1 bits per letter, roughly on par with the performance of GPT2-4. <Footnote>TODO crunch the numbers more precisely. </Footnote> As far as I can say, this is the first experiment about next-token prediction; Claude Shannon was so based he did it 60+ years before it became cool!
</RiddleExplanation>