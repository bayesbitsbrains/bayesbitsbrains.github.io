# Loss functions

In this chapter, we will leverage our newfound KL superpowers to tackle machine learning. We will understand a crucial aspect of it: [setting up loss functions](00-riddles#machine-learning). 

The main point here is that KL divergence and its friends cross-entropy & entropy are a guiding beacon: **Starting with a rough idea about important aspects of data, they help us to build a concrete estimation algorithm**. Specifically, we can use the _maximum entropy principle_ to transform our initial concept into a fully probabilistic model, and then apply _maximum likelihood_ to derive the loss function that needs to be optimized.


<KeyTakeaway>
Maximum entropy and maximum likelihood principles explain many machine-learning algorithms. 
</KeyTakeaway>


## Good old machine learning <a id="examples"></a>

In the following widget, you can explore four problems. The first one, estimating the mean & variance of a bunch of numbers, is a classical statistics problem. The following ones - linear regression, $k$-means, logistic regression - are "good old machine learning problems", it's the kind of stuff you see in ML 101 class before diving into neural networks. 

I want to convey how it all __makes sense__. Starting with some simplistic visual feel for what we want (like the picture in the widget's canvas), maximum entropy gives us a concrete probabilistic model for what's happening. We can then minimize the cross-entropy (or, if you want, use maximum likelihood approach) to find the best parameters. 

<MLProblemExplorer />

Let's talk about our [statistics riddle](00-riddles#statistics). 

<RiddleExplanation id = "statistics">
I love how in mathematics, we typically have clear answers. We all agree on how to add numbers, compute areas under curves, and how to handle probabilities. 

This unambiguous logical structure of most of mathematics is what makes the statistics so hard to love. Look, there's not even a single obviously right way of estimating the sample variance of bunch of numbers. Sure, the formula should look roughly like this: 
<Math displayMode={true} math="\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n} (X_i - \hat{\mu})^2" />
and this is what we derived by MLE in the above widget, but there are competing approaches. 

For example, if $n = 0$, our formula always says zero. But the underlying probabilistic process that generated the data likely has nonzero variance, and we still always answer 0! Our estimate is _thus_ <a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">biased</a>, we know that it tends to be smaller than the true variance. If we want to make it unbiased, we have to compute it like this: <Math math="\hat{\sigma}^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \hat{\mu})^2" />. And the formula <Math math="\hat{\sigma}^2 = \frac{1}{n+1} \sum_{i=1}^{n} (X_i - \hat{\mu})^2" /> can also be justified by <a href="https://en.wikipedia.org/wiki/Mean_squared_error">the mean squared error</a> minimization. 

It's not so bad though. There actually is a crisp, beautiful, and unambiguous way of doing statistics, and it's called [Bayesian statistics](https://en.wikipedia.org/wiki/Bayesian_statistics). Long story short, Bayesian statistics is about what our Bayesian detective from [the first chapter](00-riddles) keeps doing: She starts with a prior $p_1, \dots, p_n$ over competing hypotheses. Then, gathers evidence and for each hypothesis computes its likelihood $\ell_i$ - the probability of seeing the evidence under the hypothesis. Then, she applies Bayes' rule to compute the posterior. I.e., she computes the products $p_1\ell_1, \dots, p_n \ell_n$ and normalize it to sum up to one, the result is $p_1', \dots, p_n'$. 

There are all kind of theorems that suggest that this is the __right__ way to think about statistics. The only downside to this framework is practical: A Bayesian detective has to start with a prior distribution $p_1, \dots, p_n$, but where to take one? Also, getting the whole posterior distribution $p_1', \dots, p_n'$ is great, but often we just want a simple formula that computes a single number as an answer. 

If we assume that we have tons of data that provides a great deal of evidence, we can simplify this process to its core. First, instead of reporting the whole posterior distribution $p_1', \dots, p_n'$, we will report just hypothesis $i$ with largest $p'_i$ -- hopefully, the posterior probability of other hypotheses is going to be very small anyway in light of all the evidence.<Footnote>This principle is called [maximum a posteriori estimation](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation). </Footnote> 

Also, if there's so much evidence, the prior $p_1, \dots, p_n$ is likely not going to matter much. For example, remember our example of a flipping a coin that is either fair, or 25/75 biased. The KL divergences between the two hypotheses are .. and .. That is, each flip, we are expecting to get around 0.5 bits of evidence for the correct hypothesis; or about $0.5n$ bits if we flip $n$ times. Whether we started with prior $1:2$ or $2:1$ is pretty irrelevant, that's just one bit of evidence that we start with before gathering evidence. So, we may simplify the Bayesian statistics all the way to simply reporting hypothesis $i$ with the largest $\ell_i$ - this is the maximum likelihood approach. 

I like this view on maximum likelihood as "poor man's Bayesian statistics" to appreciate where it's coming from. Perhaps this is also the reason why maximum likelihood approach is so useful in machine learning! It also helps to appreciate the flip side -- if you want to estimate the variance and $n = 3$, perhaps you shouldn't worry too much about using a formula with $\frac{1}{2}, \frac{1}{3}$ or $\frac{1}{4}$. The justifications for why we can simplify Bayes' rule are anyway breaking down, so we should rather spend our time thinking carefully about the prior to use in the Bayesian approach.    

</RiddleExplanation>


<Expand advanced={true} headline="Variational autoencoders"> <a id="variational-autoencoders"></a>

Here's an observation relevant to the previous example as well. When we store images on a hard drive, we do so pixel by pixel, i.e., in the *pixel space*. However, in our brains, we store them very differently; perhaps we remember "it was a picture of a brown dog that looked like a muffin," or something similar. This corresponds to utilizing what's called the *latent space*. This is a hypothesized space where two images are considered "close" if they refer to similar objects. For instance, two images might be close in the latent space even if they are extremely far apart in the pixel space.

The ability to represent the latent space is crucial for any interesting work involving images. In fact, the reason we could separate dogs and muffins in the previous example was that the inner layers of the network were able to access concepts in the latent space, such as "are there ears in the picture."

As the next step, we now desire a system that can not only classify images but also generate new ones from scratch (think DALL-E or Midjourney) or do stuff like interpolating between two images *in the latent space* (see e.g. [this video](https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/videos/interpolations-ffhq.mp4) that uses a different architecture than autoencoders). 

An autoencoder (see below) is a neural network architecture that can do this more interesting stuff. It processes an input image $x$ through an encoder, $\textrm{Enc}(x)$, compressing it into a smaller list of numbers $y$. Subsequently, a decoder, $\textrm{Dec}(y)$, attempts to reconstruct the original image $x$. The hope is that due to the bottleneck in the middle, $y$ captures the essential aspects of image $x$ while discarding less important ones. In other words, the encoder is hoped to encode the image into the latent space.

One challenge with traditional autoencoders is their tendency to learn a "hash function": The decoder might simply memorize all images $x_1, x_2, \dots$, and the encoder and decoder then decide on "names" for these images. The encoder then merely converts an input image to its name, and the decoder outputs the image associated with that name. To combat this issue, practitioners use variational autoencoders. These are autoencoders where the encoder outputs two lists: $\mu_1, \dots, \mu_d$ and $\sigma^2_1, \dots, \sigma^2_d$. The input to the decoder consists of $d$ samples drawn from $N(\mu_1, \sigma_1^2), \dots, N(\mu_d, \sigma_d^2)$. By introducing noise in the middle of the architecture, it becomes more difficult for the network to "cheat" by simply memorizing all images.

![vae](07-machine_learning/vae.png "image from https://en.wikipedia.org/wiki/Variational_autoencoder#/media/File:VAE_Basic.png")

The significant question now is: How do we optimize the variational autoencoder? Specifically, what is the loss function to minimize? Let's try to derive it! This will be more challenging than previous examples, hopefully demonstrating the depth of what we've learned in this minicourse!

Where do we begin? The first item on our to-do list is to formulate a probabilistic model of our data. In this example, we are interested in the joint distribution $p(x, y)$ over imagesâ€”where $x$ represents the image in pixel space, and $y$ represents it in latent space. Notice that we are working with a probability distribution; a particular $y$ corresponds to an entire distribution over images that capture the concept of $y$, not just a single image.

Now, the encoder and decoder represent two distinct ways in which we can factorize the joint distribution.
The encoder corresponds to the factorization $p(x,y) = p(x) \cdot p(y | x)$: First, sample a random image and then encode it into the latent space. The distribution $p(x)$ is known to us: it's the empirical distribution over the large dataset of images we collected for training our autoencoder.
Unfortunately, the distribution $p(y | x)$ is not known, but the encoder attempts to represent it. More precisely, the distribution $p(y | x)$ is approximated by <Math math = "p'(y | x) = N(\textrm{Enc}_\mu(x), \textrm{Enc}_{\sigma^2}(x))" />.

The decoder provides us with a second way to model the data: We first sample a random latent-space representation and then decode it into an image, i.e., $p(x,y) = p(y) \cdot p(x|y)$. We need to be more creative to transform this idea into a concrete probabilistic model. For a start, we don't know the marginal distribution $p(y)$; so we will simply model it with a Gaussian distribution $q(y) = N(0, I)$ ($I$ represents the identity matrix). How should we model $p(x|y)$? Using $\textrm{Dec}(y)$ is a good starting point, but we should also introduce some randomness, so let's add Gaussian noise $N(0,1)$ to every pixel of the final image and model it as $q(x|y) = N(\textrm{Dec}(y), I)$. We have arrived at a different probabilistic model for $p(x,y)$, namely $p(x,y) = q(y) \cdot q(x|y)$.

We will optimize our variational autoencoder by minimizing the KL divergence between these two models of $p(x,y)$. When using KL divergence, we typically prefer the first parameter to represent the "truth" and the second to be a model of it. In this instance, both distributions are models that we will train in unison. However, the first model, $p(x,y) = p(x) \cdot p'(y|x)$, is closer to the truth since it incorporates the actual data $p(x)$, while the second model is primarily a collection of parameters we wish to optimize. Thus, we will train our network by minimizing:

<Math displayMode={true} math = "D\left( p(x) \cdot p'(y | x), q(y) \cdot q(x|y)\right)" />

Finishing the job now boils down to algebra that I hid in the following Expand box so as not to scare you right away. 
<Expand headline="Algebra part">
Let's call our KL loss function $\mathcal L$. We can decompose it into two parts, known as the **reconstruction loss** and the **regularization term**, as follows:
<Math displayMode = {true} math = "\mathcal L = \underbrace{\sum_{x,y} p(x) p'(y|x) \log \frac{p(x)}{q(x|y)}}_{\textrm{Reconstruction loss } \mathcal L_1} + \underbrace{\sum_{x,y} p(x) p'(y|x) \log \frac{p'(y|x)}{q(y)}}_{\textrm{Regularization term } \mathcal L_2} "/>

Let's analyze each term in turn, starting with the reconstruction loss.

### Reconstruction Loss
We can further expand the term $\mathcal L_1$ as:
<Math displayMode={true} math = "\mathcal L_1 = \sum_x p(x) \log p(x) - \sum_{x,y} p(x) p'(y|x) \log q(x|y)"/>
The first term is simply the entropy of the distribution $p(x)$. In our case, $p(x)$ is a uniform distribution over $N$ images $X_1, \dots, X_N$, so it equals $\log N$. In any event, this term is a constant independent of all the parameters we optimize, so we can disregard it. Thus, minimizing $\mathcal L_1$ amounts to minimizing the second term, which we will now interpret.

To do this, recall that $q(x|y)$ involves running the decoder on $y$ and adding Gaussian noise $N(0,1)$ to the result; we therefore have
<Math displayMode={true} math = "q(x | y) \propto e^{-\frac{\| x - \textrm{Dec}(y)\|^2}{2d}}"/>
and hence, up to an additive constant that does not affect our optimization problem, we have
<Math displayMode={true} math = "\log q(x | y) = -\frac{\| x - \textrm{Dec}(y)\|^2}{2d}"/>

So, if we were in the setup of a regular autoencoder where $p'(y|x)$ is a deterministic encoding $\textrm{Enc}(x)$, minimizing the term $\mathcal L_1$ would reduce to minimizing the mean square *reconstruction* loss:
<Math displayMode={true} math = "\frac{1}{N}\sum_{i = 1}^N \frac{\| X_i - \textrm{Dec}(\textrm{Enc}(X_i))\|^2}{2d}. " />

Our case is more complex; we need to minimize:
<Math displayMode={true} math = "\frac{1}{N}\sum_{i = 1}^N \sum_{y} p'(y|X_i) \frac{\| X_i - \textrm{Dec}(y)\|^2}{2d}. " />

The term $p'(y|x)$, which samples from <Math math = "N(\textrm{Enc}_\mu(x), \textrm{Enc}_{\sigma^2}(x))" /> instead of being a deterministic encoding, is problematic to compute directly. We will discuss how to estimate this part of the loss function at the end.

### Regularization Term

Let's analyze the term <Math math = "\mathcal L_2 = \sum_{x,y} p(x) p'(y|x) \log \frac{p'(y|x)}{q(y)}" />. Remember, $p(x)$ is simply a uniform distribution over our image dataset, so we can rewrite this as:
<Math displayMode={true} math = "\mathcal L_2 = \sum_{i = 1}^N \frac{1}{N} \sum_y p'(y|X_i) \log \frac{p'(y|X_i)}{q(y)} = \sum_{i = 1}^N \frac{1}{N} D(p'(y|X_i), q(y))"/>
Thus, minimizing this term reduces to minimizing the sum of KL divergences between $p'(y|X_i)$ and $q(y)$ across our dataset $X_1, \dots, X_N$.

The first distribution $p'(y |X_i)$ is simply a Gaussian with mean $\textrm{Enc}_\mu(X_i)$ and variance that is a diagonal matrix with entries <Math math = "\textrm{Enc}_{\sigma^2}(X_i)" />. The second distribution $q(y)$ is precisely the Gaussian $N(0,I)$. There's a [simple formula](https://leenashekhar.github.io/2019-01-30-KL-Divergence/) for the KL divergence between two Gaussians that looks like this:
<Math displayMode = {true} math = "D(N(\mu, \sigma^2), N(0, I)) = \frac12 \sum_{j = 1}^d \left( \mu_j^2 + \sigma_j^2 - 1 - \log\sigma_j^2\right) "/>

Plugging this into $\mathcal L_2$, we find that we need to minimize the expression:
<Math displayMode = {true} math = "\frac{1}{N} \sum_{i = 1}^N \left( \frac12 \sum_{j = 1}^d \textrm{Enc}_{\mu, j}(X_i)^2 + \textrm{Enc}_{\sigma^2, j}(X_i) - \log \textrm{Enc}_{\sigma^2, j}(X_i) \right)"/>
This is called the regularization term since it ensures that our variational autoencoder can't simply degenerate into a vanilla autoencoder by setting $\sigma_i^2 = 0$.
</Expand>

After performing the algebra, we arrive at the following loss function:
<Math displayMode={true} math = "\frac{1}{N} \sum_{i = 1}^N \left( \sum_y p'(y | X_i) \frac{\| X_i - \textrm{Dec}(y)\|^2}{2d} \,+\, \left( \frac12 \sum_{j = 1}^d \textrm{Enc}_{\mu, j}(X_i)^2 + \textrm{Enc}_{\sigma^2, j}(X_i) - \log \textrm{Enc}_{\sigma^2, j}(X_i) \right)\right)"/>

In general, computing the expression <Math math = "\sum_{i = 1}^N \sum_y p'(y | X_i) \frac{\| X_i - \textrm{Dec}(y)\|^2}{2d}"/> is super hard (NP-hard), as it would require iterating over all the whole range of $p'(y|X_i)$. In practice, this expression can be estimated using the Monte Carlo method. 
That is, for each image $X_i$, we simply sample a few times from $p'(y|X_i)$ and compute the reconstruction loss $\| X_i - \textrm{Dec}(y)\|^2$. We estimate the expectation $E_y$ by plugging in these few samples. 
</Expand>

## ðŸš€ What's next? <a id="next-steps"></a>

In the [next chapter](05-max_entropy), we will see what happens if we minimize the _first_ parameter in $D(p, q)$. 




