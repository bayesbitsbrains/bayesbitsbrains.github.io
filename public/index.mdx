# What's KL divergence?

ðŸ”¥ KL divergence is a formula:
$$D(p,q) = \sum_{i = 1}^n p_i \cdot \log (p_i / q_i)$$
Pretty exciting, isn't it?!

ðŸš€ If you agree, follow to the [Introduction](00-introduction) to see some cool applications. If you disagree, do likewise and maybe we excite you a bit more.

ðŸ§­ We designed this page so that it spends a lot of time on basics, but also goes into some deep rabbit holes, so don't feel like you should read this linearly. Use the menu on the left and Expand boxes to your advantage.

ðŸ‘¥ More meta info on the [about page](about).
