# What's KL divergence?

KL divergence is a formula:
$$D(p,q) = \sum_{i = 1}^n p_i \cdot \log p_i / q_i$$
Pretty exciting, isn't it?!

If you agree, follow to the [Introduction](00-introduction) to see some cool applications. If you disagree, do the same and maybe we excite you a bit more. ðŸ™‚

We designed this page so that it spends a lot of time on basics, but also goes into some deep rabbit holes, so don't feel like you should read this linearly.

More about the authors [here](about).
