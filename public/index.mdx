# What's KL divergence?

ðŸ”¥ KL divergence is a formula:
$$D(p,q) = \sum_{i = 1}^n p_i \cdot \log (p_i / q_i)$$
Pretty exciting, right?!

ðŸš€ If you agree, follow to the [Introduction](00-introduction) to see some cool applications. Otherwise, do likewise and maybe we excite you a bit more.

ðŸ§­ We designed this page so that it covers basics, but also goes into some deep rabbit holes. Don't feel like you should read this linearly. Use the menu on the left and Expand boxes to your advantage.

ðŸ‘¥ More meta info on the [about page](about).
