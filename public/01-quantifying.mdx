# Part 1: Quantifying Information, Belief, and Surprise

> This part introduces the fundamental tools for measuring information, uncertainty, and the relationships between probability distributions. We define Entropy, KL Divergence, Cross-Entropy, and Mutual Information, exploring their interpretations and properties.

---

<a id="introduction-motivation"></a>
## Introduction & Motivation

_(Initial motivating text from the LaTeX intro, discussing the goal and LessWrong inspiration, can be adapted here)_

This text introduces and explains core concepts from information theory, particularly the so-called KL divergence, sometimes also known as relative entropy. It is a function that takes two distributions as input—one being the _true_ distribution and the other our model for it—and outputs a measure of how well the model fits the true distribution.

Surprisingly, understanding this function and related concepts like entropy explains many ideas in probability, statistics, machine learning, and deep learning. This text aims to present takes on probability theory that are not typically covered in standard university courses, focusing on intuitive understanding and connections.<Footnote>Inspired by many conversations, internet explorations, LessWrong blog posts, and hidden gems in technical books.</Footnote>

---

<a id="riddle-predictions"></a>
## Riddle: Scoring Predictions

> It would be great to know what the future holds—or at least to know someone who does. To this end, you have gathered a bunch of experts and asked them to assign probabilities to a list of events for the coming year. Each expert provides an estimate for each event. When the year ends, you know for each event whether it occurred or not. How do we score the experts so that we can pick the best ones?

_(Placeholder Table)_

_(We will revisit this riddle later in this Part)_

---

<a id="riddle-information-theory"></a>
## Riddle: Measuring Dependence

> Suppose you are interested in two things. First, whether a day is rainy or sunny—say, with probabilities 30% and 70%, respectively. Second, how you commute to work (e.g., walk, bike, or take the bus with probabilities 20%, 30%, and 50% respectively). Knowing these two marginal distributions does not give you the full picture. You can form a 2x3 table representing the joint distribution of these outcomes.

> If the two distributions were independent, the probability in each cell would be the product of the marginal probabilities. For example, modeling the distributions as independent, we would guess that the probability of rain and walking is $0.3 \times 0.2=0.06$ (i.e., 6%). But in fact, it is much lower (since I don't want to walk in the rain).

> Is there a good measure of how far the joint distribution is from being independent? For example, given two tables, how would you determine which one is closer to independence? Correlation doesn't work well for categorical data or non-linear dependencies. Can we do better?

_(We will answer this using Mutual Information later in this Part)_

---

<a id="definition-of-kl-divergence"></a>
## Definition of KL Divergence

> **TLDR:** KL divergence measures how well a ``true distribution'' or empirical data $p$ is fitted by a model distribution $q$. The formula is $D(p\|q) = \sum p_i \log p_i / q_i$. The meaning behind this formula is that if you keep sampling from the true distribution $p$ and applying Bayes theorem, KL tells you how quickly you are able to distinguish whether you are sampling from $p$ or $q$.

KL divergence is a measure that takes two distributions as input: the true distribution $p$ (represented by $p_1, \dots, p_n$) and the model distribution $q$ (represented by $q_1, \dots, q_n$), with both summing to one.

KL divergence is defined as
$$ D*{\mathrm{KL}}(p\| q) = \sum*{i = 1}^n p_i \log \frac{p_i}{q_i} $$

_(Placeholder for interactive widget showing KL calculation)_

Notice the asymmetry between $p$ and $q$. If you push the value $q_i$ to zero for some $i$ where $p_i > 0$, the KL divergence becomes infinite. This reflects the fact that if your model predicts an event is impossible (\(q_i=0\)) and it occurs (\(p_i>0\)), your model is infinitely bad. On the other hand, if $p_i=0$ and $q_i>0$, nothing dramatic happens—being prepared for an event that never occurs only slightly detracts from your performance.

Sometimes, KL divergence is introduced as ``something like a distance,'' but it is not symmetric, so it is not a true distance metric. The asymmetry is important because the two distributions play different roles: $p$ is the true distribution (or reference), and $q$ is our model for it.

We now discuss two mathematical intuitions for the formula.

<a id="expected-distinguishing-evidence"></a>
### Expected Distinguishing Evidence

First, a quick recap of Bayes' theorem in terms of log-odds. Suppose you have a coin and consider two hypotheses:

1.  The coin is biased (p): heads prob 0.4, tails prob 0.6.
2.  The coin is fair (q): heads prob 0.5, tails prob 0.5.

Assume hypothesis 1 (biased) is true. Initially you believe it's probably fair, maybe prior odds $1:2$ for biased vs fair (Prob(biased)=1/3).

If you observe Heads (H), the likelihood ratio is $p(H)/q(H) = 0.4/0.5 = 4/5$. Bayes' theorem says update odds by multiplying:
Prior Odds $\times$ Likelihood Ratio = Posterior Odds
$1:2 \times 4:5 = 4:10 = 2:5$.

If you observe Tails (T), the likelihood ratio is $p(T)/q(T) = 0.6/0.5 = 6/5$.
$1:2 \times 6:5 = 6:10 = 3:5$.

It's often easier to work with logarithms (log-odds and log-likelihood ratios, often called _evidence_ in bits or nats). Let's use $\log_2$.
Initial log-odds: $\log_2(1/2) = -1$ bit.
Evidence from H: $\log_2(4/5) \approx -0.32$ bits. New log-odds = -1 - 0.32 = -1.32 bits.
Evidence from T: $\log_2(6/5) \approx 0.26$ bits. New log-odds = -1 + 0.26 = -0.74 bits.

Since the coin is truly biased (hypothesis p is true), we will see Heads with probability 0.4 and Tails with probability 0.6. The _expected_ evidence per flip in favor of the true hypothesis p over the model q is:
$$ \mathbb{E}_{x \sim p}[\text{Evidence for p vs q}] = p(H) \log_2 \frac{p(H)}{q(H)} + p(T) \log_2 \frac{p(T)}{q(T)} $$
$$ = 0.4 \times \log_2(0.4/0.5) + 0.6 \times \log_2(0.6/0.5) $$
$$ = 0.4 \times (-0.32) + 0.6 \times (0.26) \approx -0.128 + 0.156 \approx 0.028 \text{ bits} $$
Notice this expression is exactly the KL divergence $D_{\mathrm{KL}}(p\|q)$ (using $\log_2$).

The positive average evidence indicates that, on average, each flip provides evidence _in favor of the true hypothesis_ p against the alternative q. By the law of large numbers, after $n$ flips, you expect to accumulate about $n \times D_{\mathrm{KL}}(p\|q)$ bits of evidence.

<a id="natural-measure-axioms"></a>
### Natural Measure Axioms

_(Placeholder: This section can discuss the axiomatic derivation, focusing on the chain rule property:_)
It seems natural to require that a measure of divergence $f(p,q)$ satisfies an additivity property for composite experiments. If we have a joint distribution $p(x,y)$ and model $q(x,y)$, we can decompose them using conditionals: $p(x,y) = p(x) p(y|x)$ and $q(x,y) = q(x) q(y|x)$. A natural requirement is:
$$ f(p(x,y), q(x,y)) = f(p(x), q(x)) + \sum_x p(x) f(p(y|x), q(y|x)) $$
This property, along with non-negativity and continuity, essentially characterizes KL divergence up to a constant factor.<Footnote>This relates to the axiomatic foundations of information theory, see works by Shannon, Aczél, Csiszár. <Cite>citation_needed</Cite></Footnote>

<a id="kl-properties-nonnegativity"></a>
### Non-negativity of KL Divergence

KL divergence is always non-negative, $D_{\mathrm{KL}}(p \| q) \ge 0$, with equality holding if and only if $p = q$. This can be proven using Jensen's inequality for the convex function $f(t) = t \log t$ or by using the inequality $\ln x \le x - 1$. See, for example, [this discussion](https://stats.stackexchange.com/questions/335197/why-kl-divergence-is-non-negative) for a proof.

---

<a id="entropy-cross-entropy-relative-entropy"></a>
## Entropy, Cross-Entropy, and Relative Entropy

> **TLDR:** You can split the formula for KL divergence as a difference between cross entropy $H(p,q)$ and entropy $H(p)$. Cross entropy measures how surprised you are on average when sampling from $p$ but using $q$ as your model. If $p=q$, it's called entropy.

It is sometimes helpful to rewrite the formula for KL divergence as a difference of two expressions:
$$ D*{\mathrm{KL}}(p\| q) = \sum_i p_i \log \frac{p_i}{q_i} = \sum_i p_i (\log p_i - \log q_i) $$
$$ = \sum_i p_i \log \frac{1}{q_i} - \sum_i p_i \log \frac{1}{p_i} $$
Using the identity $\log x = -\log(1/x)$:
$$ D*{\mathrm{KL}}(p\| q) = \underbrace{\left(-\sum*i p_i \log q_i\right)}*{\text{Cross-Entropy } H(p, q)} - \underbrace{\left(-\sum*i p_i \log p_i\right)}*{\text{Entropy } H(p)} $$

So, $D_{\mathrm{KL}}(p\| q) = H(p, q) - H(p)$.

**Interpretation via Surprise:**

- **Entropy \(H(p)\):** The expected surprise when outcomes are generated by \(p\) and evaluated using the _true_ model \(p\). It's the irreducible average uncertainty/information content. Minimum average coding length required.
- **Cross-Entropy \(H(p, q)\):** The expected surprise when outcomes are generated by \(p\) but evaluated using the _model_ \(q\). Average coding length if using an optimal code for \(q\) to encode data from \(p\).
- **KL Divergence \(D\_{\mathrm{KL}}(p\| q)\):** The _extra_ expected surprise (or extra coding bits) incurred by using model \(q\) instead of the true model \(p\).

Since \(D\_{\mathrm{KL}}(p\| q) \ge 0\), we always have \(H(p, q) \ge H(p)\). The cross-entropy is minimized (and equals the entropy) when the model \(q\) matches reality \(p\).

<a id="additivity"></a>
### Additivity

Just like KL divergence, entropy and cross-entropy are additive for independent distributions. If $Z=(X,Y)$ where X and Y are independent, then $H(Z) = H(X) + H(Y)$ and $H(p_Z, q_Z) = H(p_X, q_X) + H(p_Y, q_Y)$.

<a id="application-predictions"></a>
### Application: Scoring Predictions

Let's revisit the riddle about scoring experts. Let $p_i$ be the true probability of event $i$ occurring (we assume this exists for now<Footnote>Philosophically, the existence of "true" objective probabilities for single future events is debatable. Often, we evaluate based on the observed _outcomes_ (0 or 1) rather than an unknown true p.</Footnote>) and $q_i$ be the expert's prediction. A good scoring rule should incentivize the expert to report their true belief.

The **Logarithmic Scoring Rule** (Log Score) assigns a score based on the probability the expert assigned to the outcome _that actually occurred_.

- If event $i$ occurs (outcome $y_i=1$), score is $\log q_i$.
- If event $i$ does not occur (outcome $y_i=0$), score is $\log (1-q_i)$.

The goal is usually to _maximize_ the score (or minimize the negative score/loss). Let's consider the _expected_ score under the true distribution $p$:
$$ \mathbb{E}[\text{Score}] = \sum_i \left[ p_i \log q_i + (1-p_i) \log(1-q_i) \right] $$
Maximizing this is equivalent to minimizing its negative:
$$ -\sum_i \left[ p_i \log q_i + (1-p_i) \log(1-q_i) \right] $$
This expression is precisely the **Cross-Entropy** between the true Bernoulli distributions (parameter $p_i$) and the expert's predictions (parameter $q_i$), summed over all events. Let $p^{(i)}$ denote the true Bernoulli dist for event $i$ and $q^{(i)}$ the expert's prediction. We are minimizing $\sum_i H(p^{(i)}, q^{(i)})$.

Since $H(p, q) = H(p) + D_{\mathrm{KL}}(p\|q)$, and $H(p)$ is fixed by reality, minimizing the cross-entropy $H(p,q)$ is equivalent to minimizing the KL divergence $D_{\mathrm{KL}}(p\|q)$ between the expert's forecast and reality. The best score is achieved when $q=p$.

In practice, we don't know $p$, we only see the outcomes $y_i$. We calculate the expert's _actual_ score (loss) as:
$$ \text{Log Loss} = -\sum_i [ y_i \log q_i + (1-y_i) \log(1-q_i) ] $$
By the law of large numbers, over many independent events, minimizing this empirical log loss approximates minimizing the true cross-entropy / KL divergence. This justifies using cross-entropy loss for training classifiers in machine learning.

<a id="coding-theory-intuitions"></a>
### Coding Theory Intuitions

One powerful intuition comes from data compression <Cite>Shannon1948</Cite>. Imagine encoding messages drawn from a distribution $p$.

- **Shannon's Source Coding Theorem:** The minimum average number of bits/nats required per symbol is the entropy $H(p)$.
- **Optimal Codes (e.g., Huffman, Arithmetic):** Assign shorter codewords to more probable symbols. An ideal code would assign a codeword of length $l_i \approx -\log p_i$ to symbol $i$. The expected length is then $\sum p_i l_i \approx \sum p_i (-\log p_i) = H(p)$.

Now, what if you design an optimal code based on a _wrong_ distribution $q$ (codeword lengths $l'_i \approx -\log q_i$) but the data actually comes from $p$? The average length you achieve is:
$$ \mathbb{E}[\text{Length using q-code}] = \sum*i p_i l'\_i \approx \sum_i p_i (-\log q_i) = H(p, q) $$
The *extra* average length you pay due to using the wrong code is:
$$ H(p, q) - H(p) = D*{\mathrm{KL}}(p \| q) $$
KL divergence quantifies the average coding inefficiency penalty.

_Example:_ Alphabet {a, b, c, d}.
True dist $p = (1/2, 1/4, 1/8, 1/8)$. $H(p) = 1/2(1) + 1/4(2) + 2 \times 1/8(3) = 0.5 + 0.5 + 0.75 = 1.75$ bits (using $\log_2$). Optimal code: a=0, b=10, c=110, d=111. Avg length = 1.75 bits.
Model dist $q = (1/4, 1/4, 1/4, 1/4)$. $H(q) = \log_2 4 = 2$ bits. Optimal code for q: a=00, b=01, c=10, d=11 (length 2 for all).
If we use q's code for p's data: Avg length = $p_a(2) + p_b(2) + p_c(2) + p_d(2) = 1/2(2) + 1/4(2) + 1/8(2) + 1/8(2) = 1 + 0.5 + 0.25 + 0.25 = 2$ bits. This is the cross-entropy $H(p, q)$.
The extra length is $H(p, q) - H(p) = 2 - 1.75 = 0.25$ bits.
Let's check KL: $D_{\mathrm{KL}}(p\|q) = \sum p_i \log_2(p_i/q_i)$
$= 1/2 \log_2(\frac{1/2}{1/4}) + 1/4 \log_2(\frac{1/4}{1/4}) + 1/8 \log_2(\frac{1/8}{1/4}) + 1/8 \log_2(\frac{1/8}{1/4})$
$= 1/2 \log_2(2) + 1/4 \log_2(1) + 1/8 \log_2(1/2) + 1/8 \log_2(1/2)$
$= 1/2(1) + 1/4(0) + 1/8(-1) + 1/8(-1) = 0.5 - 0.125 - 0.125 = 0.25$ bits. Matches!

---

<a id="application-distance-from-independence"></a>
### Application: Distance from Independence (Mutual Information)

Let's solve the earlier riddle: Given a joint distribution $p(x,y)$, how far is it from independence? The independent version would have the joint distribution $q(x,y) = p(x)p(y)$, where $p(x) = \sum_y p(x,y)$ and $p(y) = \sum_x p(x,y)$ are the marginal distributions.

We can measure the "distance" from the actual joint $p(x,y)$ to the hypothetical independent joint $q(x,y) = p(x)p(y)$ using KL divergence:

**Definition: Mutual Information**
$$ I(X; Y) = D*{\mathrm{KL}}( p(x,y) \,\|\, p(x)p(y) ) $$
$$ I(X; Y) = \sum*{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)} $$

**Interpretations:**

- Measures the inefficiency (in nats/bits) of assuming X and Y are independent when they are not.
- Equals 0 if and only if X and Y are independent ($p(x,y) = p(x)p(y)$).
- Quantifies the reduction in uncertainty about X given Y (or vice versa): $I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$.
- Measures the "shared information" between X and Y: $I(X;Y) = H(X) + H(Y) - H(X,Y)$.

Mutual information provides a general way to measure dependence between variables, applicable to categorical data and non-linear relationships where correlation might fail.

_(Maybe add example calculation for a simple 2x2 table)_

---

> **Summary of Part 1:** We've defined key measures: Entropy \(H(p)\) for uncertainty, KL Divergence \(D\_{\mathrm{KL}}(p\|q)\) for directed difference between distributions (interpreted as evidence or coding cost), and Mutual Information \(I(X;Y)\) for statistical dependence. These form the building blocks for the rest of the course.

> **Next:** [Link to 02-part-updating.mdx] Part 2: Updating Beliefs & Learning from Constraints
