# Max Entropy Distributions <a id="max-entropy-distributions"></a>

In the [previous chapter](03-minimizing) we've seen how minimizing KL divergence generalizes two important statistical principles -- maximum likelihood & maximum entropy. 

Let's now discuss maximum entropy a bit more in depth. In particular, we will try to understand how do maximum entropy distributions look like. For such a general question, it turns out the answer is surprisingly simple!


## General form of maximum entropy distributions

The question we would like to understand is: If we consider only a set of distributions $P$ that have some properties, how does the maximum entropy distribution in $P$ look like? 

Let's see some examples first. If we consider distributions with $E[X] = \mu$ (and the domain is nonnegative real numbers), the maximum entropy distribution is the exponential distribution. This is the distribution with <Math math = "p(x) \propto e^{-\lambda x}" />. Here, $\lambda$ is some parameter -- fixing $E[X]$ to have different values leads to different $\lambda$s. <Footnote>In this particular case, the precise shape of the distribution is <Math math = "p(x) = \frac{1}{\mu} e^{- x/\mu}" />, but it will help if we don't worry about constants. </Footnote>

Another example: If we fix both $E[X]$ and $E[X^2]$ to be equal to some values, the maximum entropy distribution is the normal distribution where <Math math = "p(x) \propto e^{-(x-\mu)^2/\sigma^2}." />. Perhaps more simply, we can write down its shape as <Math math = "p(x) \propto e^{-\lambda_1 x - \lambda_2 x^2}." /> for some constants $\lambda_1, \lambda_2$. 

Can you spot a pattern? 

Here's the general form of maximum entropy distributions: Let's say that we are given a bunch of constraints $E[f_1(X)] = \alpha_1, \dots, E[f_m(X)] = \alpha_m$. Then, among all the distributions that satisfy the constraints, the distribution $p$ with maximum entropy has the shape: 
<Math displayMode = {true} math = "p(x) \propto e^{\lambda_1 f_1(X) + \dots + \lambda_m f_m(X)}"/>
The constants $\lambda_1, \dots, \lambda_m$ has to be chosen based on the values $\alpha_1, \dots, \alpha_m$. But I advise you not to focus on $\alpha$s and $\lambda$s too much. The point is that the maximum entropy distribution looks like an exponential with the stuff we care about  i.e. the functions $f_1, \dots, f_m$ -- in the exponent. 

## Why? <a id = "intuition"></a>

There's a beautiful proof of this fact using Lagrange multipliers. 
<Footnote>
Here's the proof. We will only consider discrete distributions, let's say those with support $\{1, \dots, K\}$. 
This is an optimization problem for $K$ variables $p_1, \dots, p_K$:

$$
VARIABLES: p_1, \dots, p_K
MAXIMIZE: \sum_{i = 1}^K p_i \log 1/p_i
CONSTRAINTS:
\sum_{i = 1}^K p_i \cdot f_1(i) = \alpha_1
...
\sum_{i = 1}^K p_i \cdot f_m(i) = \alpha_m
\sum_{i = 1}^K p_i = 1
$$

If there were no constraints, the problem would be simple to solve! We would just take the expression to maximize (the entropy) and set all partial derivates by $p_1, \dots, p_K$ to be zero.

Fortunately, there's a technique called [Lagrange multipliers](https://en.wikipedia.org/wiki/Lagrange_multiplier) that will help us here. It involves turning _hard_ constraints (as in our problem) into _soft_ constraints. A soft-constraint variant of our problem would be:

$$
VARIABLES: p_1, \dots, p_K
MAXIMIZE: \sum_{i = 1}^K p_i \log 1/p_i
- \lambda_1 (\sum_{i = 1}^K p_i \cdot f_1(i) - \alpha_1)
- ... -
- \lambda_m (\sum_{i = 1}^K p_i \cdot f_m(i) - \alpha_m)
- \lambda_{m+1} (\sum_{i = 1}^K p_i - 1)
$$

What happens is that instead of requiring that the solution has to exactly satisfy all the constraints, we just say that we have to pay some additional cost for being off the target. The constants $\lambda_1, \dots, \lambda_{m+1}$ are kind of telling us how important each constraint is -- large $\lambda_j$ means that $\sum_{i = 1}^K p_i \cdot f_m(i) - \alpha_m$ should better be small! (In fact, we are not just paying for $\sum_{i = 1}^K p_i \cdot f_m(i) - \alpha_m$ being larger than 0. In this soft constraint formulation, we are even getting paid if it is less than 0! With negative $\lambda_i$, it would be the other way around. )

Long story short, Lagrange tells us that if somebody gives him a solution to our problem satisfying all the $m+1$ _hard_ constraints, then he can find $m+1$ numbers $\lambda_1, \dots, \lambda_{m+1}$ so that the solution also solves the soft-constraints variant. That's neat! We can now implement our plan of solving by smashing it with the power of differentiation. 

More precisely, let's use $\mathcal{L}$ for the soft-constrained objective:
$$
\mathcal{L}(p_1, \dots, p_K) = \sum_{i = 1}^K p_i \log 1/p_i
- \lambda_1 (\sum_{i = 1}^K p_i \cdot f_1(i) - \alpha_1)
- ... -
- \lambda_m (\sum_{i = 1}^K p_i \cdot f_m(i) - \alpha_m)
- \lambda_{m+1} (\sum_{i = 1}^K p_i - 1)
$$

We compute the derivative with respect to some $p_i$. Using the fact that $(p \log 1/p)' = -(p\log p)' = -\log p - 1$, we conclude that


<Math displayMode={true} math = "\frac{\partial \mathcal L}{\partial p_i} = -\log p_i - 1 + \lambda_1 f_1(i) + \dots + \lambda_m f_m(i) + \lambda_{m+1}"/>


That's stellar! If we now set <Math math = "\frac{\partial \mathcal L}{\partial p_i} = 0"/>, we get a formula for $p_i$:
<Math displayMode={true} math = "p_i = e^{ -1 - \lambda_1 f_1(i) - \dots - \lambda_m f_m(i) - \lambda_{m+1}}"/>
I find it more helpful to rewrite like this:
<Math displayMode={true} math = "p_i \propto e^{ - \lambda_1 f_1(i) - \dots - \lambda_m f_m(i)}"/>

I just swept some proportionality constants under the rug: if we remember that probabilities have to sum up to 1, we have arrived at a shockingly simple formula!

We could of course generalize the formula to other discrete sets than $\{1, \dots, K\}$, we would then have:

<Math displayMode={true} math = "p(x) \propto e^{ - \lambda_1 f_1(x) - \dots - \lambda_m f_m(x)}"/>

and the same formula is also the solution if the probability distribution is continuous, except of discrete. The only difference is that $p(x)$ is then technically not probability, but probability density. Also the right buzzword is no longer Lagrange multipliers, but it is its generalization called [calculus of variations](https://en.wikipedia.org/wiki/Calculus_of_variations) (You also have to spend a few more years of your life trying to understand under which conditions it can be used. I am not sure what those are even in the Lagrange multiplier case -- even there we have just found local extrema and saddle points, not necessarily the maximizer, so one has to be careful. )

Now this formula is not the full solution to our problem, because it involves some magical constants $\lambda_1, \dots, \lambda_m$ that we don't know. To find their values, we have to go back to our constraints -- do you still remember that there were $m$ numbers $\alpha_1, \dots, \alpha_m$ there? We can use the conditions $E[f_j(x)] = \alpha_j$ to work out the precise shape of the distribution. 
</Footnote>
But I want to give you a concrete intuition. Remember, we understand from [the previous chapter](03-minimizing) that the max-entropy principle is really about finding a distribution that is close to being uniform, in the sense of minimizing KL divergence. 

So, in which sense are max-entropy distributions close to being uniform? Let's see it on the example of exponential distribution $p(x) \propto e^{-x}$. Let's say that I independently sample two numbers from it $x_1$ and $x_2$. Here's a riddle: Is it more probable that I sample $x_1 = 10, x_2 = 20$ or that I sample $x_1' = 15, x_2' = 15$? 

The answer is that both have the same probability since $p(x_1)\cdot p(x_2) = e^{-x_1 - x_2}$ and we have in our riddle that $x_1 + x_2 = x_1' + x_2'$. In other words, in this density graph, the lines with the same sum $x_1 + x_2$ have uniform density. 

![Exponential distribution](04-max_entropy/exponential.png)

This is not specific only to the exponential distribution, but works for all max-entropy distributions with the shape from ??. In general, consider some distribution of the shape <Math math = "p(x) \propto e^{\lambda_1 f_1(x) + \dots + \lambda_m f_m(x)}" /> and think of independently sampling a few points $x_1, \dots, x_k$ from that distribution. Now if you tell me what are the average values $a_1 = \frac{1}{k} \sum_{i = 1}^k f_1(x_i), \dots, a_m = \frac{1}{k} \sum_{i = 1}^k f_m(x_i)$ and I condition on this information, the conditional distribution over $x_1, \dots, x_k$ is actually uniform! That's because the occurence probability of any such tuple under our distribution $p$ is the same and equal to <Math math = "p(x_1, \dots, x_k) \propto e^{k(\lambda_1 a_1 + \dots + \lambda_m a_m)}$" />.  

So what should we take of it? If you sample from max-entropy distributions, the experience is actually pretty similar to sampling from the uniform distribution! After you measure the empirical averages of $f_1, \dots, f_m$ of your sample, it's distribution is uniform. This is a very concrete sense in which max-entropy distributions are close to the uniform distributions. 
<Footnote>
Even better, using the law of large numbers, if you sample a large sample of $k$ samples, you know that with high probability, the empirical averages $\frac{1}{k} \sum_{i = 1}^k f_j(x_i)$ are going to be close to $E[f_j(X)]$. Thus, sampling from maximum entropy distribution feels like sampling from the uniform distribution over "typical" instances. 
</Footnote>


## Catalogue of examples

It turns out that many distributions you encountered are maximum entropy distributions for some choice of functions $f_1, \dots f_m$. That's not so suprising -- after all, in the case of discrete distributions over e.g. $\{1, \dots, K\}$, literally any distribution is a maximum entropy distribution for certain $K$ constraints.<Footnote>For example, for your favorite distribution $p_1, \dots, p_K$, choose the $j$-th constraint to be $E[\mathbb{1}_j(i)] = p_j$ where $\mathbb{1}_j$ returns 1 for $i$ and 0 otherwise. </Footnote> So, when you encounter a distribution, the right approach is not really to ask the question "Is this a max entropy distribution?". It's more "For what parameters is this a max entropy distribution?" For example, you can forget the formula for Gaussian distribution -- suffices to remember that if you use it, it means that you believe that the mean and the variance are the important parameters to look at. 

Here's a catalogue of some max entropy distributions. 

### No constraints

In case of no constraints, we have $p(x) \propto e^{0}$, i.e., the distribution is uniform. This is the principle of indifference that we [already discussed]() as a special case of the max entropy principle. 

One issue to keep in mind is that the uniform distribution does not always exist -- in particular there's no uniform distribution over real numbers. 

### Fixing $E[X]$

If we believe that the mean is an important parameter of a distribution (as we usually do), the max entropy principle says that the right family of distributions to work with are those with the shape

<Math displayMode={true} math = "p(x) \propto e^{-\lambda x}"/>


This kind of distribution is known under many names, depending on what is the underlying ground set over which we define it. Let's see some cases:

#### Bernoulli distribution & Logits

If we are talking about distributions over $\{0, 1\}$, then the shape from ?? takes form

<Math displayMode={true} math = "p(0) = \frac{1}{1 + e^{- \lambda}}, p(1) = \frac{e^{-\lambda x}}{1 + e^{- \lambda}}"/>

Remember that $\lambda$ is some kind of parameter that defines a family of distributions. This family of distributions is simply all the Bernoulli distributions, i.e., all distributions of type $\{p, 1-p\}$, i.e., literally all possible distributions one can define over the set $\{0, 1\}$. 

You can probably appreciate the usefulness of Bernoulli distributions even without reading this KL divergence pean, so what to take of it? 

What we can still learn from ?? is how to convert numbers into probabilities -- The equation says that (after choosing some $\lambda$) there is a single most natural way of converting real numbers into probabilities -- exponentiating + normalizing them. 

The function $\sigma(x) = 1/(1 + e^{-x})$ is called the logistic function<Footnote>Or maybe a sigmoid function. Apparently, [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) is a bit more general term. </Footnote> and the parameter $x$ is typically called a logit. So, ?? is telling us that if you want to talk about probabilities, but you want to talk about general real numbers instead of numbers from $[0,1]$, you should talk about logits and use the logistic transformation to convert them to probabilities. The choice of $\lambda$ then corresponds to choosing your favorite base of the logarithm function. 

Example:
Remember the Bayesian hero from the [first chapter](01-kl_intro)? It was easier for us to talk about her using the language of odds and their logarithms -- which is pretty much the logits. 

Example: 
The Elo system is the system used to assign ratings to e.g. chess players. The basic idea of it is that when you have two players with Elos $E_1$ and $E_2$, you can estimate the probability that the first one wins from the difference $D = E_1 - E_2$. As we know, max-entropy principle suggests that converting numbers to probabilities should be done via the logistic function. 

This is indeed how Elo system was designed; the interpretation of chess Elo is that the probability that the first player<Footnote>Due to the existence of draws, it's actually the expected score of the first player</Footnote> wins should be <Math math = "\frac{1}{1 + 10^{D / 400}}" />. This is the logistic function with $\lambda = 1/400 \cdot \ln 10$. 

Notice that without knowing the value of $\lambda$, the information "the Elo difference between these two players is 200" is meaningless -- we don't know how Elo is scaled. Yet, even without knowing $\lambda$, we know a lot: if the chance of an upset (i.e., the lower-rated player wins) is $p$ for $D = 200$, then for $D=400$ it's roughly $p^2$, for $D=600$ it's roughly $p^3$ and so on. 

<Footnote>
We will see an application of logits in a minute, and also later in logistic regression, but let me mention another (in)famous application -- the neural net sigmoid [activation function](https://en.wikipedia.org/wiki/Activation_function). In the beginnings of the field, the logistic function was used to convert aggregated inputs to a neuron to its output. We can now give a probabilistic rationale to this operation -- this way of handling neurons corresponds to modelling them as "each neuron aggregates its inputs and then it decides whether to fire or not". 

It is possible that this is a good way of understanding how our brain works, but during the development of the field, researchers understood that the nets work better if instead if sigmoids, one uses the ReLU activation function. That activation function corresponds to neurons that not only contemplate whether to send any signal or none, but also send out a stronger output signal if they got strong input. 
</Footnote>

[TODO logit widget]

#### Categorical distribution & Softmax <a id = "softmax"></a>

A more general instantiation of ?? is when our distribution is over some arbitrary set of numbers $\{a_1, \dots, a_k\}$ -- these are also called categorical distributions. In that case, the max entropy distribution is what's called a softmax (or softmin) distribution -- each number $a_i$ is chosen with probability 

<Math displayMode={true} math = "p_i \propto e^{\lambda a_i}. "/>


You can think about the softmax function as the generalization of the logistic function above -- it's _the right way_ of how to convert numbers to probabilities. In fact, the numbers $a_i$ are often also called logits. 

Example: Typical neural networks output probabilites. For example, a typical neural network classifying images in fact outputs the whole probability distribution that a given picture is a dog/cat/horse/... instead of outputting a single prediction. 
The problem with this is that usually, most of these probabilities are extremely small, e.g. of order of $10^{-10}$. That's because given a picture of a dog, the neural net is typically pretty sure it's not an octopus. 

This makes it awkward to work with the probabilities directly inside the net -- there are all kinds of numerical and stability issues with this. So the neural nets are instead trying to predit logits, and the conversion to probabilities is just the final layer of the network. 


Example: A nice bonus to the previous example is that if you train the network using softmax with $\lambda = 1$, you can change the parameter $\lambda$ after the training. For example, setting $\lambda$ to infinity in ?? is equivalent to just choosing the most probable option -- this way you make the output of the model deterministic which can be useful at deployment. The parameter $1/\lambda$ is analogous to temperature in physics, so this is often described as "running the model at 0 temperature". 

There are also application for higher temperatures. For example, if you want to use LLM to solve a hard math problem, you might want to run it a million times to increase the probability the net solving it. But then you should probably run it at higher temperatures to make sure you want get the same thought process a million times in a row.  

Notice how softmax allowed us to elegantly solve a problem that's so fuzzy that it's otherwise pretty hard to approach -- given a probability distribution $p$, how do we find distributions $p'$ that preserve as much as possible from $p$, yet they are more or less concentrated on the most probable values. 

TODO playing-with-temperature-widget

#### Exponential and Geometrical distribution

If the domain of the probability distribution is nonnegative integers/reals, then we get the so-called [geometric](https://en.wikipedia.org/wiki/Geometric_distribution)/[exponential](https://en.wikipedia.org/wiki/Exponential_distribution) distribution -- another very basic and useful distribution. 
You should think about this distribution as _the most natural_ distribution to model positive numbers. 

Here's an example. Let's say you told me that you measured how long it took you to eat your lunch during the past year and I am to guess how the data looks like. My approach would be to first guess the average time (15 minutes? 30 minutes?) and use the exponential distribution with that mean. 
Only after this first model, I would actually start thinking about whether there's something specific on "eating the lunch" and your personality to add into the model. 

<Footnote>
In practice, my guess would probably not have tail $p(x) \propto e^{-x}$, but a heavier tail, like <Math displayMode={false} math = "p(x) \propto e^{-\sqrt{x}}"/> or even $p(x) \propto 1/x^{C}$. Why? Well, notice how I am not sure about what the actual mean is. So, I should perhaps use a 2-layered model: 1) I sample your average lunch time $\mu$ from some distribution 2) I use exponential distribution with mean $\mu$. 

How should I sample $\mu$ in the first step? Applying max-entropy principle again, perhaps I should sample it from exponential distribution, with mean being how long it takes me / average person to eat their lunch. If you crunch the numbers, the overall distribution that I will use as my input is going to have tails <Math displayMode={false} math = "p(x) \propto e^{-C\sqrt{x}}"/>. That is, the model is getting less sure that I won't see some really large numbers. Using power-law distribution in step 1) would lead to $p(x) \propto 1/x^{C}$. 

This discussion is mostly meant to show what I love on the max entropy principle -- it enables us to rapidly operationalize and concretize our thought processes. Modelling real life is hard, but now we have a powerful tool that automatically builds concrete probabilistic models from vague thoughts like "there's two types of randomness, first in the average lunch time, and second in the variation around that average". 
</Footnote>

There still an elephant in the room though -- exponential distribution can't be normalized if the data are general real numbers. 

### Normal distribution <a id = "normal"></a>

If we believe that both mean and the variance are interesting, max entropy principle suggests that we should choose the function that has a quadratic function of $x$ in the exponent. That means it has the shape
<Math displayMode={true} math = "p(x) \propto e^{-\lambda_1 x^2 - \lambda_2 x}"/>
This gets more familiar if we rewrite the equation like this:
<Math displayMode={true} math = "p(x) = A e^{-B(x-C)^2}"/>
where $A,B,C$ are some constants chosen so that $p(x)$ integrates to $1$. This is the family of [normal (or Gaussian) distributions](https://en.wikipedia.org/wiki/Normal_distribution), and the density formula is usually written more precisely as <Math math = "p(x) = \frac{1}{2\pi\sigma^2} e^{-(x-\mu)^2/\sigma^2}" /> <Footnote>For a finite domain, the probability function from ??could also look like $P(x) \propto e^{x^2}$, i.e., it could have the smallest probabilities in the middle and large probabilities at the edges. However, this is not normalizable to 1 if the domain is real numbers. There, the only possible solution is the familiar bell-shaped curve. </Footnote> 

I think this is a good (partial) explanation of "why is there $x^2$ in Gaussian"? It simply means that Gaussian is the *right* distribution to work with if you care about the first two moments. If you cared about the first four moments, the right family of distributions would be those with four-degree polynomials in the exponent. 

You should view the normal distribution as _the right distribution_ if what you care is mean and variance. A related view is that this is the simplest possible distribution on real numbers -- Neither the uniform distribution nor the exponential distribution can be normalized there, so we can try to salvage this by fixing the first two moments. 

#### Central limit theorem
The normal distribution is extremely important because of the [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem).  Roughly speaking: If you keep adding some random variables $X_1, \dots, X_N$, each with mean $\mu$ and variance $\sigma^2$, then the value of <Math math = "\frac{X_1 + \dots + X_N - N\mu}{\sqrt{N\sigma^2}}"/> is going to be distributed like $N(0,1)$. 

Here's how the max entropy principle can help you think about this theorem: For the theorem, the mean and the variance are clearly important parameters. Thus, max entropy principle kind of suggests that if there is a single distribution that the sums are converging to, the guassian is a natural candidate. 
<Footnote>
In fact, here's how one proof the central limit theorem works: We keep track of the entropy of <Math math = "S_N = \frac{X_1 + \dots + X_N - N\mu}{\sqrt{N\sigma^2}}"/>. We prove that the operation of adding random variables -- called convolution -- has smearing-out properties that increase the entropy of the $S_N$ at least a little bit. This is actually the hard part -- Although it is simple to argue from the definition that $H(X+Y) \ge \max(H(X), H(Y))$ for any two random variables, the proof requires a stronger inequality called [entropy power inequality](https://en.wikipedia.org/wiki/Entropy_power_inequality). As the entropy of $S_N$ keeps converging to the entropy of $N(0,1)$, we argue that the distribution of $S_N$ also has to keep looking more and more similar to the distribution of $N(0,1)$, thus proving the central limit theorem. 
</Footnote>

Example: One of the running examples we used was [measuring of feet](00-introduction/statistics). Why should we model the data using the normal distribution? The typical reasoning is that the length of my foot is a random variable that can probably be modelled as a sum of many small and relatively independent random variables. Thus, using central limit theorem, the normal distribution is a good fit. 

We could also try to approach this modelling problem purely from the max-entropy perspective. Since the data is positive, the simplest model for it is the exponential distribution (see above). But that seems to predict that we will e.g. see many people with feet length twice the average, which does not seem to be a great feature of the model. So, we can do better if we also take into account that the typical deviation of the distribution from the mean is probably quite a bit smaller than the mean. The max entropy distribution if we now consider both the mean and the variance (that represents the scale) is going to be the normal distribution. <Footnote>More precisely, since the domain is _positive_ real numbers, the max entropy distribution is a Gaussian that is clipped to zero for negative numbers and rescaled to sum up to 1. </Footnote> 

### Other Examples

Here are a few more examples you might have encountered. Feel free to skip whatever doesn't ring a bell. 

#### Power law distributions
Sometimes we care about random variables that range over large range of values. For example, the median US city has population about 2400, but there are a bunch of cities with more than million inhabitants (see the picture). 

![US cities](04-max_entropy/cities.png)

These situations are typically modelled by power-law distributions where $p(x) \propto 1/x^C$ for some constant $C$. On above picture, such a distribution would correspond to a straight line so it would fit the city populations pretty well. From the perspective of maximum entropy, these distributions are simply max-entropy distributions when we fix $E[\log X]$. 

That is, whenever you use a power-law distribution, you are implicitly saying "The important thing to look at is not $X$, but $\log X$". In the case of cities for example, it feels more sensible to classify them in log-scale (10^4 -- small, 10^5 -- medium, 10^6 -- large, 10^7 -- megacity) instead of normal scale (10^5 -- small, 2*10^5 -- medium, ... ). Power-law distribution is then just the exponential into which we plug in $\log X$ instead of $X$. 

#### [Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution)
This is a family of distributions on $[0,1]$ that are max-entropy if we fix $E[\log X]$ and $E[\log 1-X]$. You might have encountered beta distribution in Bayesian statistics, where it is used since it behaves very nicely with respect to Bayesian updates. 

But look at our Bayesian hero from [the first chapter](01-kl_intro). She didn't really work with the probabilities of the two hypotheses directly, she instead dealt with the logits $\ell_1, \ell_2$. Caring about logits means that if $X$ represents probability, you care about $\log X$ and $\log (1-X)$. So, maximum entropy principle suggests that using Beta distribution means that we believe that counting logits is natural. 

#### Exponential family

We already discussed that in case of distributions on a finite domain, any distribution is maximum entropy for a clever choice of functions. But with infinite domain (like distribution over real numbers), if you have only finitely many functions $f_1, \dots, f_m$, then only a small proportion of distributions are max-entropy for something. 

Exponential family is a family of distributions that you can define from maximum entropy principle. So, any distribution with finite domain is in exponential family. Any distribution we discussed so far (exponential, geometrical, normal, power-law, beta) are in the exponential family. [Wikipedia](https://en.wikipedia.org/wiki/Exponential_family) contains a long list of distributions in that family, many of which I've never heard about. 
<Footnote>
It would be more precise to say that exponential family are those distributions that minimize $D(p,q)$ from some distribution $q$, given some constraints $E[f_1(X)] = \alpha_1, \dots, E[f_m(X)] = \alpha_m$. In particular, $q$ does not have to be uniform. 

Example: Take the domain $\{0, 1, \dots, n\}$ and consider the constraint $f(X) = X$. We have already seen that this leads to the exponential distribution, given we start with uniform $q$. Now, consider $q$ with <Math math = "q_i = {n \choose i} / 2^n"/> instead. The new solution is the distribution $p$ with <Math math = "p_i \propto {n \choose i} \cdot e^{\lambda i}"/>. This may not look familiar, but if we substitute $\lambda = \log p/(1-p)$, we get the formula for the binomial distribution. Thus, binomial distribution belongs to the exponential family using $f(X) = X$, even though it's not max-entropy distribution for that $f$. 
</Footnote>
The importance of this family is that the distributions from it have a bunch of nice and intuitive properties. 
<Footnote>
Here's one of the nice properties which is actually quite relevant in the next chapter but perhaps too obscure to appreciate on the first read of this. 

Suppose that you have some data with mean $\mu$ and variance $\sigma^2$ and you want to fit it with a distribution. There are two ways to go about this. 

1. Use maximum entropy principle to work out that you want to model it by a normal distribution. Then use maximum likelihood to find the best parameters <Math math = "\hat{\mu}, \hat{\sigma^2}" /> of this general model -- it turns that <Math math = "\hat\mu_{MLE} = \mu, \hat\sigma^2_{MLE} = \sigma^2"/>. 

2. Use maximum entropy principle directly -- the max entropy distribution with mean $\mu$ and variance $\sigma^2$ is $N(\mu, \sigma^2)$. 

Both approaches gave the same answer, but it's not a priori clear that this should be the case! Perhaps MLE could have decided that $N(\mu, \frac{n-1}{n} \sigma^2)$ is a better fit for the data. 

Fortunately, the distributions in the exponential family have the nice property that if you settle the parameters of the distribution by MLE, they turn out to be what they should be. Formally, if your destribution is defined by constraints $E[f_1(X)] = \alpha_1, \dots, E[f_m(X)] = \alpha_m$, then <Math math = "\hat\alpha_{1, MLE} = \alpha_1, \dots, \hat\alpha_{m,MLE} = \alpha_m" />.
</Footnote>


#### Boltzmann distribution

We have many particles in a box, each particle $i$ having some energy $E_i$. What's the distribution of the energies? Maximum entropy principle suggests that the energies are going to be distributed according to the formula <Math math = "p_i \propto e^{-\lambda E_i}" />. This is indeed typically the case.  
<Footnote>
The reason for this is essentially [our intuition](04-max_entropy/intuition): When you have many particles in a box, every now and then two particles fly close to each other and exchange energies. Basically, the particles first have energies $E_1, E_2$, then some interesting physics happens, and then they have energies $E_1', E_2'$. What's important is that $E_1 + E_2 = E_1' + E_2'$; that's conservation of energy. 

Now the meaning of max-entropy distribution is that after we single out two particles with total energy $E$, we are guessing that the distribution of possible pairs $E_1, E_2$ is uniform. This seems to be a reasonable guess!

More technical explanation would have to go into Markov chains and use physical law reversibility, see [this](??). 
</Footnote>

#### Solomonoff prior

Given an algorithm, the most basic measure of its complexity is its length. Given a mathematical object, the most basic measure of its complexity is the length of the shortest algorithm that describes that object. This is called Kolmogorov complexity $K(x)$. 

For example, even though $\pi$ has infinitely many digits, its Kolmogorov complexity is small because there's a small Turing machine that keeps printing its digits. 

In some abstract theories (like [Solomonoff induction](https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference) or [Tegmark multiverse](https://en.wikipedia.org/wiki/Multiverse)), we want to have a distribution over all Turing machines (i.e., algorithms) or over all mathematical objects (whatever that means). We then choose the max-entropy distribution, i.e., <Math math = "p(x) \propto e^{-\lambda K(x)}" />. This is called the Solomonoff prior. 

## Application: XKCD joke

After four chapters of developing the theory of KL divergence, we are finally ready to [dissect](https://www.goodreads.com/quotes/440683-explaining-a-joke-is-like-dissecting-a-frog-you-understand) the XKCD joke from [the list of riddles](00-introduction/xkcd). 

![Are the odds in our favor?](00-introduction/countdown.png)

It's all about how to choose a prior for the number on the wall. The most natural prior is simply the uniform one, i.e., before looking at the wall, we think the number is a uniform random number with 14 decimal digits. After observing the right 8 digits, the first 6 are still uniformly random (modulo the fact we also partially observed the first digit). So, the probability that all 6 digits are zero is $1/10^6$ -- we are probably fine. 

Let's try to think of a better model though. We have seen that for distributions over numbers, max-entropy principle suggests that the exponential distribution <Math math = "p(x) \propto e^{-\lambda x}" /> or the power-law distribution $p(x) \propto x^{-C}$ are pretty reasonable. 

In this case, I would argue for choosing the distribution $p(x) \propto 1/x$ because I am very uncertain about the scale in this example. I chose $C = 1$ in the power-law fit because this is the most natural constant -- it corresponds to uniform distribution over the scales -- see the left graph below. <Footnote>Technically speaking, this distribution satisfies <Math math = "\int_{10^1}^{10^2} \frac{1}{x} dx = \int_{10^2}^{10^3} \frac{1}{x} dx = \dots " />. </Footnote> The only problem with $1/x$ is that we can't normalize this distribution if the domain is real numbers, but now the domain is integers between $0$ and $10^{14}$, so it's OK. 


![xkcd posterior](04-max_entropy/xkcd.png)

The graph on the right shows what happens after we condition that the last digits of $x$ are "00002382" and group $x$-values to bins between $10^1, 10^{1.5}, 10^2, \dots$

First of all, almost all the probability is now concentrated on a single outcome -- $x_0 = 2382$. That makes sense: in the prior, it had probability $p(x_0) = 1/x_0 \approx 5 \cdot 10^{-4}$, while the next most likely outcome -- $x_1 = 100002382$ has probability $p(x_1) = 1/x_1 \approx 10^{-8}$. 

You can also notice how the binned distribution still looks uniform for numbers $>10^9$. This again makes sense -- for large numbers, their last few digits are a coin change, learning them does not make much change. The whole point is that max-entropy principle leads us to the prior that puts more weight into small numbers. The upshot is that we are not necessarily fine. 

## Application: Understanding Financial Data <a id="application-understanding-financial-data"></a>

Let's return to our financial mathematics riddle. We observed that the normal distribution is a better fit for more recent data, and Laplace better models more long-term data. Also, Laplace fits Bitcoin better than S&P. 

Laplace distribution has the shape $p(x) \propto e^\{-|x-\mu|}$. We could think of it as the max entropy distribution for $E[|X-\mu|]$, but there's another illuminating way to understand it. The Laplace distribution can be viewed as the result of the following process:<Footnote>I actually don't know any way observing this fact directly. Let me know if you know a way to do that. The fact can be verified algebraically, but it's not so easy. </Footnote>

1. Sample a variance $\sigma^2$ from the exponential distribution
2. Output a sample from $N(\mu, \sigma^2)$

To understand how this helps to explain our financial data, consider the story for price fluctuations:

For the S&P index, which represents an average of many large companies, there is some variance in the daily change mostly because many (somewhat independent) trades happen that day, each making the value of the index slightly higher or lower. This is the setup for the  central limit theorem which predicts that the data is going to look approximately Gaussian.

However, now consider looking at Bitcoin which is more volatile than the S&P. Or consider S&P, but longer time period. Then the above simplified model is perhaps too simple. There are definitely periods in time where not much happens, but also periods with large volatility. <Footnote> For example: US elections, financial crisis, pandemic hits, war starts, Bitcoin-related laws are passed etc.</Footnote>

Perhaps we should try a more advanced two-layered model predicting the shape of our data: Maybe each month, we sample $\sigma^2$ -- the volatility for that month. Let's sample it from the max-entropy distribution, i.e., exponential. Next, the rest of the month, we sample from the Gaussian with that variance. Notice that we are now sampling from the Laplace distribution. 

That is, while Gaussian is saying "I am pretty sure about today's volatility and unsure about the fluctuation around the mean" and Laplace is saying "I am uncertain both about the volatility, and the actual fluctuation of the day." This explains why Gaussian fits near-term data (volatility of the time period stays roughly the same), while more long-term data start looking more like Laplace. 
<Footnote>
I am not sure whether Laplace distribution is a good practical model. As far as I know, one standard way of modelling financial data is the Student-t distribution. I will not bore you with its formula, but let you know that it is the distribution that you get from the similar process as how we arrived at Laplace; but instead of sampling $\sigma^2$ from the exponential distribution, you sample it from the so-called inverse gamma distribution. Gamma distribution is itself max-entropy distribution for fixing $E[X]$ and $E[\log X]$ (and inverse gamma distribution means we measure volatility by $1/\sigma^2$ instead of $\sigma^2$). 

That is, Student-t distribution takes our model a bit further and adds uncertainty about the _scale_ of the volatility. For example, maybe if you take into large enough past, you start seeing some extreme events like world wars/the Great Depression/industrial revolution, so perhaps we should be open to power-law fits instead of exponential ones. 
</Footnote>

I like this example because I think it nicely shows the strength of the max-entropy principle. Notice how maximum entropy allows us to focus only on the important aspects of our model (do we fix volatility or is it itself a random variable?) All the details are filled in automatically by max entropy. Or if we take it from the other side -- when we see a concrete formula, like a fit by Laplace distribution, max entropy enables us to see through it and understand that it's about the uncertainty in the variance. 

## Next Steps <a id="next-steps"></a>

In the next part, we will reach the punchline of this minicourse -- How KL divergence explains the choice of loss functions optimized in machine learning. 