# Bayes, bits & brains

This site is about probability, information theory, and how it helps us to understand machine learning in particular, and building models of the world in general. 

## A few riddles <a id="introduction"></a>

More about the content, prerequisites, and logistics later. I hope you get a feel for what this is about by checking out the following riddles. I hope some of them [nerd-snipe](https://xkcd.com/356/) you! 😉 You will understand all of them at the end of this minicourse. 


<RiddleStatement id="intelligence" openByDefault={true}>
Test your intelligence with the following widget! You will be given a bunch of text snippets cut from Wikipedia at a random place. Your job: predict the next letter! Try at least five snippets and compare your performance with some neural nets (GPT-2 and Llama 4). 

<LetterPredictionWidget/>

Don't feel bad if a machine beats you; they've been studying for this test their entire lives! But why? And why did Claude Shannon - the information theory [GOAT](https://www.merriam-webster.com/dictionary/goat) - make this experiment in the 1940s? 
</RiddleStatement>




<RiddleStatement id="financial-mathematics">

{/* ![Financial Data Distribution](00-riddles/sap_graph.png) */}

Every day, the S&P index price jumps around a bit. <Footnote>If you don't know what the S&P is, just think Bitcoin, Apple stock, or how many euros you can buy with a dollar</Footnote>

I grabbed some historical data, calculated the daily price changes, and normalized them. <Footnote>I.e., I am plotting the so-called log-return <Math math = "\ln P_t/P_{t-1} \approx 1 + \frac{P_t - P_{t-1}}{P_{t-1}}" /> for each consecutive daily prices $P_{t-1}, P_t$. </Footnote> I threw them in a histogram. The x-axis shows how big the change was (positive about half the time, negative the other half) and the y-axis shows how often that size change happens. Before I show you the plot, take a guess—what's it gonna look like? 

<ImageGallery images={[{src: "00-introduction/shapes.png", alt: "shapes"}]} fullWidth={true} />

<MultipleChoiceQuestion
  options={["A", "B", "C"]}
  correctIndices={[1, 2]}
  feedbackType="all-show"
  explanation={<>This is hard to say, try it below yourself. I fitted the data by two curves - Gaussian distribution (looks like the shape in C), and Laplace distribution (looks like the shape in B). 
  <FinancialDistributionWidget showBTC={false} showSAP={true} />
  What's happening here? We will discuss this example in the <a href = "05-max_entropy">chapter about the max-entropy principle</a> which is extremely useful whenever we want to model some kind of data. <br/><br/></>}
/>

</RiddleStatement>





<RiddleStatement id="wikipedia">
Marcus Hutter, an AI researcher, started <a href="http://prize.hutter1.net/">this challenge</a>: Take a 1GB file of Wikipedia text—how small can you compress it? <a href="https://en.wikipedia.org/wiki/ZIP_(file_format)">Zipping it</a> gets you about 300MB. But we can do way better. What's the current record? 

<MultipleChoiceQuestion
  options={["around 1MB", "around 10MB", "around 100MB"]}
  correctIndices={[2]}
  explanation={<>It's around 100MB. But wait—why does an <em>AI</em> researcher care about compression? We'll see the connection with <a href="02-crossentropy#coding">entropy</a>, and explain how large language models compress texts. In the meantime, you can try to compare how various texts be compressed by various algorithms. 
  <CompressionWidget />
</>}
/> 
</RiddleStatement>




<RiddleStatement id="predictions">

It would be great to know the future—or at least know someone who does. So you've gathered some experts and come up with five questions (Q1 to Q5). You ask the experts to give you probabilities for all 5 questions.
A year later, you know what actually happened. Time to find the best expert!  

<ImageGallery images={[{src: "00-introduction/questions.png", alt: "questions"}]} width="75%" />

The question is: who's the best here?

<MultipleChoiceQuestion
  options={["🧑 Expert 1", "👵🏿 Expert 2", "👶 Expert 3"]}
  correctIndices={[0, 1, 2]}
  feedbackType="all-show"
  explanation={<><p>This is actually hard to tell with just 5 questions, we would need many more of them. Expert 🧑 looks pretty impressive, but I am worried about his failed 99%-confidence prediction! Later, we'll derive the log-score that forecasting tournaments use; this score penalizes failed 99%-confidence predictions quite heavily and we'll get some intuition for why. </p>
  <p>  In the following widget, you can confirm that if Expert 🧑 gave only 80% or 90% probabilities instead of 99%, he would be doing great!</p> 
  <ExpertRatingWidget
    title="Comparing expert predictions"
    showBrierScore={false}
  />  </> }
  />
</RiddleStatement>






<RiddleStatement id="statistics">

<ImageGallery images={[{src: "00-introduction/rod.png", alt: "rod"}]} fullWidth={true} caption="Determining the length of the foot in [1522 Frankfurt](https://commons.wikimedia.org/wiki/File:Determination_of_the_rute_and_the_feet_in_Frankfurt.png)." />

Back in the day, people measured lengths in feet. Sure, we all kinda know how long a foot is, but eventually you need to nail it down precisely. One way is to grab 16 random people and average their foot lengths:
<Math displayMode={true} math="\hat{\mu} = \frac{1}{16} (X_1 + \dots + X_{16})" />

This gives you a fairly stable estimate that shouldn't change much if you do it again. To measure how good your estimate is, you'd usually calculate the standard deviation. Remember the formula? Which one's right:

<MultipleChoiceQuestion
  options={[
    <Math math="\hat{\sigma}^2 = \frac{1}{15} \sum_{i=1}^{16} (X_i - \hat{\mu})^2" />,
    <Math math="\hat{\sigma}^2 = \frac{1}{16} \sum_{i=1}^{16} (X_i - \hat{\mu})^2" />,
    <Math math="\hat{\sigma}^2 = \frac{1}{17} \sum_{i=1}^{16} (X_i - \hat{\mu})^2" />
  ]}
  correctIndices={[0, 1, 2]}
  feedbackType="all-show"
  explanation={<>In a sense, they're all correct! Well, more like they're all defensible options from the viewpoint of frequentist statistics. Using <Math math="1/(n-1)" /> gives you the <a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">unbiased estimate</a>, <Math math="1/n" /> gives the <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood estimate</a>, and <Math math="1/(n+1)" /> minimizes <a href="https://en.wikipedia.org/wiki/Mean_squared_error">the mean squared error</a> between your guess and the truth.<br/><br/>But here's the thing: out of all these ways to estimate stuff, only maximum likelihood became the rockstar of machine learning. Using KL divergence, <a href="04-minimizing#mle">we'll see</a> why maximum likelihood is so special and what makes it tick.</>}
/>
</RiddleStatement>





{/*
<Expand headline="🧠 LLM training"> <a id="deep-learning"></a>

So you wanna train a large language model (LLM) like GPT/Gemini/Claude. These beasts take text, do some crazy computations, and spit out the next letter (Well, actually the next [token](https://en.wikipedia.org/wiki/Large_language_model#Tokenization), which is like a little chunk of letters).
LLMs don't just guess a single letter—they predict the whole distribution $p$ of what might come next. 

Training LLMs is complicated as hell, but one super important bit is picking the _loss function_. Here's the deal: take some text that shows up everywhere online, like "My name is A". We know what usually comes next—maybe "l" is common (all those Alexes), but "a" not so much. Call this the ground-truth distribution $p$.

Your LLM tries to guess this distribution with its own guess $q$. We need to measure how close $q$ is to $p$ using a loss function, then make that number as small as possible during training. So given distributions $p = \{p_1, \dots, p_k\}$ and $q = \{q_1, \dots, q_k\}$, which loss function should we pick?

<MultipleChoiceQuestion
  options={[
    <><Math math="\mathcal L(p,q) = \sum_{i = 1}^k |p_i - q_i|" /></>,
    <><Math math="\mathcal L(p,q) = \sum_{i = 1}^k (p_i - q_i)^2" /></>,
    <><Math math="\mathcal L(p,q) = \sum_{i = 1}^k p_i \cdot \log (p_i / q_i)" /></>
  ]}
  correctIndices={[2]}
  explanation={<>The first two work okay, but practioners typically use KL divergence. Here's why it's nice: if <Math math="p_i = 0.5" /> and <Math math="q_i = 0.49" />, KL thinks that's basically fine. But if <Math math="p_i = 0.01" /> and <Math math="q_i = 0.0" />, KL freaks out (the ratio <Math math="p_i/q_i" /> is infinite). On the other hand, the other two loss functions think both situations are similar problem. <br/><br/>Check out this widget with two distributions and all three loss functions. <br/><br/><DistributionComparisonWidget title="KL Divergence Explorer" /></>}
/>
</Expand>
*/}


<RiddleStatement id="xkcd">

<a href="https://www.explainxkcd.com/wiki/index.php/1159:_Countdown" target="_blank" rel="noopener noreferrer">
  <ImageGallery images={[{src: "00-introduction/countdown.png", alt: "Are the odds in our favor?"}]} fullWidth={true} />
</a>

So... are the odds in our favor?

<MultipleChoiceQuestion
  options={["Yes", "No", "depends"]}
  correctIndices={[1, 2]}
  explanation={<>
  <p>The guy in the hat seems to be confident that at least one digit behind the picture is not zero. If those digits are uniformly random, it's a safe assumption. However, we will use the maximum entropy principle to justify a different probabilistic model and confirm our gut feeling: Given all the zeros we can see, it's actually very plausible that all other hidden digits are zeros too.</p>

  <p>More on that <a href="05-max_entropy#xkcd">later</a>; We will use the widget below to confirm that there's a huge difference between seeing "00002382" and just "2382".</p>

  <XKCDCountdownWidget />
  </>}
/> 
</RiddleStatement>

{/*
<RiddleStatement id="machine-learning">

When you first dive into machine learning, it looks like total chaos. There's linear regression, and logistic regression, and $k$-means. All of those  look like a bunch of random tricks and optimization problems. 

Then, there are neural networks. Very impressive, but still pretty daunting. Take the standard architecture for image generation called a variational autoencoder. You train it by optimizing this absolute monster of a function: 

<Math displayMode={true} math = "\frac{1}{N} \sum_{i = 1}^N \left( \sum_y p'(y | x) \frac{\| X_i - \textrm{Dec}(y)\|^2}{2d} \,+\, \left( \frac12 \sum_{j = 1}^d \textrm{Enc}_{\mu, j}(X_i)^2 + \textrm{Enc}_{\sigma^2, j}(X_i) - \log \textrm{Enc}_{\sigma^2, j}(X_i) \right)\right)"/>

Where the hell are all these, so-called "loss functions", coming from? [We'll see](07-machine_learning) how KL divergence & friends make sense of this mess—and tons of other standard ML algorithms too, like those in the widget below. 

<MLProblemExplorerSimple defaultMode="logisticRegression" />

</RiddleStatement>
*/}

### Onboarding

As we go through the mini-course, we'll revisit each puzzle and understand what's going on. But more importantly, we will understand some important pieces of mathematics and get solid theoretical background behind machine learning. 

Here are some questions we will explore. 

- What's KL divergence, entropy and cross-entropy? What's the intuition behind them? (chapters 1-3)
- Where do the machine-learning principles of maximum likelihood & maximum entropy come from? (chapters 4-5)
- Why do we use logits, softmax, and Gaussian all the time? (chapter 5)
- How to set up loss functions? (chapter 6)
- How compression works and what intuitions it gives about LLMs? (chapter 7)

<Expand headline = "Prerequisites"> <a id="what-we-assume"></a>

I assume you have the probability knowledge after taking a typical introductory course at university.  You should be familiar with the basic language of probability theory: probabilities, distributions, random variables, independence, expectations, variance, and standard deviation. 

[Bayes' rule](https://www.lesswrong.com/w/bayes-rule) is going to be especially important.<Footnote>I love reading [Yudkowsky's](https://www.lesswrong.com/w/bayes-rule) [explanations](https://www.lesswrong.com/w/test-2) of The Rule. </Footnote>
I also assume that you get the gist of [the law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers) <Footnote>If you keep flipping a fair coin $n$ times, you get around $n/2$ heads. Or, more generally & technically, if you keep flipping a biased coin with bias $p$ and $X_i \in \{0,1\}$ is whether the $i$-th outcome is Heads, the sample bias $\hat{p} = 1/n \cdot \sum_{i = 1}^n X_i$ is likely to be very close to the true bias $p$. </Footnote> and maybe even the [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem). <Footnote>If you keep flipping a coin, the number of heads you'll see follows a distribution that looks a bit like a [devoured elephant](https://www.amazon.com/KIUB-Postcard-Little-elephant-10x15cm/dp/B0CKSVVWDL). More generally & technically, if $X_1, \dots, X_n$ follow the same (reasonable) distribution, and $n$ is large enough, the distribution of the sample mean looks like the [Gaussian distribution](https://en.wikipedia.org/wiki/Normal_distribution), with standard deviation of the order of $1/\sqrt{n}$. </Footnote>

Knowing example uses of machine learning, statistics, or information theory helps a good bit to appreciate the context.
</Expand>

<Expand headline = "How to read this">

Skip stuff you find boring, especially expanding boxes and doubly especially boxes labeled with ⚠️.

<Expand advanced = {true} headline = "Advanced block">
You were supposed to skip this one. 😏
</Expand>

Follow links, get nerdsniped, and don't feel the need to read this linearly. 

This mini-course does not contain many formal theorem statements or proofs since the aim is to convey intuition in an accessible way. The downside is that some discussions are necessarily a bit imprecise. To get to the bottom of the topic, check [resources](resources) or just copy-paste the chapter to your favorite LLM and ask for details. 

The total length of the text, if you skip most expand boxes and footnotes, is about 2 chapters of Harry Potter and the Philosopher's stone (until Harry's first experience with magic). It's about 5 chapters if you read all footnotes, bonus, and advanced expand boxes (Harry learns he's a wizard). In either case, I don't promise you will feel the same as Harry.  
</Expand>


## What's next?

This is your last chance. You can go on with your life and believe whatever you want to believe about KL divergence. Or you go to the [first chapter](/01-kl_intro) and see how far the rabbit-hole goes.

<DualActionImage src="fig/pills.png" alt="pills" width="75%" />

{/* Hidden visit tracker */}
<a href="https://clustrmaps.com/site/1c74m" title="ClustrMaps" style={{ display: "none" }}>
  <img src="//www.clustrmaps.com/map_v2.png?d=L0z8whUoI15EX4qt9onNpnKGgpyOGpWCXs1xUryszUM&cl=ffffff" alt="Visitor map" style={{ display: "none" }} />
</a>
