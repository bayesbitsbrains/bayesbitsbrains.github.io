
<Block headline = "If there is one thing you remember from this chapter...">
Multiplicative weights update is a useful algorithmic trick based on treating probabilities multiplicatively. 
</Block>

Application: what the hell is randomness? and pseudorandomness (vrátit se k laplacovi -- proč budoval pravděpodobnost?)

ukázka stringů -- náhodnost = nejde komprimovat

příklad relativní složitosti (ústava US má velkou složitost, ale malou relativní. Nebo, můžu dělat kompresi textu pomocí LLM, pak je to relativní vůči vahám (dát příklad perplexity velkých modelů))

Application: pseudorandomness "takže všechny výsledky jsou vlastně redukce -- kdyby na spoustě dat to nesedělo, tak umíme zkomprimovat pseudonáhodné bity)

takhle ale koukám filosoficky na náhodu -- pokud

Def kolmogorov complexity

relative = KL

prove equivalent to entropy

occam razor -> solomonoff induction

aplikace kolmogorov - kdykoliv používám entropii, nejprv používám kolmogorov

universal probability

kolmogorov/Solomonoff fun? Pascal stuff? 

random program

(relative) Kolmogorov complexity (import numpy)
(ale taky relativita vůči programovacímu jazyku)


equivalence to (relative) entropy

solomonoff prior, training gpt is search over algorithms. 

Solomonoff is just first order model. In reality, some algorithms are much more natural in neural nets. Put differently, the pi printing C algorithm or turning bikes into cars can't easily be represented / learnt   

Evan Hubinger alignment stuff

hutter: compression = understanding, LLMs cite Aaronson as "biggest scientific surprise"
