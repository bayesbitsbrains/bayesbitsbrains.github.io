# Updating Beliefs <a id="updating-beliefs"></a>

In the previous section, we introduced KL divergence and related concepts as tools for measuring the difference between probability distributions. Now, we'll explore how these concepts connect to the process of updating beliefs in light of new evidence.

## Min KL and the Maximum Entropy Principle <a id="min-kl-and-max-entropy"></a>

Minimizing KL divergence is a powerful guiding principle for finding distributions that model data.

There are two important scenarios to consider:

1. If you start with a good model or empirical data $p$, find a simpler model by solving $\min_q D(p, q)$. A special case of this is the maximum likelihood principle.

2. If you start with a basic reference model $q$, find a better model by solving $\min_p D(p, q)$. A special case of this is the maximum entropy principle.

Let's explore these two approaches in more detail.

### Finding the Best Model for Data <a id="finding-best-model"></a>

In one typical application of KL divergence, we want to use it as a guide towards finding a good model. For example, let's say we have some set of numbers $X_1, \dots, X_n$ and we want to fit them by a Gaussian distribution. What does KL suggest?

Well, we can first think about the so-called empirical distribution over $X_1, \dots, X_n$, let's call it $p$.
We can also think about the space of possible models for the data -- this time it would be the set of all possible $N(\mu, \sigma^2)$ for all $\mu \in \mathbb{R}$ and $\sigma^2 \in \mathbb{R}^+$. Let's call this set $Q$.

How do we find the best $q \in Q$? It seems reasonable to look for the $q$ that minimizes:

$$\arg\min_{q\in Q} D(p, q)$$

This approach is equivalent to what is commonly known as the maximum likelihood principle.

### Finding a Better Model with Constraints <a id="finding-better-model"></a>

Here's a second typical application of KL divergence. Let's say that we start with some basic probabilistic model. Maybe I want a probability distribution for how long it takes me to eat lunch. Let's say that I have a guess that the average length is about 15 minutes. What kind of distribution should I choose to model this?

What does KL suggest?

Let's pick some really broad and simple model $q$, like the uniform distribution between $0$ and $24$ hours.
Then, we can think about the space of all possible distributions with mean equal to 0.25 (hours). Let's call this set $P$. As our best guess $p$ for the true distribution, it seems reasonable to pick $p$ that minimizes:

$$\arg\min_{p \in P} D(p,q)$$

Doing this is closely related to what people call the maximum entropy principle.

## Maximum Likelihood Estimation (MLE) <a id="maximum-likelihood-estimation"></a>

Suppose we are given dataâ€”say, the 16 foot length measurements $X_1, \dots, X_{16}$ from our statistics riddle. Assuming the order of the data is irrelevant, we can represent them as an empirical distribution $p$ that assigns each outcome a probability of $1/16$. Now, suppose we believe that a normal distribution is a good model for these data. How do we choose the best parameters $\mu$ and $\sigma^2$ to match the empirical distribution?

KL divergence offers a natural answer: Choose $\mu$ and $\sigma$ to minimize $KL(p, N(\mu,\sigma^2))$. More generally, if we have a set of candidate distributions $Q$ (e.g., all Gaussians), select the $q\in Q$ that minimizes $KL(p,q)$.

In fact, we can rewrite KL divergence as:

$$KL(p,q) = \sum_{X_i\in\mathcal{X}} \frac{1}{n}\log\frac{1/n}{q(X_i)}$$

Splitting this into entropy and cross-entropy:

$$KL(p,q) = \sum_{X_i\in\mathcal{X}} \frac{1}{n}\log\frac{1}{q(X_i)} - \sum_{X_i\in\mathcal{X}} \frac{1}{n}\log\frac{1}{n}$$

Note that the entropy term (the second sum) is $\sum \frac{1}{n}\log n = \log n$, which is constant with respect to $q$.<Footnote>It could happen that we measured the same length twice. Then, $\mathcal{X}$ has to be regarded as a multiset and the corresponding frequencies are not necessarily $1/n$. In that case, the entropy happens to be smaller than $\log n$. But it doesn't matter, the only important fact is that this term is constant, independent of $q$.</Footnote> Thus, minimizing KL divergence is equivalent to minimizing the cross-entropy:

$$\sum_{X_i\in\mathcal{X}} \frac{1}{n}\log\frac{1}{q(X_i)}$$

Here, $q(X_i)$ is the probability (or probability density) that the model assigns to the data point $X_i$. Dropping the constant factor $1/n$ and using the identity $\log(1/x)=-\log x$, this is equivalent to maximizing the product:

$$\prod_{X_i} q(X_i)$$

This expression has very clear meaning: it is the conditional probability of seeing the data $X_i$, provided that they have been sampled independently from the model distribution $q$. This kind of conditional probability is also called the "likelihood of $X$ under $q$".

So, we conclude that we should go for $q$ that maximizes the likelihood of our data, which is exactly what's called the maximum likelihood estimation (MLE) approach.

### Application to the Statistics Riddle <a id="application-to-statistics-riddle"></a>

Let's return to our statistics riddle. When estimating the variance of a distribution from samples, we had three candidate formulas:

$$\sigma^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2$$
$$\sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (X_i - \bar{X})^2$$
$$\sigma^2 = \frac{1}{n+1} \sum_{i=1}^{n} (X_i - \bar{X})^2$$

Using the MLE approach derived above, we would choose $\frac{1}{n}$ as our coefficient, yielding the maximum likelihood estimate. This is one valid approach, though as mentioned, the unbiased estimate uses $\frac{1}{n-1}$ and the minimum mean squared error uses $\frac{1}{n+1}$. Each of these options is defensible depending on your criteria.

## The Maximum Entropy Principle <a id="maximum-entropy-principle"></a>

Let's recall the second example. I want to have a distribution modeling how long it takes me to eat a lunch. Maybe it's rounded to minutes and it never took more than 10 hours, so let's say the domain of my distribution is $\{1, \dots, 600\}$.

As a first, probably very basic model, we can just take the uniform distribution as a prior guess $q_{unif}$. That is, we assign $1/600$ probability to each option. This principle of using uniform distribution when we don't know anything makes sense since uniform distribution is literally the only distribution that gives the same weight to each outcome. (The fancy name for this is the principle of indifference.)

Now let's say that we know a bit more, maybe that we want a distribution with average 15 minutes. In that case, for $P$ being the set of all distributions with average 15, KL divergence suggests finding $p \in P$ minimizing:

$$KL(p,q_\text{uniform})$$

Using our splitting of KL divergence:

$$KL(p,q_\text{uniform}) = \sum_i p_i\log p_i - \sum_i p_i\log (1/600)$$

Since the second term is a constant $\log (1/600) = \log(600)$, minimizing KL divergence is equivalent to maximizing the negative of the first term, which is the entropy of $p$:

$$H(p) = -\sum_i p_i\log p_i$$

This is the maximum entropy principle: given a set $P$ of distributions subject to certain constraints, choose the $p\in P$ that maximizes the entropy $H(p)$.

### The Principle of Indifference as a Prior <a id="principle-of-indifference"></a>

The maximum entropy principle can be seen as a natural extension of the principle of indifference. When we have no information, we use a uniform distribution (the distribution with maximum entropy). When we gain some information in the form of constraints (like knowing the mean), we choose the distribution that is as "uniform" as possible while satisfying those constraints.

In other words, the maximum entropy principle tells us to assume as little as possible beyond what we know. It's a formalization of Occam's razor for probability distributions.

## Max Entropy Distributions <a id="max-entropy-distributions"></a>

Maximum entropy principle helps you find the most generic distribution for your data. You just need to specify the constraints you care about, and the principle determines the "least biased" distribution that satisfies those constraints.

The form of the maximum entropy distributions is very simple -- for example, if you work with numbers and care about their mean $E[X] = \mu$ and variance $\sigma^2 = E[(X-\mu)^2]$, the maximum entropy distribution is going to look like $P(X=x)\propto e^{\lambda_1 x + \lambda_2 x^2}$, i.e., it will be the normal distribution.

We can derive this by using the method of Lagrange multipliers. Without going through the full derivation, the key insight is that the maximum entropy distribution subject to constraints always takes the form of an exponential family:

$$p(x) \propto e^{\sum_i \lambda_i f_i(x)}$$

where $f_i(x)$ are the functions whose expectations $E[f_i(X)]$ we're constraining, and $\lambda_i$ are Lagrange multipliers determined by the specific constraint values.

### Examples of Maximum Entropy Distributions <a id="examples-of-max-entropy-distributions"></a>

Here are some examples of maximum entropy distributions given different constraints:

1. **No constraints**: The maximum entropy distribution is the uniform distribution.

2. **Fixed mean $E[X] = \mu$**:

   - On a finite set $\{a, b\}$: Logistic distribution
   - On $\{0, 1, 2, \ldots\}$: Geometric distribution $P(x) \propto e^{-\lambda x}$
   - On $\mathbb{R}^+$: Exponential distribution $P(x) \propto e^{-\lambda x}$

3. **Fixed mean $E[X] = \mu$ and variance $E[(X-\mu)^2] = \sigma^2$**:

   - On $\mathbb{R}$: Normal (Gaussian) distribution $P(x) \propto e^{-(x-\mu)^2/2\sigma^2}$

4. **Fixed $E[\log X]$**:

   - On $\mathbb{R}^+$: Power-law distribution $P(x) \propto x^{-\alpha}$

5. **Fixed $E[X]$ and $E[X^2]$**:
   - On $\mathbb{R}^+$: Gamma distribution

### Application to the Modeling Riddle <a id="application-to-modeling-riddle"></a>

Let's return to our restaurant modeling riddle. We wanted to find a family of distributions that interpolates between random choice (uniform distribution) and always picking the tastiest item.

The maximum entropy principle provides us with a beautiful answer. If we represent the tastiness of each item as $a_i$, then the maximum entropy distribution subject to a constraint on the average tastiness $E[a_X] = c$ is:

$$P(X=i) = \frac{e^{\lambda a_i}}{\sum_j e^{\lambda a_j}}$$

This is known as the softmax distribution. The parameter $\lambda$ controls how strongly we prefer tastier options:

- When $\lambda = 0$, we get the uniform distribution (completely random choice)
- As $\lambda \to \infty$, the probability concentrates on the item with maximum tastiness

This distribution is exactly what we were looking for in our modeling riddle. It naturally emerges from the maximum entropy principle when we constrain the expected tastiness. It's also commonly used in machine learning, particularly for the output layer of neural networks in classification tasks.

## Connecting KL and Bayes' Theorem <a id="connecting-kl-and-bayes-theorem"></a>

One fascinating aspect of KL divergence is its deep connection to Bayesian reasoning. In fact, Bayes' theorem itself can be derived as a special case of the principle of minimizing KL divergence.

When deriving the minimum KL principle, we relied on an analogy with Bayes' theorem, essentially starting with $q$ and conditioning on new information. One can show that Bayesian updating is a special case of the minimum KL principle.

For example, consider a joint distribution (such as the table for weather and commuting). Upon learning that it is sunny, Bayesian updating dictates that we set $P(\text{sunny})=1$ while keeping the conditional distribution $P(\cdot|\text{sunny})$ unchanged (i.e., zeroing out the other rows and normalizing the sunny row).

According to the minimum KL principle, given the original distribution $q$ and the new data (that outcomes outside the sunny row have probability zero), the updated distribution $p$ is the one that minimizes $KL(p,q)$ while satisfying the constraint. In fact, choosing $p=q(\cdot|\text{sunny})$ achieves $KL(p,q)=0$ (the minimal possible value). Thus, the minimum KL principle endorses Bayesian updating.

In some sense, we can even say that the principle of "minimize KL whenever you can" and the principle of "use Bayes' theorem whenever you can" are kind of equivalent.

## Summary <a id="summary"></a>

In this section, we've explored how KL divergence provides a framework for updating beliefs and choosing probability distributions:

1. **Minimizing KL Divergence** - Two key approaches:

   - $\min_q D(p, q)$: Finding the best model for given data (leads to MLE)
   - $\min_p D(p, q)$: Finding the best distribution subject to constraints (leads to MaxEnt)

2. **Maximum Likelihood Estimation** - Choosing parameters that maximize the probability of observed data

   - Applied to our statistics riddle about estimating variance

3. **Maximum Entropy Principle** - Choosing the least biased distribution subject to constraints

   - Applied to our modeling riddle about customer choice
   - Leads to well-known distributions like Gaussian, exponential, and softmax

4. **Connection to Bayes' Theorem** - Bayesian updating as minimizing KL divergence

These principles provide a unified framework for inference and modeling. By understanding the connections between KL divergence, maximum likelihood, and maximum entropy, we gain deeper insights into why certain distributions and methods are commonly used in statistics and machine learning.

## Next Steps <a id="next-steps"></a>

In the next part, we'll explore more concrete modeling applications of these principles. We'll see how they apply to various fields like machine learning, statistical modeling, and information theory.

Continue to [Modeling Reality](/03-modeling).
