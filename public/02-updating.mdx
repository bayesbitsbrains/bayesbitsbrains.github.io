# Part 2: Updating Beliefs & Learning from Constraints

> How do we rationally change our minds when faced with new evidence? If we only know certain average properties of a system, what's the most reasonable probability distribution to assume? This part explores Bayesian updating and the powerful Maximum Entropy Principle, both deeply connected to KL divergence.

---

<a id="min-kl-and-mep-intro"></a>
## The Philosophy: Minimizing KL Divergence

> **TLDR:** Minimizing KL divergence is a really good guide for finding distributions that model data or incorporate constraints.
>
> - If you have empirical data $p$ and want a simpler model $q$ from family $Q$, solve $\min_{q \in Q} D_{\mathrm{KL}}(p\| q)$. (Leads to Max Likelihood - Part 3).
> - If you start with a prior/reference model $q$ (e.g., uniform) and learn constraints defining a set $P$, find the updated model $p$ by solving $\min_{p \in P} D_{\mathrm{KL}}(p\| q)$. (Leads to Max Entropy).

We've seen KL measures the difference or "cost" between a true/reference distribution $p$ and a model $q$. A core theme in information-theoretic modeling is to **choose distributions by minimizing KL divergence** subject to relevant constraints.

There are two main perspectives:

1.  **Given Data/Truth \(p\), Find Best Model \(q\):** We have empirical data (giving $p_{emp}$) or a complex true distribution $p$. We want to approximate it with a simpler model $q$ from some family $Q$ (e.g., Gaussians, exponentials). The goal is find the $q \in Q$ that is _closest_ to $p$. Minimizing $D_{\mathrm{KL}}(p \| q)$ w.r.t $q$ achieves this. As we'll see in Part 3, this is equivalent to Maximum Likelihood Estimation.
2.  **Given Prior Model \(q\), Incorporate Constraints on \(p\):** We start with a prior belief or reference distribution $q$ (often uniform, representing ignorance). We then learn some constraints that the true distribution $p$ must satisfy (e.g., $\mathbb{E}_p[X] = \mu$). These constraints define a set $P$ of possible distributions. We want to find the distribution $p \in P$ that is _closest_ to our prior $q$, incorporating the new information minimally. This involves minimizing $D_{\mathrm{KL}}(p \| q)$ w.r.t $p$ over the set $P$. This leads to the Maximum Entropy Principle.

---

<a id="bayes-theorem"></a>
## Bayes' Theorem as a Foundation

Before diving into MEP, let's recall the fundamental rule for updating beliefs: Bayes' Theorem.

Consider a hypothesis H and evidence E.
$$ P(H|E) = \frac{P(E|H) P(H)}{P(E)} $$
In odds form:
$$ \frac{P(H|E)}{P(\neg H|E)} = \frac{P(E|H)}{P(E|\neg H)} \times \frac{P(H)}{P(\neg H)} $$
$$ \text{Posterior Odds} = \text{Likelihood Ratio} \times \text{Prior Odds} $$
Taking logs:
$$ \log(\text{Posterior Odds}) = \log(\text{Likelihood Ratio}) + \log(\text{Prior Odds}) $$
The log-likelihood ratio, $\log \frac{P(E|H)}{P(E|\neg H)}$, is the _evidence_ provided by E differentiating H from ¬H. As we saw in Part 1, KL divergence is the _expected_ evidence per sample when comparing two distributions.

<a id="bayes-from-min-kl"></a>
### Bayes' Theorem from Minimum KL

Bayesian updating can be viewed as a special case of minimizing KL divergence. Suppose our prior belief is represented by distribution $q(x)$. We then receive information that restricts the possibilities to a subset $S$ (i.e., we learn that $p(x)=0$ for $x \notin S$). What should our posterior distribution $p(x)$ be?

We seek $p(x)$ such that $p(x)=0$ for $x \notin S$, $\int_S p(x) dx = 1$, and $p(x)$ minimizes $D_{\mathrm{KL}}(p \| q)$.
The solution turns out to be the conditional distribution:
$$ p(x) = q(x | x \in S) = \frac{q(x)}{\int_S q(z) dz} \quad \text{for } x \in S $$
This is exactly the standard Bayesian update rule for conditioning on an event S.<Footnote>This principle, known as Minimum Information Update or Minimum Relative Entropy (MRE), provides a powerful generalization of Bayesian conditioning. See <Cite>citation_needed</Cite>.</Footnote>

<a id="more-on-kl-minimization"></a>
### More on KL Minimization and Bayes

_(Placeholder: This section can contain the "postmodern rant" about truth vs models and the connection between minimizing KL and solving problems using Bayes theorem, potentially using the Wallis derivation idea briefly)_

The principle of minimizing KL divergence provides a unifying framework. Both updating beliefs based on new constraints (MEP) and fitting models to data (MLE) can be seen as finding a distribution that is "closest" in the KL sense to some reference, subject to the available information. One can argue that solving problems using Bayes' theorem is fundamentally about minimizing information loss or surprise, which is what minimizing KL achieves.

---

<a id="maximum-entropy-principle"></a>
## The Maximum Entropy Principle (MEP)

> **TLDR:** If you only know certain average properties (constraints) of a distribution, the most unbiased / least presumptive choice is the distribution that maximizes entropy subject to those constraints. It's equivalent to finding the distribution satisfying the constraints that is closest (in KL divergence) to the uniform distribution.

**Puzzle (Restaurant):** When I'm at my favorite restaurant, I tend to order depending on how hungry I am. If I'm very hungry, I might order the giant pasta bowl. If I'm not so hungry, I might go for a salad. Given only my average hunger level, what's the probability distribution over how hungry I'll be on any given visit?

**Principle: Maximum Entropy**

> Given constraints $\mathbb{E}_p[f_i(X)] = c_i$, choose $p(x)$ to maximize $H(p)$ subject to these constraints.

**Justifications:**

1.  **Least Informative:** Maximizes uncertainty given constraints.
2.  **Combinatorics:** Most microstates compatible with macro-constraints correspond to the max-entropy distribution.
3.  **KL Minimization:** Equivalent to $\min_{p \in P} D_{\mathrm{KL}}(p \| q_{\text{uniform}})$, where P is the set of distributions satisfying constraints.

**The Solution Form (Exponential Family):**
The solution has the form:
$$ p(x) = \frac{1}{Z(\lambda)} \exp\left( \sum\_{i=1}^k \lambda_i f_i(x) \right) $$

Where $Z(\lambda) = \sum_x \exp\left( \sum_{i=1}^k \lambda_i f_i(x) \right)$ is the partition function ensuring $p(x)$ sums to 1, and the $\lambda_i$ parameters are chosen to satisfy the given constraints.

![MEP Visualization](/02-updating/mep-visualization.png)

---

<a id="maxent-distributions"></a>
## Maximum Entropy Distributions

> **TLDR:** Applying MEP with different common constraints derives many famous probability distributions. The type of constraint determines the shape of the distribution (often via the exponential family form).

Let's see what distributions arise from MEP with common constraints:

<a id="constraint-none-finite-set"></a>
### Constraint: None (on a finite set {1,...,k})
* \(f_i(x)\) = none.
* Maximize \(H(p)\). Solution: \(p(x) = 1/k\). **Uniform Distribution**.

<a id="constraint-mean-positive-reals"></a>
### Constraint: Fixed Mean \(\mathbb{E}[X] = \mu\) (on \(\mathbb{R}^+\))
* \(f_1(x) = x\). Constraint: \(\int_0^\infty x p(x) dx = \mu\).
* MaxEnt form: \(p(x) \propto \exp(\lambda_1 x)\). To be normalizable on \(\mathbb{R}^+\), we need \(\lambda_1 < 0\). Let \(\lambda = -\lambda_1 > 0\).
* \(p(x) = \lambda e^{-\lambda x}\) for \(x \ge 0\). Parameter \(\lambda = 1/\mu\). **Exponential Distribution**.

<a id="constraint-mean-nonnegative-integers"></a>
### Constraint: Fixed Mean \(\mathbb{E}[X] = \mu\) (on \(\mathbb{N}_0 = \{0, 1, 2, ...\}\))
* \(f_1(k) = k\). Constraint: \(\sum_{k=0}^\infty k p(k) = \mu\).
* MaxEnt form: \(p(k) \propto \exp(\lambda_1 k) = (e^{\lambda_1})^k = r^k\), where \(r = e^{\lambda_1}\). For normalizability, need \(0 < r < 1\).
* \(p(k) = (1-r) r^k\). Parameter \(r = \mu / (1+\mu)\). **Geometric Distribution**.

<a id="constraint-mean-finite-set-scores"></a>
### Constraint: Fixed Mean Score \(\mathbb{E}[a_X] = \bar{a}\) (on finite set \(\{1, ..., n\}\) with scores \(a_1, ..., a_n\))
* \(f_1(i) = a_i\). Constraint: \(\sum p_i a_i = \bar{a}\).
* MaxEnt form: \(p_i \propto \exp(\lambda_1 a_i)\).
* \(p_i = \frac{e^{\lambda a_i}}{\sum_j e^{\lambda a_j}}\). This is the **Softmax** or **Boltzmann Distribution**. Parameter \(\lambda\) (often related to inverse temperature \(1/T\)) is determined by \(\bar{a}\). Solves the Restaurant Riddle.

<a id="constraint-mean-variance-reals"></a>
### Constraint: Fixed Mean \(\mathbb{E}[X] = \mu\) and Variance \(\mathbb{E}[(X-\mu)^2] = \sigma^2\) (on \(\mathbb{R}\))
* Equivalent to fixing \(\mathbb{E}[X] = \mu\) and \(\mathbb{E}[X^2] = \sigma^2 + \mu^2\).
* \(f_1(x) = x\), \(f_2(x) = x^2\).
* MaxEnt form: \(p(x) \propto \exp(\lambda_1 x + \lambda_2 x^2)\). For normalizability, need \(\lambda_2 < 0\).
* Completing the square in the exponent leads to:
* \(p(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\). **Gaussian (Normal) Distribution**.

<a id="constraint-mean-log"></a>
### Constraint: Fixed Mean Logarithm \(\mathbb{E}[\log X] = c\) (on \(\mathbb{R}^+\))
* \(f_1(x) = \log x\).
* MaxEnt form: \(p(x) \propto \exp(\lambda_1 \log x) = x^{\lambda_1}\).
* With appropriate reference measure and range, this leads to **Power Law / Pareto Distributions**.

<a id="constraint-mean-log-x-and-log-1-x"></a>
### Constraint: Fixed \(\mathbb{E}[\log X]\) and \(\mathbb{E}[\log(1-X)]\) (on [0, 1])
* MaxEnt form: \(p(x) \propto \exp(\lambda_1 \log x + \lambda_2 \log(1-x)) = x^{\lambda_1} (1-x)^{\lambda_2}\).
* This is the form of the **Beta Distribution**: \(p(x) \propto x^{\alpha-1} (1-x)^{\beta-1}\). Relevant in Bayesian statistics for modeling probabilities.

<a id="application-financial-data"></a>
### Application: Financial Data Revisited

> Why might Normal distribution fit S&P daily returns (many small independent effects -> Central Limit Theorem -> Gaussian ≈ MaxEnt for mean/variance) while Laplace fits Bitcoin better?

Laplace distribution \(p(x) \propto e^{-\lambda|x-\mu|}\) is MaxEnt if constraint is fixed mean absolute deviation \(\mathbb{E}[|X-\mu|]\).
Alternatively, Laplace arises as a _scale mixture_ of Gaussians: Sample variance \(\sigma^2\) from an Exponential distribution, then sample \(X\) from \(N(\mu, \sigma^2)\).
This matches the story for volatile assets like Bitcoin: the underlying "Gaussian fluctuation" process might have a variance that itself changes significantly over time (modeled by the exponential prior on \(\sigma^2\)). Normal distribution assumes fixed variance.
If S&P data includes high-volatility periods (like crises), it might also look more Laplace-like or Student-t-like (Student-t comes from a different prior on variance, Inverse Gamma, which is MaxEnt for \(\mathbb{E}[\log X]\) and \(\mathbb{E}[1/X]\)).

![Financial data graph showing Normal vs Laplace fits](/02-updating/financial-returns.png)

<a id="independence-as-maxent"></a>
### Independence as MaxEnt

If we want a joint distribution \(p(x,y)\) and only constrain the marginals \(p(x)\) and \(p(y)\), maximizing the joint entropy \(H(X,Y)\) subject to these marginal constraints leads to the product distribution \(p(x,y) = p(x)p(y)\). Assuming independence is the maximum entropy choice when only marginals are known.

---

> **Summary of Part 2:** We explored Bayesian updating as KL minimization and delved into the Maximum Entropy Principle. MEP provides a powerful justification for using common probability distributions, linking them directly to underlying assumptions about known average quantities via the exponential family form.

> **Next:** [Link to 03-modeling.mdx] Part 3: Building & Comparing Models
