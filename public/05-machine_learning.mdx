# Loss functions

At last, the final chapter! We will use our new KL superpowers to solve machine learning! Or, at least, understand an important aspect of it -- [setting up loss functions](00-introduction/machine-learning). 

The essence to take out from this is that given a machine learning problem, there are four stages of understanding it, before we have an algorithm. It's like this:

1. We start with some data and rough idea of which aspects of it are important. 
2. Using maximum entropy principle, we convert this into a probabilistic model. 
3. Using maximum likelihood principle, we convert this into a loss function. 
4. We optimize the loss function. 

This may not make much sense now, so let's see some examples. 

## Example: Estimating mean and variance <a id="application-to-statistics-riddle"></a>

Our [basic statistics riddle](00-introduction/statistics) was like this: We have some numbers $X_1, \dots, X_n$ and we want to estimate their mean and variance. 

We have already discussed this riddle from several angles; now we can combine all of them to this four-stage recipe:

1. What are the important parameters? Seems like the mean and the variance are important. 

2. Let's turn this into a concrete probabilistic model. Maximum entropy principle from [the previous chapter](04-max_entropy) suggests to model the data by being [independent samples](03-minimizing/a-few-observations) from [the normal distribution](04-max_entropy/normal). 

3. Once we have a set of possible models -- all gaussian distribution -- we can pick the best model out of this set using [the maximum likelihood principle](04-minimizing/mle). 
We [did this in an earlier chapter](04-minimizing/mle_for_mean_sigma), the upshot is that we casted the problem of finding $\hat\mu_{MLE}, \hat\sigma^2_{MLE}$ as the problem of minimizing _loss functions_: 

<Math displayMode={true} math = {"\hat\mu_{MLE}, \hat\sigma^2_{MLE} = \argmin_{\mu, \sigma^2} 2N \cdot \log \sigma + \sum_{i = 1}^N \frac{(X_i-\mu)^2}{\sigma^2}"}/>

4. Finally, we forget the details of the full probabilistic model and just focus on the important parameters $\mu, \sigma^2$. We have to decide on an algorithm optimizing the loss function ??. In this case, the optimization problem has a closed-form formula $\hat\mu_{MLE} = \frac{1}{N} \cdot \sum_{i = 1}^N X_i$ and $\hat\sigma^2_{MLE} = \frac{1}{N} \cdot \sum_{i = 1}^N (X_i-\hat\mu_{MLE})^2$, so the algorithm is pretty trivial. 

What I want to stress is how the only assumption we made about the data was at the very beginning, where we said "we have a bunch of numbers, we care about their mean and variance". The rest came automatically from our understanding of KL divergence. In particular, at no point we argued about central limit theorem.


## Linear Regression <a id="linear-regression"></a>

Suppose we are given a list of pairs $(x_1, y_1), \dots, (x_n, y_n)$; we think that the data are roughly linearly dependent, meaning that there are some $a,b$ such that $y_i \approx a\cdot x_i + b$. The task is to find $a,b$. Let's do it in four stages. What we said about the data so far is the first step. 

The second step is to turn this into a concrete probabilistic model. Let's model the data as $y_i$ coming from the distribution $a\cdot x_i + b + \text\{noise\}$. <Footnote>We could also model the distribution from which $x_i$'s are coming, but that's not going to be relevant.</Footnote> 

The noise is generated from a distribution on real numbers -- how do we choose it? The uniform distribution does not normalize on real numbers, so it's not a choice; the same holds for the exponential distribution. The next choice in line is Gaussian $N(\mu, \sigma^2)$, so let's choose that one. This is a bit awkward since we now have two new parameters $\mu, \sigma^2$ in our model, although we don't really care about them; we only care about $a$ and $b$.

But that's ok. First, we can assume that $\mu = 0$, since otherwise we can replace $N(\mu, \sigma^2)$ by $N(0, \sigma^2)$ and $b$ by $b + \mu$ and get the same model of the data. We will get rid of $\sigma$ in a minute. 

Let's now start the third step of our recipe and use the max likelihood principle. We write down the log-likelihood:

$$\log P(\text\{data\} | a, b, \sigma^2) = -\frac\{n\}\{2\}\log(2\pi\sigma^2) - \frac\{1\}\{2\sigma^2\} \sum_\{i=1\}^n (a\cdot x_i + b - y_i)^2$$

This is pretty complicated, but we notice that whatever value $\sigma^2$ we fix, the optimization problem for $a,b$ becomes to minimize:

$$\sum_\{i=1\}^n (a\cdot x_i + b - y_i)^2$$

So, we may forget $\sigma^2$ again<Footnote>Statistical software like R or Python's statsmodels also outputs $\sigma^2$. This can be helpful if you also, given some new point $x_\{n+1\}$, want to predict not just $y_\{n+1\}$, but also some kind of interval in which you expect $y_\{n+1\}$ to lie.</Footnote> and just optimize this expressions which is the classical [least-squares loss function](https://en.wikipedia.org/wiki/Ordinary_least_squares) for linear regression <Footnote>Usually, there are more variables $x^1, \dots, x^k$ that we use to predict $y$. This leads to more complicated formula, but the derivation stays the same. </Footnote>. Notice how the square in the loss function came from our max entropy assumption of Gaussian noise.

The fourth step is to turn the optimization problem from ?? into an algorithm. In this case, the problem is still sufficiently nice so that there is a formula, though this formula contains matrix inversion so in practice it's going to be solved by some kind of gradient descent. 
<Footnote>
Notice how our approach is pretty flexible so that if we have additional information about the data, we can add it to the model.

For example, maybe we think that the parameters $a$ and $b$ are small. Our framework suggests to add that $a,b \sim N(0, w^2)$ to the model, for some number $w$ that we have to supply by either guessing it or using some other machine learning trick like cross-validation. This leads to optimizing $\sum_\{i=1\}^n (a\cdot x_i + b - y_i)^2 + \frac\{1\}\{w^2\}(a^2 + b^2)$, known as [Ridge regression](https://en.wikipedia.org/wiki/Ridge_regression).

As another example, consider the more general case where we have many predictors $x^1, \dots, x^k$, but we believe that only a small fraction, let's say $p$ fraction of them are actually useful. For the rest, the corresponding coefficient $a_i$ is equal to zero. Our framework suggests that we should optimize:

$$\min_\{S \subseteq \\\{1, \dots, k\\\}, |S|=pk\} \sum_\{i=1\}^n \|a\cdot x_i - y_i\|^2$$

The only problem is that optimizing this function is hard (in fact, NP-hard) -- we have to try all possible subsets of nonzero coefficients and fit each one of them which takes exponential time. Some more complicated math shows though, that optimizing a different objective, called Lasso, often solves this complicated problem (see e.g. [10.6 here](https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.html) or [this book](https://link.springer.com/book/10.1007/978-3-642-20192-9)). So, you should think of Lasso as an attempt to solve our probabilistic model.
</Footnote>

### Logistic Regression <a id="logistic-regression"></a>

This time, we get red and blue points in a plane and we want to find the best line separating them. Ideally, we would like all the red points on one side and all the blue ones on the other, but that's not always possible. Let's use our combo of max entropy + max likelihood to approach the general case.

As in the previous case of linear regression, we want to find a line $y = ax + b$; this time, we will make our life a bit simpler and assume the line has to pass through the origin, so $y = ax$ -- that's just to make the math simpler. We will represent the line using the normal vector $\theta = (a, -1)$ orthogonal to that line, to avoid the edge case of a vertical line. It seems that given a point $p = (x_i, y_i)$, the relevant quantity is the dot product $\theta \cdot p = ax_i - y_i$. This quantity is related to the distance of $p$ from the line; the important detail though is that the same line can be represented by $\theta_1 = (1,-1)$ and $\theta_2=(100,-100)$; the dot product is proportional both to the distance of $p$ from the line and the scale of $\theta$.

Now, using the max entropy principle, it makes sense to use the model where, given the line with normal vector $\theta$, the probability that a point $x$ is red or blue is the max entropy distribution if we care about $\theta \cdot x$, which is the logistic distribution with $P(\text\{red\}) = \sigma(\theta\cdot x)$ and $P(\text\{blue\}) = 1 - \sigma(\theta \cdot x)$<Footnote>We should use the general max entropy distribution that has form $\sigma(\lambda\theta\cdot x)$ for a parameter $\lambda$, but we can set it to $1$ since $\theta$ already can have different scales; but this shows that the scale is a relevant factor, even if we started without it.</Footnote>, where $\sigma(z) = \frac\{1\}\{1+e^\{-z\}\}$ is the sigmoid function.

Now when we know the family of distributions we care about, we can use the max likelihood principle to conclude that we should optimize this expression:

$$\sum_i y_i \log \sigma(\theta\cdot x_i) + (1-y_i)\log (1-\sigma(\theta\cdot x_i))$$

where $y_i$ is the indicator ($y_i \in \\\{0,1\\\}$) for whether $x_i$ is red. This is hard to solve exactly, but gradient descent usually works pretty well. At the end, we get to know $\theta$ which hides not only the direction of the line, but also a scale which is larger the more confident we are about the classification. In the end, we can use $\theta$ to not just predict the more likely color of each new point $x_\{n+1\}$, but also our confidence that it has that color.

### K-means Clustering <a id="k-means-clustering"></a>

Another classic machine learning algorithm is k-means clustering. In this problem, we have a set of points $x_1, \dots, x_n$ in a space, and we want to group them into $k$ clusters. The algorithm iteratively assigns points to the nearest cluster center and then updates the centers to be the means of the assigned points.

How does this relate to KL divergence? We can view k-means as minimizing the KL divergence between the empirical distribution of the data and a mixture of $k$ Gaussians with identical spherical covariance matrices. In the limit as the covariance approaches zero, this becomes equivalent to the standard k-means algorithm.

More specifically, if we have a mixture model with $k$ components, each with mean $\mu_j$ and the same covariance matrix $\sigma^2 I$, the likelihood of a data point $x_i$ is:

$$p(x_i) = \sum_\{j=1\}^k \pi_j \mathcal\{N\}(x_i | \mu_j, \sigma^2 I)$$

where $\pi_j$ is the weight of the $j$-th component. As $\sigma^2 \to 0$, the negative log-likelihood (which we want to minimize) becomes proportional to the sum of squared distances from each point to its nearest center:

$$-\log p(x_i) \propto \min_j \|x_i - \mu_j\|^2$$

This is exactly the objective that k-means seeks to minimize. So, k-means can be seen as a special case of maximum likelihood estimation for a specific probabilistic model.

## Four Viewpoints on Machine Learning Models <a id="four-viewpoints"></a>

Given some machine learning model, there are four viewpoints that one can have:

1. **What is important about the data?** - Identifying the key features or aspects of the data that we care about.
2. **What is a good probabilistic model?** - Determining a suitable family of probability distributions based on the identified features.
3. **What loss function should we optimize?** - Deriving an objective function from the probabilistic model.
4. **What algorithm efficiently optimizes this loss?** - Finding efficient ways to minimize or maximize the objective.

The point of our discussion has been explaining how understanding KL divergence brings us from the first column to the second one (max entropy) and then from the second one to the third one (max likelihood). Sometimes, like in the mean estimation example, loss function implies exact simple formulas, so there's not much more we can say. Sometimes, the optimization simply means running gradient descent on the loss. And sometimes, it's a bit more tricky though and understanding KL is helpful.

### Variational Autoencoders <a id="variational-autoencoders"></a>

One of our riddles mentioned the "beastly" loss function of variational autoencoders (VAEs), which are a class of generative models. Let's demystify this loss function using our understanding of KL divergence.

A VAE consists of two parts:

1. An encoder network that maps an input $x$ to a distribution over latent variables $z$
2. A decoder network that maps a latent variable $z$ back to a distribution over possible inputs $x$

The goal is to find a good latent representation that captures the important features of the data while allowing for efficient generation of new samples.

The loss function for a VAE can be written as:

$$\mathcal\{L\}(\theta, \phi; x) = -\mathbb\{E\}_\{z \sim q_\phi(z|x)\}[\log p_\theta(x|z)] + D_\{KL\}(q_\phi(z|x) \| p(z))$$

Let's break this down:

- The first term is the reconstruction loss, measuring how well we can reconstruct $x$ after encoding it to $z$ and then decoding it back.
- The second term is the KL divergence between the encoder's distribution $q_\phi(z|x)$ and a prior distribution $p(z)$ (usually a standard normal distribution).

This loss function can be derived from the principle of maximizing the evidence lower bound (ELBO) on the log-likelihood of the data. The ELBO is:

$$\log p_\theta(x) \geq \mathbb\{E\}_\{z \sim q_\phi(z|x)\}[\log p_\theta(x|z)] - D_\{KL\}(q_\phi(z|x) \| p(z))$$

Maximizing this lower bound is equivalent to minimizing the negative of the right-hand side, which gives us the VAE loss function.

What's interesting here is that the KL term acts as a regularizer, encouraging the encoder's distribution to be similar to the prior. This enables sampling from the model by simply sampling from the prior and passing it through the decoder. It also ensures that the latent space has nice properties like continuity and completeness.

The "beastly" appearance of this loss function is simply due to the combination of reconstruction and regularization terms, both of which have clear interpretations in terms of our information-theoretic toolkit.

## What's next

Deriving the loss function for variational autoencoder was meant to be the culmination of this minicourse. We have used all the intuition, understanding, and technology developed thus far!

This means you are finished reading the main part, congrats! If you want to continue, we cover a few more bits and pieces next. You can also check out [Resources](/resources), the [About](/about) page.
