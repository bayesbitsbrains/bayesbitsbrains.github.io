### Application to the Statistics Riddle <a id="application-to-statistics-riddle"></a>

Let's return to our statistics riddle. When estimating the variance of a distribution from samples, we had three candidate formulas:

$$\sigma^2 = \frac\{1\}\{n-1\} \sum_\{i=1\}^\{n\} (X_i - \bar\{X\})^2$$
$$\sigma^2 = \frac\{1\}\{n\} \sum_\{i=1\}^\{n\} (X_i - \bar\{X\})^2$$
$$\sigma^2 = \frac\{1\}\{n+1\} \sum_\{i=1\}^\{n\} (X_i - \bar\{X\})^2$$

Using the MLE approach derived above, we would choose $\frac\{1\}\{n\}$ as our coefficient, yielding the maximum likelihood estimate. This is one valid approach, though as mentioned, the unbiased estimate uses $\frac\{1\}\{n-1\}$ and the minimum mean squared error uses $\frac\{1\}\{n+1\}$. Each of these options is defensible depending on your criteria.


todo what a strange coincidence! (definovat expo family jako rodinu "vsech" max entropy distributc√≠)


## Application: measuring feet

We can now understand [our statistical riddle](00-introduction/statistics). Remember, if we measure $N$ lengths $X_1, \dots, X_N$, we were asking about the formula estimating the variance of the underlying data. 

The combo of maximum entropy & maximum likelihood offer us a way:

1. We can use maximu






# Modeling Reality <a id="modeling-reality"></a>

In the previous sections, we introduced KL divergence and showed how it relates to belief updating, maximum likelihood estimation, and the maximum entropy principle. Now, we'll explore how these concepts apply to practical modeling problems, particularly in machine learning.

## Good Old Machine Learning <a id="good-old-machine-learning"></a>

In the previous sections, we've seen how, when we have a distribution and want a different one, it's really useful to find it by minimizing KL divergence. There were two important cases of that:

1. When we have a distribution $p$ and want to find a less precise $q$ from some family $Q$ that approximates it well, we choose it by minimizing $\min_\{q \in Q\} D(p,q)$. A special case of this is the maximum likelihood estimation (see previous section) where we derived that if we measured some data $D$, that we want to fit by some distribution $q \in Q$, we should choose $q$ that maximizes the likelihood $P_q(D)$.

2. When we have a model distribution $q$ and learn some new information about the truth, like that the true distribution is from some family $P$, we should choose our new model by minimizing $\min_\{p \in P\} D(p,q)$. A special case of this is the maximum entropy principle where we started with the most general -- uniform -- $q$ and $P$ corresponded to learning the value of some parameter of the distribution, like its mean or variance.

Combining these two principles, we can derive many basic approaches in statistics and classical machine learning. Let's see some examples.

### Mean and Variance Estimation <a id="mean-and-variance-estimation"></a>

Suppose we are given a list of numbers $\\\{X_1, \dots, X_n\\\}$. We care about their mean and variance. Let's use the combo of max entropy + max likelihood to derive the formulas for that.

First, the maximum entropy principle suggests that once we fix the mean of a distribution to be $\mu$ and variance to be $\sigma^2$, and consider all the distributions with that mean and that variance, there is a single distribution that stands out -- the maximum entropy distribution after fixing the mean and variance -- the normal distribution.<Footnote>The principle also suggests an even more basic thing -- to assume that the numbers have been drawn independently from some distribution, which allows us to represent the sequence $X_1, \dots, X_n$ as a set, or equivalently an empirical distribution.</Footnote>

Now that we have fixed a set of relevant distributions $Q$, we will find a particular $q \in Q$ approximating the empirical distribution of $X$s as well as possible. This is done by the maximum likelihood method. Concretely, we should pick $\hat\mu, \hat\sigma^2$ maximizing:

$$\prod_\{i=1\}^n \frac\{1\}\{\sqrt\{2\pi\hat\sigma^2\}\} \cdot e^\{-(X_i - \hat\mu)^2/2\hat\sigma^2\}$$

If we rewrite this as the log-likelihood and crunch the numbers, we derive the formula:

$$\hat\mu = \frac\{1\}\{n\} \sum X_i$$

and

$$\hat\sigma^2 = \frac\{1\}\{n\} \sum (X_i - \hat\mu)^2$$

What I want to stress is how the only assumption we made was at the beginning, where we said "we have a bunch of numbers, we care about their mean and variance", the rest flowed from our understanding of KL divergence. First, we used the max entropy principle to turn the knowledge of the important parameters (mean, variance) into a concrete probabilistic model of our data. Then, maximum likelihood tells us how to turn this model into a concrete loss function to optimize.

In particular, we didn't really argue that Gaussian is the right fit because of some kind of central limit theorem, we instead used it as the max entropy distribution.

### Linear Regression <a id="linear-regression"></a>

Suppose we are given a list of pairs $(x_1, y_1), \dots, (x_n, y_n)$; we think that the data are roughly linearly dependent, meaning that there are some $a,b$ such that $y_i \approx a\cdot x_i + b$. Let's see how we can find $a,b$.

We model the data as $y_i$ coming from the distribution $a\cdot x_i + b + \text\{noise\}$. The noise is generated from a distribution on real numbers. The natural choice is Gaussian $N(\mu, \sigma^2)$, though it's a bit awkward since we now added two parameters $\mu, \sigma^2$ that we don't really care about; we care about $a$ and $b$.

But that's ok. First, we can assume that $\mu = 0$, since otherwise we can replace $N(\mu, \sigma^2)$ by $N(0, \sigma^2)$ and $b$ by $b + \mu$ and get the same, but more reasonable model of the data. Second, if we now use the max likelihood principle and write down the log-likelihood:

$$\log P(\text\{data\} | a, b, \sigma^2) = -\frac\{n\}\{2\}\log(2\pi\sigma^2) - \frac\{1\}\{2\sigma^2\} \sum_\{i=1\}^n (a\cdot x_i + b - y_i)^2$$

we notice that whatever value $\sigma$ we fix<Footnote>Statistical software like R or Python's statsmodels also outputs $\sigma^2$. This can be helpful if you also, given some new point $x_\{n+1\}$, want to predict not just $y_\{n+1\}$, but also some kind of interval in which you expect $y_\{n+1\}$ to lie.</Footnote>, the optimization problem becomes to minimize:

$$\sum_\{i=1\}^n (a\cdot x_i + b - y_i)^2$$

We arrive at the classical linear regression loss function (usually there are more variables $x^1, \dots, x^k$ that we use to predict $y$ which leads to more complicated formulas, but the derivation stays the same). Notice how the square in the loss function came from our max entropy assumption of Gaussian noise.

Notice how this approach is pretty flexible so that if you have additional information, you can add it to the model.

For example, maybe you think that the parameters $a$ and $b$ are small and you want to add this information to the model. Our framework tells you to add that $a,b \sim N(0, w^2)$ to the model, for some number $w$ that you have to supply by either guessing or using some other machine learning trick like cross-validation. This leads to optimizing $\sum_\{i=1\}^n (a\cdot x_i + b - y_i)^2 + \frac\{1\}\{w^2\}(a^2 + b^2)$, known as Ridge regression.

As another example, consider the more general case where you have many predictors $x^1, \dots, x^k$, but you believe that only a small fraction, let's say $p$ fraction of them are actually useful. For the rest, the corresponding coefficient $a_i$ that we try to find is equal to zero. Our framework suggests that you should optimize:

$$\min_\{S \subseteq \\\{1, \dots, k\\\}, |S|=pk\} \sum_\{i=1\}^n \|a\cdot x_i - y_i\|^2$$

The only problem is that optimizing this function is hard, as we would have to essentially try all possible subsets of nonzero coefficients and fit each one of them. Some more complicated math shows though, that optimizing a different objective, called Lasso, often solves this, in general complicated, problem. So, you should think of Lasso as an attempt to solve this problem.

### Logistic Regression <a id="logistic-regression"></a>

This time, we get red and blue points in a plane and we want to find the best line separating them. Ideally, we would like all the red points on one side and all the blue ones on the other, but that's not always possible. Let's use our combo of max entropy + max likelihood to approach the general case.

As in the previous case of linear regression, we want to find a line $y = ax + b$; this time, we will make our life a bit simpler and assume the line has to pass through the origin, so $y = ax$ -- that's just to make the math simpler. We will represent the line using the normal vector $\theta = (a, -1)$ orthogonal to that line, to avoid the edge case of a vertical line. It seems that given a point $p = (x_i, y_i)$, the relevant quantity is the dot product $\theta \cdot p = ax_i - y_i$. This quantity is related to the distance of $p$ from the line; the important detail though is that the same line can be represented by $\theta_1 = (1,-1)$ and $\theta_2=(100,-100)$; the dot product is proportional both to the distance of $p$ from the line and the scale of $\theta$.

Now, using the max entropy principle, it makes sense to use the model where, given the line with normal vector $\theta$, the probability that a point $x$ is red or blue is the max entropy distribution if we care about $\theta \cdot x$, which is the logistic distribution with $P(\text\{red\}) = \sigma(\theta\cdot x)$ and $P(\text\{blue\}) = 1 - \sigma(\theta \cdot x)$<Footnote>We should use the general max entropy distribution that has form $\sigma(\lambda\theta\cdot x)$ for a parameter $\lambda$, but we can set it to $1$ since $\theta$ already can have different scales; but this shows that the scale is a relevant factor, even if we started without it.</Footnote>, where $\sigma(z) = \frac\{1\}\{1+e^\{-z\}\}$ is the sigmoid function.

Now when we know the family of distributions we care about, we can use the max likelihood principle to conclude that we should optimize this expression:

$$\sum_i y_i \log \sigma(\theta\cdot x_i) + (1-y_i)\log (1-\sigma(\theta\cdot x_i))$$

where $y_i$ is the indicator ($y_i \in \\\{0,1\\\}$) for whether $x_i$ is red. This is hard to solve exactly, but gradient descent usually works pretty well. At the end, we get to know $\theta$ which hides not only the direction of the line, but also a scale which is larger the more confident we are about the classification. In the end, we can use $\theta$ to not just predict the more likely color of each new point $x_\{n+1\}$, but also our confidence that it has that color.

### K-means Clustering <a id="k-means-clustering"></a>

Another classic machine learning algorithm is k-means clustering. In this problem, we have a set of points $x_1, \dots, x_n$ in a space, and we want to group them into $k$ clusters. The algorithm iteratively assigns points to the nearest cluster center and then updates the centers to be the means of the assigned points.

How does this relate to KL divergence? We can view k-means as minimizing the KL divergence between the empirical distribution of the data and a mixture of $k$ Gaussians with identical spherical covariance matrices. In the limit as the covariance approaches zero, this becomes equivalent to the standard k-means algorithm.

More specifically, if we have a mixture model with $k$ components, each with mean $\mu_j$ and the same covariance matrix $\sigma^2 I$, the likelihood of a data point $x_i$ is:

$$p(x_i) = \sum_\{j=1\}^k \pi_j \mathcal\{N\}(x_i | \mu_j, \sigma^2 I)$$

where $\pi_j$ is the weight of the $j$-th component. As $\sigma^2 \to 0$, the negative log-likelihood (which we want to minimize) becomes proportional to the sum of squared distances from each point to its nearest center:

$$-\log p(x_i) \propto \min_j \|x_i - \mu_j\|^2$$

This is exactly the objective that k-means seeks to minimize. So, k-means can be seen as a special case of maximum likelihood estimation for a specific probabilistic model.

## Four Viewpoints on Machine Learning Models <a id="four-viewpoints"></a>

Given some machine learning model, there are four viewpoints that one can have:

1. **What is important about the data?** - Identifying the key features or aspects of the data that we care about.
2. **What is a good probabilistic model?** - Determining a suitable family of probability distributions based on the identified features.
3. **What loss function should we optimize?** - Deriving an objective function from the probabilistic model.
4. **What algorithm efficiently optimizes this loss?** - Finding efficient ways to minimize or maximize the objective.

The point of our discussion has been explaining how understanding KL divergence brings us from the first column to the second one (max entropy) and then from the second one to the third one (max likelihood). Sometimes, like in the mean estimation example, loss function implies exact simple formulas, so there's not much more we can say. Sometimes, the optimization simply means running gradient descent on the loss. And sometimes, it's a bit more tricky though and understanding KL is helpful.

### Variational Autoencoders <a id="variational-autoencoders"></a>

One of our riddles mentioned the "beastly" loss function of variational autoencoders (VAEs), which are a class of generative models. Let's demystify this loss function using our understanding of KL divergence.

A VAE consists of two parts:

1. An encoder network that maps an input $x$ to a distribution over latent variables $z$
2. A decoder network that maps a latent variable $z$ back to a distribution over possible inputs $x$

The goal is to find a good latent representation that captures the important features of the data while allowing for efficient generation of new samples.

The loss function for a VAE can be written as:

$$\mathcal\{L\}(\theta, \phi; x) = -\mathbb\{E\}_\{z \sim q_\phi(z|x)\}[\log p_\theta(x|z)] + D_\{KL\}(q_\phi(z|x) \| p(z))$$

Let's break this down:

- The first term is the reconstruction loss, measuring how well we can reconstruct $x$ after encoding it to $z$ and then decoding it back.
- The second term is the KL divergence between the encoder's distribution $q_\phi(z|x)$ and a prior distribution $p(z)$ (usually a standard normal distribution).

This loss function can be derived from the principle of maximizing the evidence lower bound (ELBO) on the log-likelihood of the data. The ELBO is:

$$\log p_\theta(x) \geq \mathbb\{E\}_\{z \sim q_\phi(z|x)\}[\log p_\theta(x|z)] - D_\{KL\}(q_\phi(z|x) \| p(z))$$

Maximizing this lower bound is equivalent to minimizing the negative of the right-hand side, which gives us the VAE loss function.

What's interesting here is that the KL term acts as a regularizer, encouraging the encoder's distribution to be similar to the prior. This enables sampling from the model by simply sampling from the prior and passing it through the decoder. It also ensures that the latent space has nice properties like continuity and completeness.

The "beastly" appearance of this loss function is simply due to the combination of reconstruction and regularization terms, both of which have clear interpretations in terms of our information-theoretic toolkit.

## What's next

Deriving the loss function for variational autoencoder was meant to be the culmination of this minicourse. We have used all the intuition, understanding, and technology developed thus far!

This means you are finished reading the main part, congrats! If you want to continue, we cover a few more bits and pieces next. You can also check out [Resources](/resources), the [About](/about) page.
