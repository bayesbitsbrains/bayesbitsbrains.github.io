# Loss functions

At last, the final chapter! We will use our new KL superpowers to solve machine learning! Or, at least, understand an important aspect of it -- [setting up loss functions](00-introduction/machine-learning). 

The essence to take out from this chapter is that when we start with some data and a rough idea about what aspects of it are important to know, KL divergence is guiding us towards a concrete algorithm estimating the aspects. 

In particular, we can use maximum entropy to turn our rough idea into a fully probabilistic model, and maximum likelihood to turn the model into a loss function to optimize. 

We will go through the examples in the following table (feel free to skip examples you don't find interesting). 

![Examples](05-machine-learning/table)

## Example: Estimating mean and variance <a id="application-to-statistics-riddle"></a>

Our [basic statistics riddle](00-introduction/statistics) was like this: We have some numbers $X_1, \dots, X_n$ and we want to estimate their mean and variance. 

We have already discussed this riddle from several angles; now we can combine them. 

First, we turn the rough idea that mean and variance are important into a concrete probabilistic model. Maximum entropy principle from [the previous chapter](04-max_entropy) suggests to model the data by being [independent samples](03-minimizing/a-few-observations) from [the normal distribution](04-max_entropy/normal). 

3. Once we have a set of possible models -- all gaussian distribution -- we can pick the best model out of this set using [the maximum likelihood principle](04-minimizing/mle)<Footnote>In the equations, it will be easier to maximize the logarithm of the likelihood function, i.e., the log-likelihood. This is perhaps not too suprising since [we understand](03-minimizing/mle) that maximizing log-likelihood is really just minimizing the KL divergence, which is fundamentally what's going on here. </Footnote>. 
We [did this in an earlier chapter](04-minimizing/mle_for_mean_sigma), the upshot is that we casted the problem of finding $\hat\mu_{MLE}, \hat\sigma^2_{MLE}$ as the problem of minimizing _loss functions_: 

<Math displayMode={true} math = {"\hat\mu_{MLE}, \hat\sigma^2_{MLE} = \argmin_{\mu, \sigma^2} 2N \cdot \log \sigma + \sum_{i = 1}^N \frac{(X_i-\mu)^2}{\sigma^2}"}/>

4. Finally, we forget the details of the full probabilistic model and just focus on the important parameters $\mu, \sigma^2$. We have to decide on an algorithm optimizing the loss function ??. In this case, the optimization problem has a closed-form formula $\hat\mu_{MLE} = \frac{1}{N} \cdot \sum_{i = 1}^N X_i$ and $\hat\sigma^2_{MLE} = \frac{1}{N} \cdot \sum_{i = 1}^N (X_i-\hat\mu_{MLE})^2$, so the algorithm is pretty trivial. 

What I want to stress is how the only assumption we made about the data was at the very beginning, where we said "we have a bunch of numbers, we care about their mean and variance". The rest came automatically from our understanding of KL divergence. In particular, at no point we argued about central limit theorem.


## Linear Regression <a id="linear-regression"></a>

Suppose we are given a list of pairs $(x_1, y_1), \dots, (x_n, y_n)$; we think that the data are roughly linearly dependent, meaning that there are some $a,b$ such that $y_i \approx a\cdot x_i + b$. The task is to find $a,b$. Let's do it in four stages. What we said about the data so far is the first step. 

The second step is to turn this into a concrete probabilistic model. Let's model the data as $y_i$ coming from the distribution $a\cdot x_i + b + \text\{noise\}$. <Footnote>We could also model the distribution from which $x_i$'s are coming, but that's not going to be relevant.</Footnote> 

The noise is generated from a distribution on real numbers -- how do we choose it? The uniform distribution does not normalize on real numbers, so it's not a choice; the same holds for the exponential distribution. The next choice in line is Gaussian $N(\mu, \sigma^2)$, so let's choose that one. This is a bit awkward since we now have two new parameters $\mu, \sigma^2$ in our model, although we don't really care about them; we only care about $a$ and $b$.

But that's ok. First, we can assume that $\mu = 0$, since otherwise we can replace $N(\mu, \sigma^2)$ by $N(0, \sigma^2)$ and $b$ by $b + \mu$ and get the same model of the data. We will get rid of $\sigma$ in a minute. 

Let's now start the third step of our recipe and use the max likelihood principle. We write down the log-likelihood:

$$\log P(\text\{data\} | a, b, \sigma^2) = -\frac\{n\}\{2\}\log(2\pi\sigma^2) - \frac\{1\}\{2\sigma^2\} \sum_\{i=1\}^n (a\cdot x_i + b - y_i)^2$$

This is pretty complicated, but we notice that whatever value $\sigma^2$ we fix, the optimization problem for $a,b$ becomes to minimize:

$$\sum_\{i=1\}^n (a\cdot x_i + b - y_i)^2$$

So, we may forget $\sigma^2$ again<Footnote>Statistical software like R or Python's statsmodels also outputs $\sigma^2$. This can be helpful if you also, given some new point $x_\{n+1\}$, want to predict not just $y_\{n+1\}$, but also some kind of interval in which you expect $y_\{n+1\}$ to lie.</Footnote> and just optimize this expressions which is the classical [least-squares loss function](https://en.wikipedia.org/wiki/Ordinary_least_squares) for linear regression <Footnote>Usually, there are more variables $x^1, \dots, x^k$ that we use to predict $y$. This leads to more complicated formula, but the derivation stays the same. </Footnote>. Notice how the square in the loss function came from our max entropy assumption of Gaussian noise.

The fourth step is to turn the optimization problem from ?? into an algorithm. In this case, the problem is still sufficiently nice so that there is a formula, though this formula contains matrix inversion so in practice it's going to be solved by some kind of gradient descent. 

Advanced:

Notice how our approach is pretty flexible so that if we have additional information about the data, we can add it to the model.

For example, maybe we think that the parameters $a$ and $b$ are small. Our framework suggests to add that $a,b \sim N(0, w^2)$ to the model, for some number $w$ that we have to supply by either guessing it or using some other machine learning trick like cross-validation. This leads to optimizing $\sum_\{i=1\}^n (a\cdot x_i + b - y_i)^2 + \frac\{1\}\{w^2\}(a^2 + b^2)$, known as [Ridge regression](https://en.wikipedia.org/wiki/Ridge_regression).

As another example, consider the more general case where we have many predictors $x^1, \dots, x^k$, but we believe that only a small fraction, let's say $p$ fraction of them are actually useful. For the rest, the corresponding coefficient $a_i$ is equal to zero. Our framework suggests that we should optimize:

$$\min_\{S \subseteq \\\{1, \dots, k\\\}, |S|=pk\} \sum_\{i=1\}^n \|a\cdot x_i - y_i\|^2$$

The only problem is that optimizing this function is hard (in fact, NP-hard) -- we have to try all possible subsets of nonzero coefficients and fit each one of them which takes exponential time. Some more complicated math shows though, that optimizing a different objective, called Lasso, often solves this complicated problem<Footnote>See e.g. [10.6 here](https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.html) or [this book](https://link.springer.com/book/10.1007/978-3-642-20192-9)</Footnote>. So, you should think of Lasso as an attempt to solve our probabilistic model.


### Logistic Regression <a id="logistic-regression"></a>

This time, we get red and blue points in a plane and we want to find the best line separating them. Ideally, we would like all the red points on one side and all the blue ones on the other, but that's not always possible. In that case, how do we find the best line? 

As in the previous case of linear regression, we want to find a line $y = ax + b$. This time, we will make our life a bit simpler and assume the line has to pass through the origin, so $y = ax$ -- that's just to make the math simpler. 
It will be more helpful to represent the line using the normal vector $\theta = \frac{1}{a^2+1} (a, -1)$ orthogonal to that line, and the distance $\delta = |b|/\sqrt{a^2+1}$ of the origin from the line. 

Given a point $(x_i, y_i)$, the relevant quantity is the distance of that point from our line. This can be computed using the dot product as $\theta \cdot (x_i, y_i) + \delta$. 

Now, using the max entropy principle, we construct the probabilistic model that converts this distance into probability -- we use the logistic function. That is, our model is
<Math displayMode={true} math = "p(red | (x_i, y_i)) = \sigma\left( \lambda (\theta \cdot (x_i, y_i) + \delta) \right)"/>
where $\sigma$ is the logistic function $\sigma(x) = e^x / (1+e^x)$. Of course, we also have $p(blue | (x_i, y_i)) = 1 - p(red | (x_i, y_i))$. 

The constant $\lambda$ is a new parameter of our model that max-entropy principle suggest we add to it. That is, we will have to optimize three parameters -- $a,b, \lambda$. The parameter $\lambda$ captures our confidence about the classification. This is handy because if a new point comes after we find the best line, we can not only color the point red/blue based on which side of the line it is on, but also use equation ?? to compute how certain we are about our classification. 

Once we have a model, we can use the max likelihood principle to find its parameters. The principle tells us to find $a,b,\lambda$ that minimize the following expression:

<Math displayMode = {true} math = "\sum_{i = 1}^n \ell_i \log \sigma(\lambda (\theta \cdot (x_i,y_i) + \delta)) + (1-\ell_i) \log (1-\sigma(\lambda (\theta \cdot (x_i,y_i) + \delta)))"/>

where $\ell_i$ is the indicator ($\ell_i \in \\\{0,1\\\}$) for whether the point $(x_i,y_i)$ is red. This problem known as the [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) is hard to solve exactly, but gradient descent usually works pretty well. 

### K-means Clustering <a id="k-means-clustering"></a>

We are given a set of points $x_1, \dots, x_n$ on, say, 2D plane. We want to group them into $k$ clusters. 

Let's turn this into a probabilistic model. Let's use $\mu_1, \dots, \mu_k$ to denote centers of the clusters. The meaning of those points is that if a point $x_i$ belongs to the cluster $j$, then the distance $||x_i - \mu_j||_2$ should be small. 

As before, we can use the max entropy distribution to turn this into a concrete model. Exponential function does not normalize, so we will use the normal distribution:

<Math displayMode={true} math = "p(x | \mu_j) \propto e^{-\frac{||x-\mu_j||^2}{\sigma^2}}"/>

The notation $p(x | \mu_j)$ means that this is only our model distribution of points coming from the $j$-th cluster. We want a full probabilistic model $p(x)$ though. We can get it by putting a prior on how likely each cluster is. The maximum entropy prior is the uniform distribution, so we will choose:

<Math displayMode={true} math = "p(x) = \frac{1}{k} \sum_{i = 1}^k p(x | \mu_j)"/>

We now have a probabilistic model parameterized by $k+1$ numbers: $\mu_1, \dots, \mu_k, \sigma$. We will use maximum likelihood to find those parameters. The principle tells us that we should minimize this expression:

<Math displayMode = {true} math = "\frac{n}{2\pi k} \log \sigma + \sum_{i = 1}^n - \sum_{i = 1}^n \log \sum_{j = 1}^k e^{-\frac{||x_i-\mu_j||^2}{\sigma^2}}"/>

Optimizing this expression corresponds to an algorithm known as [soft $k$-means](https://en.wikipedia.org/wiki/Fuzzy_clustering) (TODO check). The "word" soft is there because the parameter $\sigma$ enables us to output a distribution for each point $x$ telling us how likely it is that $x$ comes from each cluster. 

In practice, people don't care that much about this; knowing the closest cluster is enough. This corresponds to being interested in the limit $\sigma \rightarrow 0$. In this limit, the beastly expression ?? simplifies pretty nicely. In particular, we can replace the summation <Math math = "\sum_{j = 1}^k e^{-\frac{||x_i-\mu_j||^2}{\sigma^2}}" /> by <Math math = "\max_{j = 1}^k e^{-\frac{||x_i-\mu_j||^2}{\sigma^2}}" /> since all other terms are simply negligible. The expression ?? simplifies to this:

<Math displayMode = {true} math = "\sum_{i = 1}^n \min_{j = 1}^k ||x_i-\mu_j||^2"/>

The problem of finding $\mu_1, \dots, \mu_k$ that minimize this expression is called [$k$-means](https://en.wikipedia.org/wiki/K-means_clustering). 

### Classification by neural nets <a id = "neural-nets"></a>

We are given a huge number of images, and each one has one of $k$ possible labels (a dog, a muffin, ...). We want to optimize a neural net that takes an image on the input and outputs a probability distribution over the $k$ possible classes. 

Coming up with an architecture of a neural net is a very tricky problem. Maximum entropy helps a little bit there -- [we already discussed](04-max-entropy/softmax) that it suggests that the last layer should convert logits into probabilities by softmax. 

Next, the maximum-likelihood principle says that we should maximize the log-likelihood, i.e., if on image $X_i$ with label $\ell_i$ the net outputs a distribution $p_1^i, \dots, p_k^i$, we should try to minimize
<Math displayMode={true} math = "\sum_{i = 1}^n \log 1/p_{\ell_i}^i">

[As we know](03-minimizing/mle), log-likelihood is pretty much the same as cross-entropy, so in this case, this is usually called "optimizing the cross-entropy loss". 

### Variational Autoencoders <a id="variational-autoencoders"></a>

One of our riddles mentioned the "beastly" loss function of variational autoencoders (VAEs), which are a class of generative models. Let's demystify this loss function using our understanding of KL divergence.

A VAE consists of two parts:

1. An encoder network that maps an input $x$ to a distribution over latent variables $z$
2. A decoder network that maps a latent variable $z$ back to a distribution over possible inputs $x$

The goal is to find a good latent representation that captures the important features of the data while allowing for efficient generation of new samples.

The loss function for a VAE can be written as:

$$\mathcal\{L\}(\theta, \phi; x) = -\mathbb\{E\}_\{z \sim q_\phi(z|x)\}[\log p_\theta(x|z)] + D_\{KL\}(q_\phi(z|x) \| p(z))$$

Let's break this down:

- The first term is the reconstruction loss, measuring how well we can reconstruct $x$ after encoding it to $z$ and then decoding it back.
- The second term is the KL divergence between the encoder's distribution $q_\phi(z|x)$ and a prior distribution $p(z)$ (usually a standard normal distribution).

This loss function can be derived from the principle of maximizing the evidence lower bound (ELBO) on the log-likelihood of the data. The ELBO is:

$$\log p_\theta(x) \geq \mathbb\{E\}_\{z \sim q_\phi(z|x)\}[\log p_\theta(x|z)] - D_\{KL\}(q_\phi(z|x) \| p(z))$$

Maximizing this lower bound is equivalent to minimizing the negative of the right-hand side, which gives us the VAE loss function.

What's interesting here is that the KL term acts as a regularizer, encouraging the encoder's distribution to be similar to the prior. This enables sampling from the model by simply sampling from the prior and passing it through the decoder. It also ensures that the latent space has nice properties like continuity and completeness.

The "beastly" appearance of this loss function is simply due to the combination of reconstruction and regularization terms, both of which have clear interpretations in terms of our information-theoretic toolkit.

## What's next

Deriving the loss function for variational autoencoder was meant to be the culmination of this minicourse. We have used all the intuition, understanding, and technology developed thus far!

This means you are finished reading the main part, congrats! If you want to continue, we cover a few more bits and pieces next. You can also check out [Resources](/resources), the [About](/about) page.
