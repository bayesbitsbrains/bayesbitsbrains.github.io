# Fisher information & statistics

In this chapter, we will explain a bit more about how KL divergence connects to statistics. In particular, one of the key statistical concepts is a so-called Fisher information, and we will see how one can derive it using KL divergence.

## The polling problem

In [one of our riddles](00-introduction), we have been talking about the problem with polling: If you want to learn the percentage of the vote for one party up to $\eps$, you need sample about $1/\eps^2$ voters. <Footnote>More precisely, it's about $1/(2\eps^2) \cdot \log 1/\delta$ if you want your estimate to be within $\pm \eps$ with probability at least $1-\delta$. In practice, this means that if you want to be withing $\pm 2\%$ with 75\% probability, you should sample around 1700 voters. </Footnote>

<PollingErrorCalculator />

## The law of large numbers

One way you can understand this is via [the law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers). This law tells you that if you flip a fair coin $n$ times, you typically get around $n/2 \pm \sqrt{n}$ heads. <Footnote>You can prove this by estimating the variance of the sum and using [Chebyshev's inequality](https://en.wikipedia.org/wiki/Chebyshev%27s_inequality). </Footnote>. If your coin is biased and heads come out with probability $1/2 + \eps$, then the law says you will typically see around $(1+\eps)n/2 \pm \sqrt{n}$ heads. If you want to be able to separate the two options, you need the intervals $n/2 \pm \sqrt{n}$ and $(1+\eps)n/2 \pm \sqrt{n}$ to be disjoint, which means that $\eps \cdot n \approx \sqrt{n}$, i.e., $n$ should be at least around $1/\eps^2$.

## KL intuition

But I want to offer a complementary intuition about the polling problem. Let's go all the way back to [the first chapter where we introduced KL](01-definition). Our running example was a Bayesian experimenter who wanted to distinguish whether his coin is fair or not fair. If the two distributions to distinguish are $(50\%, 50\%)$ and $(50\% + \eps, 50\% - \eps)$, then the problem of our experimenter gets extremely similar to the problem of estimating the mean up to $\eps$!

Remember, if we have two very similar distributions, KL between them is going to be positive, but very small. It turns out that

$$
D((0.5 + \eps, 0.5 - \eps), (0.5, 0.5)) \approx 2\eps^2
$$

The intuition we should take from this is that each time we flip a coin, we get around "$2\eps^2$ bits of information" about whether the coin is fair or biased by $\eps$. Intuitively, once we get around 1 bit of information, we begin to be able to say which of the two coins we are looking at. We conclude that the number of trials we need to estimate the bias of a coin should scale like $n = 1/\eps^2$. <Footnote>Here's one concrete place where this way of thinking is helpful. It's rather easy to prove that $O(1/\eps^2)$ samples suffice to estimate the bias up to $\eps$ (Chebyshev inequality). But how do you prove that you need $\Omega(1/\eps^2)$ samples? A fast way to do that is to compute that KL is $\le 2\eps^2$, hence it is at most $1/10$ even after $1/(100\eps^2)$ samples. Then you can use the so-called [Pinsker inequality](https://en.m.wikipedia.org/wiki/Pinsker%27s_inequality) to get bound on variation distance which in turn implies that you can't distinguish the two distributions with a good success probability. </Footnote>

## Fisher information

The trick with looking at KL divergence between two very similar distributions can be generalized further. Think of a setup where you have a family of distributions characterized by some number. For example:

-- biased coins with heads-probability $p$ for all $p \in [0,1]$.
-- gaussians $N(\mu, 1)$ for all $\mu \in \mathbb{R}$.
-- gaussians $N(0, \sigma^2)$ for all $\sigma^2 \ge 0$.

More generally, we have a set of distributions $p_\theta$ parameterized by a number $\theta$.

One of the most basic tasks in statistics is that we have data $p$ and we want to find the $\theta$ so that $p_\theta$ fits the data the best. We have already seen this a few times, and we have seen in [the chapter about minimizing KL](03-minimizing) why the maximum likelihood estimators are a reasonable approach to this.

But we can go a bit further -- if we assume that $p$ has literally been generated by one of the distributions $p_\theta$, we may ask how many samples we need to estimate $\theta$ up to additive $\eps$. Our polling question is just a special case of this more general question.

Let's see what intuition we get using KL. Our intuition tells us if the data were generated by $p_\theta$, it's key to understand the value of <Math math="D(p_{\theta + \eps}, p_\theta)" />. The most general formula for it is a bit opaque:

<Math displayMode={true} math="
D(p_{\theta + \eps}, p_\theta) = \int p_{\theta + \eps}(x) \cdot \log \frac{p_{\theta + \eps}}{p_\theta} dx
" />

We can estimate the value of the right-hand side, if we are able to compute the derivatives <Math math="\frac{\partial}{\partial \theta} p_{\theta}, \frac{\partial^2}{\partial \theta^2} p_{\theta}, \dots" /> -- we can then use Taylor approximation of <Math math="p_{\theta + \eps}" /> around $p_\theta$ to approximate the right-hand side. Long story short, if we know the first two derivatives, we can use the second-order Taylor approximation:

<Math displayMode={true} math="
p_{\theta + \eps}(x)
\approx
p_{\theta}(x)
+ \eps\cdot \frac{\partial}{\partial \theta} p_{\theta}(x)
+ \frac12 \frac{\partial^2}{\partial \theta^2} p_{\theta}(x)
+ O(\eps^3)
" />

After plugging it in and some calculations <Footnote>The detailed calculation involves Taylor expansion and integration by parts, which we'll skip here for brevity.</Footnote>, we get

<Math displayMode={true} math="
D(p_{\theta + \eps}, p_\theta) \approx \frac12 I(\theta) \cdot \eps^2 + O(\eps^3)
" />

The term $I(\theta)$ is the result of this daunting integral: <Math math="\int p_\theta(x) \cdot (\frac{\partial}{\partial \theta} \log p_\theta(x))^2 dx" /> and its called _Fisher information_. Let's not try to understand what this integral mean. The short story is that you can approximate KL divergence of similar distributions as $I(\theta)\cdot \eps^2$ and $I(\theta)$ can often be computed.

Our KL intuition is telling us that estimating a parameter $\theta$ up to $\eps$ should require about <Math math="\frac{1}{I(\theta)\eps^2}" /> many samples. Let's see what this intuition is saying in some cases:

1. If you want to estimate $p$ of a biased coin, $I(p) = \frac{1}{p(1-p)}$. Thus, our KL intuition suggests that estimating the value of $p$ (with constant probability) should take about $p/\eps^2$ samples. Our polling example was this for $p = 1/2$.

2. If you want to find the mean of normal random variable with known variance $\sigma^2$, we have $I(\mu) = 1/\sigma^2$. Hence, our KL intuition suggests that estimating $\mu$ up to $\eps$ requires about $\sigma^2 / \eps^2$ samples.

3. If you want to find the variance of normal random variable with known mean $\mu$, we have $I(\sigma^2) = 1/(2\sigma^4)$. Hence, our KL intuition suggests that estimating $\sigma^2$ requires about $\sigma^4 / \eps^2$ samples.

In all above cases, it turns out to be the case that our intuition is correct and really describes how many samples you need to solve given problem. In fact, there are two important results in statistics: One of them is [Cram√©r-Rao lower bound](https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound) that basically says that our formula of $n = 1/(I(\theta)\eps^2)$ is indeed a lower bound. On the other hand, the so-called [asymptotic normality theorem for MLE](https://gregorygundersen.com/blog/2019/11/28/asymptotic-normality-mle/) says that the MLE estimator is as good as our formula. <Footnote> These theorems use somewhat different language and measure the goodness of an estimator using its variance. </Footnote>

## Fisher metric

In fact, if we did the computation for <Math math="D(p_\theta, p_{\theta + \eps})" /> instead of <Math math="D(p_{\theta + \eps}, p_\theta)" />, we would get the same expression. So, although KL divergence is asymetrical in general, in this setup where we try to understand two distributions that are very close together, we can really think of it as being symetric. This way, we can define a proper distance function on the space of distributions -- it's called [Fisher information metric](https://en.wikipedia.org/wiki/Fisher_information_metric) and can give useful intuitions. For example, for the case of biased-coin-flipping distributions where coins have bias $p \in [0,1]$, we can represent all of them using a segment:

Coming soon: a visual representation of how Fisher metric transforms distance measurement between probability distributions.

We of course know how to measure the distance between points $p,q$ on a segment, its just $d(p,q) = |p-q|$. But Fisher information metric is saying that this normal way of doing it is not the right one -- around a point $p$, the distances should be scaled by $1/p$. This for example means that the distance between any two points $p$ and $q$ (with $p \le q \le \frac12$), is equal to $d(p,q) = \int_{p}^{q} 1/x dx = \ln \frac{q}{p}$. So, the distance equals the ratio of the two points. In other words, this is a supercharged-abstract-fancy way of saying what we have already seen many times -- probabilities should often be handled multiplicatively, not additively.

![Fisher metric](07-fisher_info/fisher2.png)


## What's next
In the [next chapter](08-kolmogorov), we will discuss Kolmogorov complexity.
